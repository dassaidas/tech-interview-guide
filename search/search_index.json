{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcd8Learning Hub","text":"<p>Purpose: This markdown document gathers curated C# learning content from multiple top-rated learning platforms (Udemy, Pluralsight, Coursera, YouTube, dotnet tutorials, questpond) and personal experience. This is for personal knowledge growth and not for redistribution.</p> <p>\ud83e\udde0 Why This Exists</p> <p>As a full-stack developer aiming to master C# for backend services, game development, and performance-critical apps, this <code>.md</code> file serves as a personal knowledge bank. It helps avoid fragmentation of learning resources by consolidating trusted references in one place.</p> <p>\ud83c\udfeb Mutiple Learning Platforms are used while creating this document</p>"},{"location":"Angular/forms/","title":"Angular Forms Interview Questions","text":""},{"location":"Angular/forms/#1-what-are-the-two-types-of-forms-in-angular","title":"1. What are the two types of forms in Angular?","text":"<p>Answer:</p> <ul> <li>Template-driven forms: Use Angular directives in HTML.</li> <li>Reactive forms: Use FormControl and FormGroup in TypeScript.</li> </ul> <p>Reactive Form Sample:</p> <pre><code>this.loginForm = this.fb.group({\n  username: [\"\", Validators.required],\n  password: [\"\", Validators.required],\n});\n</code></pre>"},{"location":"Angular/fundamentals/","title":"Angular Fundamentals","text":"<ul> <li>What is Angular?</li> <li>Explain the component lifecycle.</li> </ul>"},{"location":"Angular/migration/","title":"Angular Migration","text":"<ul> <li>How do you upgrade Angular from v10 to v18?</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/","title":"AzureFunctions","text":""},{"location":"Cloud/Azure/AzureFunctions/#1-explain-function-apps","title":"1. Explain function apps ?","text":"<p>\ud83d\udcd8 Azure Function Apps</p> <p>\u2705 Overview</p> <p>Azure Function App is a serverless compute service provided by Microsoft Azure that enables you to run event-driven, short-lived functions without provisioning or managing servers. It scales automatically and charges only for the execution time.</p> <p>\ud83d\ude80 Key Features</p> <ul> <li>Serverless: No need to manage infrastructure.</li> <li>Event-Driven: Triggered by HTTP requests, timers, queues, blobs, etc.</li> <li>Auto-Scaling: Functions scale out based on demand.</li> <li>Pay-per-Use: Billed only for the time your code runs.</li> <li>Multiple Language Support: C#, JavaScript, Python, Java, PowerShell, etc.</li> </ul> <p>\ud83e\udde9 Components</p> Component Description Function A unit of execution; the code you write to perform a task. Trigger Defines how the function is invoked (e.g., HTTP request, timer, queue). Binding Connects functions to other resources (input/output without custom code). Function App A container for one or more related functions sharing the same configuration. <p>\u2699\ufe0f Common Triggers</p> Trigger Type Description HTTP Trigger Executes function via HTTP request. Timer Trigger Executes on a schedule (CRON-based). Queue Trigger Executes when a message is added to a queue. Blob Trigger Executes when a blob is created/updated. Event Grid Reacts to events across Azure services. <p>\ud83d\udce6 Hosting Plans</p> Plan Type Description Consumption Auto-scaling, pay-per-use; suitable for most use cases. Premium Pre-warmed instances for zero startup delay and VNET support. Dedicated (App Service Plan) Fixed resources, always-on, suitable for high-load scenarios. <p>\ud83d\udd10 Security</p> <ul> <li>Integration with Azure Active Directory (AAD)</li> <li>Support for managed identities</li> <li>HTTPS-only traffic enforcement</li> <li>CORS configuration</li> </ul> <p>\ud83d\udcca Monitoring</p> <ul> <li>Application Insights for real-time telemetry, logs, and metrics</li> <li>Logs include function invocations, errors, dependencies, and performance metrics</li> </ul> <p>\ud83d\udee0\ufe0f Deployment Options</p> <ul> <li>Azure Portal</li> <li>Visual Studio / VS Code</li> <li>Azure CLI</li> <li>ARM/Bicep Templates</li> <li>GitHub Actions / Azure DevOps</li> </ul> <p>\ud83d\udcda Use Cases</p> <ul> <li>Backend APIs (via HTTP triggers)</li> <li>Data processing (timers, queues, blobs)</li> <li>Event-driven automation (Event Grid)</li> <li>Scheduled tasks and background jobs</li> <li>Integration with IoT, SaaS, or on-prem systems</li> </ul> <p>\ud83d\udccc Best Practices</p> <ul> <li>Keep functions small and single-responsibility.</li> <li>Use durable functions for stateful workflows.</li> <li>Monitor using Application Insights.</li> <li>Secure HTTP endpoints with authentication.</li> <li>Avoid blocking calls; use async code.</li> </ul> <p>\ud83d\udd17 Useful Links</p> <ul> <li>Azure Functions Documentation</li> <li>Pricing Calculator</li> <li>Triggers and Bindings Guide</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#2-explain-the-term-serverless","title":"2. Explain the term \u201cServerLess\u201d ?","text":"<p>\u2705 Definition</p> <p>Serverless is a cloud computing execution model where the cloud provider dynamically manages the allocation and provisioning of servers. With serverless architecture, developers can focus purely on writing code without worrying about server infrastructure, scaling, or maintenance.</p> <p>Despite the name, \"serverless\" does not mean servers are not involved. It simply means that server management is abstracted away from the user.</p> <p>\ud83d\ude80 Key Characteristics</p> Feature Description No Server Management Developers do not provision, scale, or manage servers. Automatic Scaling Serverless platforms scale up/down based on demand automatically. Event-Driven Functions run in response to events like HTTP requests, queue messages, etc. Micro-Billing Charges are based on exact resource consumption (per invocation, per GB-sec). Stateless Functions Each invocation is independent; state is not preserved between calls. <p>\ud83e\udde9 Common Serverless Services</p> Cloud Provider Serverless Offerings Azure Azure Functions, Azure Logic Apps, Event Grid AWS AWS Lambda, Step Functions, API Gateway Google Cloud Functions, Cloud Run <p>\ud83d\udcda Benefits of Serverless</p> <ul> <li>\u2705 Faster time to market</li> <li>\u2705 Reduced operational overhead</li> <li>\u2705 Built-in fault tolerance and scaling</li> <li>\u2705 Cost-effective for intermittent workloads</li> <li>\u2705 Better developer productivity</li> </ul> <p>\u26a0\ufe0f Limitations</p> <ul> <li>\u274c Cold starts can cause slight delays in execution</li> <li>\u274c Limited execution time (e.g., 5-15 minutes for functions)</li> <li>\u274c Stateless: Must manage state externally (e.g., databases, storage)</li> <li>\u274c Vendor lock-in concerns</li> </ul> <p>\ud83d\udccc Use Cases</p> <ul> <li>RESTful APIs</li> <li>Real-time file or data processing</li> <li>Scheduled jobs or cron tasks</li> <li>IoT data ingestion</li> <li>Event-driven automation</li> </ul> <p>\ud83d\udd17 Useful Links</p> <ul> <li>Serverless on Azure</li> <li>Serverless on AWS</li> <li>Google Cloud Functions</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#3-how-to-create-function-apps-using-portal","title":"3. How to create function apps using portal ?","text":"<p>This guide explains how to create an Azure Function App through the Azure Portal step-by-step.</p> <p>\u2705 Prerequisites</p> <ul> <li>An active Azure account</li> <li>Permissions to create resources in your subscription</li> </ul> <p>\ud83e\udded Step-by-Step Guide \ud83d\udd39 Step 1: Sign in to Azure Portal</p> <ul> <li>Go to: https://portal.azure.com</li> <li>Sign in with your credentials.</li> </ul> <p>\ud83d\udd39 Step 2: Search and Open \"Function App\"</p> <ol> <li>In the search bar at the top, type \u201cFunction App\u201d.</li> <li>Select Function App from the search results.</li> <li>Click + Create to start the setup.</li> </ol> <p>\ud83d\udd39 Step 3: Basics Tab</p> <p>Fill in the following details:</p> Field Description Subscription Choose your Azure subscription Resource Group Select existing or create a new resource group Function App name Unique name for the Function App (global DNS name) Region Select the Azure region closest to your users Code / Docker Choose Code (for normal use cases) Runtime stack Choose from .NET, Node.js, Python, Java, PowerShell Version Choose the runtime version (e.g., .NET 6, Node 18) Operating System Choose Windows or Linux Plan type Select Consumption (Serverless) for pay-per-use <p>Click Next to proceed.</p> <p>\ud83d\udd39 Step 4: Hosting Tab</p> <p>Configure storage and hosting:</p> Field Description Storage Account Create new or use an existing one Windows Plan SKU (If applicable) Choose S1 or B1 for dedicated <p>\ud83d\udd39 Step 5: Monitoring Tab</p> <ul> <li>Enable Application Insights for monitoring.</li> <li>Choose region (ideally same as Function App).</li> </ul> <p>\ud83d\udd39 Step 6: Tags (Optional)</p> <ul> <li>Add tags to organize your resources (e.g., <code>env=dev</code>, <code>team=backend</code>).</li> </ul> <p>\ud83d\udd39 Step 7: Review + Create</p> <ul> <li>Review all settings.</li> <li>Click Create to deploy your Function App.</li> </ul> <p>\u2705 After Deployment</p> <p>Once deployment completes:</p> <ol> <li>Go to the resource.</li> <li>Click Functions in the left menu.</li> <li>Click + Add to create a new function (select trigger like HTTP, Timer, etc.).</li> <li>Write code in the integrated editor or link external deployment.</li> </ol> <p>\ud83d\udce6 Sample Use Case: HTTP Trigger</p> <ul> <li>Choose HTTP trigger</li> <li>Authorization level: <code>Function</code> or <code>Anonymous</code></li> <li>Add simple code (e.g., return \"Hello World\")</li> <li>Test via the browser or Test/Run button</li> </ul> <p>\ud83d\udd17 Useful Resources</p> <ul> <li>Azure Function App Overview</li> <li>Triggers and Bindings</li> <li>Azure Pricing Calculator</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#4-for-function-app-template-in-visual-studio-what-has-to-be-installed","title":"4. For function app template in visual studio , what has to be installed ?","text":"<p>To develop and run Azure Function Apps using Visual Studio, you must install the appropriate workloads and extensions.</p> <p>\u2705 Visual Studio Requirements</p> <p>\ud83d\udd39 Visual Studio Version</p> <ul> <li>Visual Studio 2022 (recommended)</li> <li>Minimum edition: Community, Professional, or Enterprise</li> </ul> <p>\ud83d\udee0\ufe0f Required Workloads (during installation)</p> <p>When installing or modifying Visual Studio via the Visual Studio Installer, select:</p> <p>\u2714\ufe0f <code>.NET Core cross-platform development</code></p> <p>Includes support for C#, .NET 6/7, and ASP.NET Core</p> <p>\u2714\ufe0f Azure Development</p> <p>Includes tools for publishing and managing Azure services</p> <p>\ud83d\udce6 Required Components</p> <p>In addition to workloads, ensure these components are installed:</p> Component Name Description Azure Functions and Web Jobs Tools Enables Function App project templates .NET 6.0 or later SDK Runtime for executing Azure Functions Azure CLI (optional) Useful for deploying from command line Storage Emulator or Azurite (for local testing) Required to emulate Azure Storage locally <p>\ud83e\uddea Verify Installation</p> <p>After setup:</p> <ol> <li>Open Visual Studio.</li> <li>Click Create a new project.</li> <li>Search for \"Azure Functions\" in the template search bar.</li> <li>Select Azure Functions template and click Next.</li> </ol> <p>\ud83d\udcc2 Template Options</p> <p>When using the Azure Functions template, you'll be prompted to:</p> <ul> <li>Choose a trigger type (HTTP, Timer, Queue, Blob, etc.)</li> <li>Select a Function runtime version (.NET 6 / .NET 7)</li> <li>Choose an Authorization level (for HTTP trigger)</li> </ul> <p>\ud83d\udd17 Useful Links</p> <ul> <li>Install Visual Studio</li> <li>Visual Studio Installer Workloads</li> <li>Azure Functions Tools for VS</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#5-how-to-create-function-apps-using-visual-studio","title":"5. How to create function apps using visual studio ?","text":"<p>This guide walks you through creating and running an Azure Function App locally using Visual Studio 2022 or later.</p> <p>\u2705 Prerequisites</p> <ul> <li>Visual Studio 2022 (Community/Professional/Enterprise)</li> <li>Installed workloads:</li> <li>\u2705 Azure Development</li> <li>\u2705 .NET Core cross-platform development</li> <li>Azure Functions and Web Jobs Tools</li> <li>.NET 6 or later SDK</li> <li>(Optional) Azurite or Azure Storage Emulator for local testing</li> </ul> <p>\ud83e\udded Step-by-Step Instructions</p> <p>\ud83d\udd39 Step 1: Open Visual Studio</p> <ol> <li>Launch Visual Studio.</li> <li>Click on Create a new project.</li> </ol> <p>\ud83d\udd39 Step 2: Select Azure Function Template</p> <ol> <li>In the search bar, type: <code>Azure Functions</code>.</li> <li>Select Azure Functions from the list.</li> <li>Click Next.</li> </ol> <p>\ud83d\udd39 Step 3: Configure Project</p> Field Example Value Project Name <code>MyFunctionApp</code> Location <code>C:\\Projects\\Azure</code> Solution Name <code>MyFunctionApp</code> Framework <code>.NET 6 (LTS)</code> <p>Click Create.</p> <p>\ud83d\udd39 Step 4: Choose a Trigger</p> <p>You will now be prompted to choose a trigger for your function.</p> Option Description HTTP Trigger Invokes function via HTTP request Timer Trigger Scheduled (CRON) execution Queue Trigger Invoked when message is added to queue Blob Trigger Runs on blob file creation/update <p>You can also configure:</p> <ul> <li>Authorization level (for HTTP): <code>Anonymous</code>, <code>Function</code>, <code>Admin</code></li> <li>Storage account (use emulator for local or connect to Azure)</li> </ul> <p>Click Create.</p> <p>\ud83d\udd39 Step 5: Write Your Function</p> <p>Replace the default code with your logic. For example:</p> <pre><code>[FunctionName(\"HelloWorld\")]\npublic static IActionResult Run(\n    [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)] HttpRequest req,\n    ILogger log)\n{\n    log.LogInformation(\"C# HTTP trigger function processed a request.\");\n    return new OkObjectResult(\"Hello from Azure Function!\");\n}\n</code></pre> <p>\ud83d\udd39 Step 6: Run Locally</p> <ol> <li>Set the Function App project as the Startup Project.</li> <li>Click Run (F5) to launch the local development runtime.</li> <li> <p>Once started, look for the URL in the output window, for example: http://localhost:7071/api/HelloWorld</p> </li> <li> <p>Open the URL in your browser or use tools like Postman to test it.</p> </li> </ol> <p>\ud83d\udd39 Step 7: Publish to Azure (Optional)</p> <p>To deploy your Function App to Azure:</p> <ol> <li>Right-click the Function App project in Solution Explorer.</li> <li>Select Publish.</li> <li> <p>In the dialog, choose:</p> </li> <li> <p>Target: <code>Azure</code></p> </li> <li> <p>Specific Target: <code>Azure Function App (Windows/Linux)</code></p> </li> <li> <p>Sign in to your Azure account.</p> </li> <li>Select an existing Function App or click Create New to provision one.</li> <li>Click Finish, then click Publish.</li> </ol> <p>\ud83d\udce6 Output</p> <ul> <li>The function executes when triggered (e.g., via HTTP request).</li> <li>Application logs appear in Application Insights, if configured during deployment.</li> </ul> <p>\ud83d\udccc Best Practices</p> <ul> <li>\u2705 Keep each function small, simple, and single-responsibility.</li> <li>\u2705 Use Dependency Injection for shared services.</li> <li>\u2705 Store sensitive config data in <code>local.settings.json</code> or use Azure Key Vault for production.</li> <li>\u2705 Use Durable Functions for orchestrating workflows or long-running tasks.</li> </ul> <p>\ud83d\udd17 Helpful Links</p> <ul> <li>Azure Functions in Visual Studio</li> <li>Triggers and Bindings Reference</li> <li>Durable Functions Overview</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#6-consumption-plan-vs-service-plan","title":"6. Consumption plan VS Service plan","text":"<p>This guide explains the key differences between Consumption Plan and App Service Plan when hosting Azure Function Apps.</p> <p>\ud83e\udde9 1. Basic Comparison</p> Feature Consumption Plan App Service Plan Pricing Pay per execution and execution duration Fixed monthly cost based on instance size Scaling Automatic (event-based scale-out) Manual or auto-scale (based on settings) Idle Billing No charges when idle Billed 24/7, regardless of usage Cold Start Yes (can delay HTTP functions) No (always-on instances) Execution Time Limit 5 min (default), up to 60 min (Premium) Unlimited VNET Integration No (unless Premium Plan is used) Yes (full integration) Custom Domains &amp; SSL Yes Yes Deployment Slots Limited (via Premium) Supported Storage Account Required Not required <p>\ud83d\udcb0 2. Billing Details</p> <p>\ud83d\udccc Consumption Plan</p> <ul> <li>Pay-as-you-go model.</li> <li>Billed based on:</li> <li>Number of executions.</li> <li>Execution time (GB-seconds).</li> <li>Ideal for:</li> <li>Sporadic workloads.</li> <li>Event-driven systems.</li> <li>Cost-sensitive applications.</li> </ul> <p>\ud83d\udccc App Service Plan</p> <ul> <li>Fixed pricing based on the SKU (e.g., B1, S1, P1).</li> <li>Compute resources are always allocated, even if idle.</li> <li>Ideal for:</li> <li>High-throughput or always-on apps.</li> <li>Apps requiring advanced networking features.</li> <li>Co-located with web apps or APIs in same plan.</li> </ul> <p>\ud83d\ude80 3. Recommended Use Cases</p> Scenario Recommended Plan Lightweight APIs or background jobs \u2705 Consumption Plan Real-time, high-performance backend services \u2705 App Service Plan Functions requiring VNET or hybrid connectivity \u2705 App Service Plan Apps with unpredictable spikes in traffic \u2705 Consumption Plan Hosting multiple apps under a single plan \u2705 App Service Plan <p>\u26a0\ufe0f 4. Key Limitations</p> <p>\ud83d\udeab Consumption Plan</p> <ul> <li>Cold start delays (especially HTTP trigger).</li> <li>Execution timeout (max 5 or 60 minutes).</li> <li>No VNET or private endpoint access (unless Premium).</li> <li>Limited to 1.5 GB memory per function instance.</li> </ul> <p>\ud83d\udeab App Service Plan</p> <ul> <li>You pay even if no requests come in.</li> <li>More manual setup for scaling and cost control.</li> </ul> <p>\ud83d\udccc 5. Summary Table</p> Criteria Consumption Plan App Service Plan Cost Efficiency Best for low usage Best for constant load Startup Performance Cold starts possible No cold starts Scaling Auto (event-driven) Manual/Auto Networking Limited (unless Premium) Full VNET support Execution Limit 5\u201360 mins No limit <p>\ud83d\udd17 6. Useful References</p> <ul> <li>Azure Function Hosting Plans Explained</li> <li>Azure Pricing Calculator</li> <li>Premium Plan Overview</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#7-what-is-the-importance-of-scale-controller","title":"7. What is the importance of Scale controller ?","text":"<p>\u2705 Overview</p> <p>The Scale Controller is a key component in Azure Functions that is responsible for automatically managing the number of function host instances based on workload demand. It plays a critical role in serverless scaling behavior, especially in the Consumption and Premium plans.</p> <p>\ud83d\ude80 What Does the Scale Controller Do?</p> <p>The Scale Controller:</p> <ul> <li>Monitors trigger metrics (e.g., queue length, message rate, HTTP requests).</li> <li>Decides when to add or remove instances of the function host.</li> <li>Ensures automatic scaling of function apps without manual intervention.</li> <li>Keeps the system cost-efficient by scaling down to zero during idle times (Consumption plan).</li> </ul> <p>\ud83d\udce6 How It Works</p> <p>\ud83d\udd39 Inputs It Monitors</p> Trigger Type Metric Used by Scale Controller HTTP Trigger Request rate and current latency Queue Trigger Queue length and message age Timer Trigger No scaling (not load-dependent) Event Hub / Service Bus Number of messages, lag, throughput <p>\ud83d\udd39 Decisions Made</p> <ul> <li>Scale Out: Increase instances if load increases.</li> <li>Scale In: Reduce instances when load decreases.</li> <li>Idle Timeout: In Consumption Plan, scale to 0 when idle.</li> </ul> <p>\ud83e\udde0 Intelligent Auto-Scaling</p> <ul> <li>Based on event-driven triggers, not CPU/memory like traditional autoscaling.</li> <li>Uses proprietary algorithms to determine scaling thresholds.</li> <li>Operates per Function App and scales independently for each.</li> </ul> <p>\ud83d\udd10 Importance in Different Plans</p> Plan Type Role of Scale Controller Consumption Plan Required for true serverless; scales to zero. Premium Plan Provides warm instances + auto-scale. App Service Plan Manual or rules-based scaling (no Scale Controller). <p>\u26a0\ufe0f Limitations &amp; Considerations</p> <ul> <li>Cold Start: Can cause delay when scaling from 0 (Consumption Plan).</li> <li>Startup Latency: May take a few seconds to spin up new instances.</li> <li>Predictability: Scaling behavior is not always instant or linear.</li> </ul> <p>\ud83d\udccc Summary</p> Feature Benefit \ud83d\udd04 Automatic Instance Scaling No need to manage infrastructure \ud83d\udcb5 Cost-Effective Scales down to zero (no idle charges) \ud83d\ude80 Performance-Aware Adds instances under high load \u26a1 Event-Driven Tailored for each type of function <p>\ud83d\udd17 References</p> <ul> <li>Azure Functions Scaling Overview</li> <li>Azure Premium Plan</li> <li>Performance and Cold Starts</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#8-in-what-case-we-have-scale-controller","title":"8. In what case we have scale controller?","text":"<p>\u2705 Overview</p> <p>The Scale Controller is used by Azure Functions to automatically scale function app instances based on demand. However, it is only active under specific hosting plans and scenarios.</p> <p>\ud83d\uddc2\ufe0f Cases When Scale Controller is Active</p> Hosting Plan Scale Controller Used? Details Consumption Plan Yes Automatically scales out/in based on trigger events. Scales to zero when idle. Ideal for event-driven, serverless workloads. Premium Plan Yes Similar to Consumption but with pre-warmed instances to avoid cold starts and longer execution limits. Auto-scales using Scale Controller. App Service Plan No Scaling is manual or based on App Service scaling rules. No Scale Controller involvement. Dedicated (Isolated) No Manual or configured autoscale; Scale Controller not used. <p>\u2699\ufe0f Trigger-Based Scaling</p> <p>The Scale Controller activates in response to metrics specific to the function's trigger type, such as:</p> <ul> <li>Queue length or message backlog (Queue trigger, Service Bus)</li> <li>HTTP request rate (HTTP trigger)</li> <li>Event Hub message throughput</li> </ul> <p>\u274c Cases Without Scale Controller</p> <ul> <li>When running Azure Functions on App Service Plans (dedicated VM-based plans).</li> <li>When hosting Functions inside a container without serverless hosting.</li> <li>In on-prem or Kubernetes-hosted Functions (using Kubernetes Horizontal Pod Autoscaler instead).</li> </ul> <p>\ud83d\udccc Summary</p> Scenario Scale Controller Usage Serverless, event-driven functions (Consumption, Premium) \u2705 Used for automatic scale Always-on dedicated hosting (App Service Plan) \u274c Not used; scaling manual or rule-based Containerized or custom hosting \u274c Not used; external scaling methods apply <p>\ud83d\udd17 References</p> <ul> <li>Azure Functions scale and hosting</li> <li>Hosting options for Azure Functions</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#9-what-is-a-web-hook-api-function-app","title":"9. What is a Web Hook / API function app?","text":"<p>\u2705 Definition</p> <p>A WebHook/API Function App in Azure is a type of trigger-based function that runs when it receives an HTTP request, typically via a URL endpoint. It allows developers to expose serverless APIs without managing servers or infrastructure.</p> <p>This type of function is ideal for:</p> <ul> <li>Building lightweight REST APIs</li> <li>Responding to WebHooks from services like GitHub, Stripe, or Teams</li> <li>Integrating with external systems over HTTP</li> </ul> <p>\ud83d\udd27 How It Works</p> <ul> <li>Uses an HTTP trigger to start execution.</li> <li>Can respond to <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, etc.</li> <li>The function URL acts like a Web API endpoint.</li> <li>Accepts parameters, headers, and body (e.g., JSON).</li> </ul> <p>\ud83d\udce6 Example Use Cases</p> Use Case Description GitHub WebHook Trigger a function when a repository is pushed or updated Slack Bot Command Respond to a slash command from Slack Form Submission Handler Process data from a contact or feedback form IoT API Gateway Receive data from devices over HTTP Serverless REST APIs Expose endpoints like <code>/api/products</code> or <code>/api/users</code> <p>\ud83d\udcdc Sample Code (C#)</p> <pre><code>[FunctionName(\"HttpExample\")]\npublic static async Task&lt;IActionResult&gt; Run(\n    [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = \"hello\")] HttpRequest req,\n    ILogger log)\n{\n    string name = req.Query[\"name\"];\n    return new OkObjectResult($\"Hello, {name ?? \"World\"}\");\n}\n</code></pre> <p>\ud83d\udccc Trigger URL Example: https://.azurewebsites.net/api/hello?name=Ganesh <p>\ud83d\udd10 Authorization Levels</p> Level Who Can Call It? Anonymous Anyone with the function URL Function Requires a function key appended to the URL Admin Requires the master key (full control) <p>\u2705 Benefits</p> <ul> <li>\ud83d\udd27 Simple HTTP-based integration with any system</li> <li>\u26a1 Fast and scalable with built-in auto-scaling</li> <li>\ud83d\udd10 Supports secure authentication via keys or tokens</li> <li>\ud83e\uddf1 Easily extendable to full REST API architecture</li> </ul> <p>\u26a0\ufe0f Considerations</p> <ul> <li>\u2744\ufe0f Cold starts may affect performance in Consumption Plan</li> <li>\ud83d\udcf6 Should use Application Gateway or API Management for advanced routing, caching, or security</li> <li>\ud83d\udcca Monitor and throttle incoming requests to avoid overuse or abuse</li> </ul> <p>\ud83d\udd17 Useful Resources</p> <ul> <li>HTTP Trigger (Azure Functions)</li> <li>WebHook Receiver Functions</li> <li>Authorization Levels in Azure Functions</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#10-what-is-hosting-plan-while-create-azure-functions","title":"10. what is Hosting Plan while create Azure Functions?","text":"<p>\u2705 Definition</p> <p>A Hosting Plan in Azure Functions determines:</p> <ul> <li>How your function app runs</li> <li>How it scales</li> <li>How it is billed</li> </ul> <p>When creating an Azure Function App, you must choose a hosting plan, which defines the compute resources used by your app.</p> <p>\ud83e\udde9 Types of Hosting Plans</p> Hosting Plan Description Consumption Plan Pay-per-use model. Auto-scales. Scales to zero when idle. Cold starts may occur. Premium Plan Auto-scaling with pre-warmed instances. No cold start. More powerful. App Service Plan Fixed-size VMs. Functions run like Web Apps. Manual or auto-scale. Elastic Premium Combines benefits of Premium + elasticity of Consumption. Kubernetes Host functions on your own AKS cluster. Full control. <p>\ud83d\udcb0 Billing Comparison</p> Plan Type Billing Model Idle Charges Auto-Scaling Cold Start Consumption Plan Per-execution (GB-seconds) \u274c No \u2705 Yes \u2705 Possible Premium Plan Based on pre-warmed instances \u2705 Yes \u2705 Yes \u274c No App Service Plan Fixed VM pricing \u2705 Yes \ud83d\udeab Manual \u274c No <p>\ud83d\ude80 Scaling Behavior</p> Plan Minimum Instances Max Instances Scale Based On Consumption Plan 0 ~200 (soft cap) Event volume, queue length Premium Plan 1+ (pre-warmed) 100+ Event volume, concurrency App Service Plan 1+ Manual CPU, memory, rule-based <p>\u2705 When to Use Which Plan?</p> Use Case Recommended Plan Low-traffic or bursty workloads \u2705 Consumption Plan Always-on APIs with low latency \u2705 Premium Plan Shared hosting with other web apps \u2705 App Service Plan Need VNET integration or long run time \u2705 Premium Plan Self-managed Kubernetes environments \u2705 Kubernetes Plan <p>\ud83d\udcdd Notes</p> <ul> <li>You can switch plans by recreating the Function App under a new plan.</li> <li>Premium and App Service Plans support custom domains, SSL, VNETs, and deployment slots.</li> </ul> <p>\ud83d\udd17 Useful Resources</p> <ul> <li>Azure Functions Hosting Options</li> <li>Pricing Calculator</li> <li>Premium Plan Details</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#11-what-to-do-if-azure-function-is-not-able-to-create-in-portal","title":"11. what to do if azure function is not able to create in portal?","text":"<ul> <li>Go to the function and click on settings and then environment variables and in there change the value of   FUNCTIONS_WORKER_RUNTIME from dotnet_isolated to the dotnet. Then we can see the option to create the function from azure portal</li> <li>and also check your plan does the plan have the functions_worker_runtime option or not.</li> </ul>"},{"location":"Cloud/Azure/AzureFunctions/#12-if-the-create-function-in-azure-portal-and-it-is-not-shown-in-visual-studio-what-to-do","title":"12. if the create function in azure portal and it is not shown in visual studio what to do?","text":"<ul> <li> <p>You can try the following steps to resolve this:</p> </li> <li> <p>Navigate to Tools -&gt; Options -&gt; Projects and Solutions -&gt; Azure Functions</p> </li> <li>Click on Check for Updates, then Download and install.</li> <li>restart viusal studio and you can see</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/","title":"AzureCosmosDB","text":""},{"location":"Cloud/Azure/Azure_Cosmos_DB/#1-what-is-the-goal-of-cosmos-db","title":"1. What is the goal of Cosmos DB ?","text":"<p>Azure Cosmos DB is Microsoft\u2019s globally distributed, multi-model database service designed to meet the modern application needs of scalability, performance, and availability.</p> <p>The primary goal of Azure Cosmos DB is to provide a globally distributed, highly available, low-latency database platform that supports multiple data models and offers guaranteed performance at scale.</p> <p>\u2705 Key Objectives</p> <ol> <li> <p>Global Distribution</p> <ul> <li>Offer seamless multi-region replication.</li> <li>Enable users to read and write data locally from anywhere in the world.</li> <li>Built-in multi-master replication for high availability.</li> </ul> </li> <li> <p>High Availability</p> <ul> <li>99.999% availability SLA for multi-region deployments.</li> <li>Automatic failover and redundancy for disaster recovery.</li> </ul> </li> <li> <p>Low Latency</p> <ul> <li>Target &lt;10ms latency for reads and &lt;15ms for writes at the 99th percentile.</li> <li>Optimized for high-performance real-time applications.</li> </ul> </li> <li> <p>Elastic Scalability</p> <ul> <li>Instant and automatic scaling of throughput (RU/s) and storage.</li> <li>Supports millions of transactions per second across regions.</li> </ul> </li> <li> <p>Multi-Model Support</p> <ul> <li>Allows storing and querying data in multiple formats:</li> <li>Document (JSON - MongoDB API)</li> <li>Key-Value</li> <li>Graph (Gremlin)</li> <li>Column-family (Cassandra API)</li> <li>Table API</li> </ul> </li> <li> <p>Multiple APIs</p> <ul> <li>Supports multiple APIs to integrate with popular databases:</li> <li> <p>SQL (Core API)</p> </li> <li> <p>MongoDB</p> </li> <li> <p>Cassandra</p> </li> <li>Gremlin</li> <li>Azure Table Storage</li> </ul> </li> <li> <p>Comprehensive SLAs</p> <ul> <li>Only database to offer guaranteed SLAs on:</li> <li>Availability</li> <li>Throughput</li> <li>Consistency</li> <li>Latency</li> </ul> </li> <li> <p>Five Consistency Levels</p> <ul> <li>Strong</li> <li>Bounded Staleness</li> <li>Session (default)</li> <li>Consistent Prefix</li> <li>Eventual</li> </ul> </li> </ol> <p>\ud83d\ude80 Real-World Use Cases</p> <pre><code>- Global e-commerce platforms\n- IoT &amp; telemetry ingestion\n- Real-time personalization &amp; recommendation engines\n- Gaming leaderboards\n- Financial transaction processing\n- Supply chain and logistics systems\n</code></pre> <p>\ud83d\udd17 References</p> <ul> <li>Azure Cosmos DB Overview \u2013 Microsoft Docs</li> <li>SLA for Azure Cosmos DB</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#2-explain-the-word-planet-scale","title":"2. Explain the word planet-scale ?","text":"<p>\ud83c\udf0d Definition</p> <p>Planet-scale refers to the ability of a software system \u2014 especially databases and cloud applications \u2014 to operate reliably, efficiently, and securely across the entire globe, serving millions or billions of users simultaneously from multiple geographic locations.</p> <p>\u2705 Key Characteristics of Planet-Scale Systems</p> Feature Description Global Distribution Data and services are automatically replicated across multiple continents and regions. Low Latency Worldwide Reads and writes are served from the nearest datacenter to reduce delay for end users globally. Elastic Scalability Can instantly scale up/down to handle sudden changes in workload or user traffic across countries. Multi-Master Write Support Allows write operations from multiple regions with automatic conflict resolution. High Availability Designed for 99.999% uptime using built-in failover and redundancy across regions. Geo-Compliance Supports data residency, privacy, and compliance requirements in different countries. <p>\ud83d\ude80 Example: Planet-Scale in Azure Cosmos DB</p> <p>Azure Cosmos DB is called a \"planet-scale\" database because it provides:</p> <ul> <li>Global data replication across any Azure region.</li> <li>Sub-10 ms latency read/write performance at 99th percentile.</li> <li>Multi-region writes and automatic failover.</li> <li>Support for localized access and compliance (e.g., GDPR, data sovereignty).</li> </ul> <p>\ud83d\udce6 Real-World Scenarios</p> Use Case Why Planet-Scale Matters Global e-commerce platforms Users expect fast performance globally. Real-time multiplayer gaming Millisecond latency is critical. IoT/Telemetry across continents Devices constantly sending data from all over the world. Social networks and messaging apps Users from different countries interacting 24/7. <p>\ud83e\udde0 Summary</p> <p>Planet-scale means building software and systems that perform reliably, quickly, and securely for a global audience, no matter where users are located.</p> <p>It\u2019s about scaling across the planet, not just within a single region or datacenter.</p> <p>\ud83d\udd17 References</p> <ul> <li>Planet-Scale Databases \u2013 Microsoft Azure Cosmos DB</li> <li>Distributed Systems Principles</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#3-explain-consistency-problem-in-cosmos","title":"3. Explain consistency problem in Cosmos ?","text":"<p>What is Consistency?</p> <p>Consistency refers to the guarantee that all users see the same data at the same time after a write operation. In distributed databases like Cosmos DB, ensuring consistency across multiple geographically distributed replicas is challenging.</p> <p>The Consistency Problem</p> <p>In a globally distributed system, data is replicated across multiple regions to provide high availability and low latency. However, this replication introduces challenges in keeping data consistent everywhere:</p> <ul> <li>When a client writes data to one region, other regions may not immediately see that update.</li> <li>This can lead to conflicting versions or stale reads, where some users see older data.</li> <li>The system must balance trade-offs between consistency, availability, and latency (the CAP theorem).</li> </ul> <p>Cosmos DB Consistency Levels</p> <p>Azure Cosmos DB addresses the consistency problem by offering five tunable consistency levels, allowing developers to choose the right balance for their application:</p> Consistency Level Description Use Case Strong Guarantees linearizability; reads always see the latest committed write. Financial transactions, critical apps Bounded Staleness Reads lag behind writes by a known, fixed amount of time or versions. Gaming leaderboards, social feeds Session (default) Guarantees monotonic reads and writes within a single client session. User sessions, personalized apps Consistent Prefix Reads see updates in the order they were committed, without gaps. Event processing Eventual Reads may see out-of-order or stale data but eventual consistency is guaranteed. Non-critical analytics, caching <p>Why is Consistency Challenging?</p> <ul> <li>Network latency: Propagating changes worldwide takes time.</li> <li>Partition tolerance: In case of network partitions, consistency may be sacrificed for availability.</li> <li>Multi-master writes: Allowing writes in multiple regions simultaneously can cause conflicts.</li> </ul> <p>Trade-Offs in Cosmos DB</p> Property Strong Bounded Staleness Session Consistent Prefix Eventual Latency High Medium Low Low Lowest Availability Lower High High High Highest Throughput Lower Medium High High Highest Data Freshness Absolute Bounded Session-scoped Ordered Eventual <p>Summary</p> <p>The consistency problem in Cosmos DB arises from the need to keep distributed replicas synchronized while providing low latency and high availability globally. Cosmos DB\u2019s tunable consistency levels give developers control over this trade-off to best fit their application's needs.</p> <p>References</p> <ul> <li>Azure Cosmos DB Consistency Levels</li> <li>CAP Theorem Explained</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#4-why-is-dr-bcp-not-the-main-goal-of-cosmos-how-is-it-different","title":"4. Why is DR , BCP not the main goal of cosmos , how is it different ?","text":"<p>Understanding DR and BCP</p> <ul> <li>Disaster Recovery (DR) refers to the strategies and processes to recover data and resume operations after catastrophic events (e.g., data center failure, natural disasters).</li> <li>Business Continuity Planning (BCP) is a broader approach to ensure that critical business functions continue during and after a disaster.</li> </ul> <p>Both focus on minimizing downtime and data loss after an incident.</p> <p>Azure Cosmos DB\u2019s Primary Goal</p> <p>Azure Cosmos DB is designed to be a planet-scale, globally distributed, multi-model database that offers:</p> <ul> <li>Seamless global distribution of data with multi-region replication.</li> <li>Low latency and high availability with 99.999% SLA.</li> <li>Tunable consistency models for application-specific requirements.</li> <li>Elastic scalability across throughput and storage.</li> <li>Multi-master writes and automatic conflict resolution.</li> </ul> <p>Why DR and BCP Are Not Its Main Goal</p> Aspect DR &amp; BCP Focus Cosmos DB Focus Objective Recovery and resumption after failures Continuous availability and global scale Scope Prepare for rare catastrophic events Built-in global distribution and availability for everyday use Data Availability Restore data from backups or replicas post-disaster Always-on read/write access across regions Latency Not primary concern Sub-10 ms latency globally User Experience Possible downtime during failover Transparent failover with no downtime Consistency Trade-off May sacrifice consistency for availability during recovery Offers tunable consistency levels with predictable SLAs <p>How Cosmos DB Differs from Traditional DR/BCP Solutions</p> <ul> <li>Built-in Geo-Replication: Cosmos DB replicates data across regions automatically, providing continuous data availability, not just backup copies.</li> <li>Multi-Master Writes: Allows writes in multiple regions simultaneously with automatic conflict resolution, reducing failover complexity.</li> <li>Automatic Failover: Cosmos DB provides automatic and transparent regional failover, minimizing downtime without manual intervention.</li> <li>SLAs Cover Availability &amp; Latency: Cosmos DB guarantees 99.999% availability and low latency as a core design goal, not just post-disaster recovery.</li> </ul> <p>Summary</p> <ul> <li>DR and BCP are about recovering from disasters, often with some downtime or data loss.</li> <li>Cosmos DB\u2019s goal is to provide always-on, globally distributed data access with low latency and high availability, making downtime or data loss a non-issue in most scenarios.</li> <li>Cosmos DB enables business continuity as part of its core functionality, not as an afterthought.</li> </ul> <p>References</p> <ul> <li>Azure Cosmos DB Overview</li> <li>Disaster Recovery and Business Continuity</li> <li>Azure Cosmos DB SLA</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#5-consistency-is-best-when-performance-is-more-important-than-consistency","title":"5. consistency is best when Performance is more important than consistency.","text":"<p>When Is Consistency Relaxed for Better Performance?</p> <p>In distributed systems, there is often a trade-off between consistency and performance due to network latency and replication delays.</p> <ul> <li>Consistency is relaxed (weaker consistency levels) when performance (latency and throughput) is more important than always reading the latest data.</li> <li>This means the system may return slightly stale or out-of-date data to achieve lower latency and higher availability.</li> <li>Examples include using Eventual Consistency or Consistent Prefix, where reads may lag behind writes but performance is optimized.</li> </ul> <p>Summary</p> Priority Choose Consistency Level Strong Consistency When correctness and latest data are critical, accepting higher latency Relaxed Consistency (e.g., Session, Eventual) When fast response and high availability matter more than immediate consistency <p>This trade-off is a key design consideration in systems like Azure Cosmos DB, which offers tunable consistency levels to fit your application needs.</p>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#6-consistency-should-be-selected-for-high-consistency-and-for-most-recent-data","title":"6. consistency should be selected for high consistency and for most recent data.","text":"<p>Strong consistency should be selected when your application requires:</p> <ul> <li>High data accuracy</li> <li>Always reading the most recent and up-to-date data</li> <li>Operations where stale or outdated data can cause errors or inconsistencies (e.g., financial transactions, inventory systems)</li> </ul> <p>Key Benefits of Strong Consistency</p> <ul> <li>Guarantees that all reads return the latest committed write</li> <li>Ensures linearizability across all replicas</li> <li>Suitable for mission-critical applications demanding correctness over latency</li> </ul> <p>Trade-Offs</p> <ul> <li>May incur higher latency due to synchronization between replicas</li> <li>Potentially lower availability during network partitions (CAP theorem)</li> </ul> <p>Choose strong consistency when data correctness is paramount and slight latency increases are acceptable.</p>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#7-explain-session-bounded-and-prefix-consistencies","title":"7. Explain session , bounded and prefix consistencies ?","text":"<p>Azure Cosmos DB offers multiple consistency levels to balance latency, availability, and data freshness. Below are explanations for three commonly used levels:</p> <ol> <li> <p>Session Consistency</p> <p>Scope: Guarantees consistency within a single client session.</p> <p>Behavior:</p> <p>Reads your own writes \u2014 after a write completes, subsequent reads from the same client session will see that write or a more recent one.</p> <p>Provides monotonic reads and monotonic writes within a session.</p> <p>Use Cases:</p> <p>Personalized user experiences (e.g., user profiles, shopping carts).</p> <p>Scenarios where clients need to see their latest writes, but strict global consistency is not necessary.</p> <p>Performance: Low latency, high availability.</p> </li> <li> <p>Bounded Staleness Consistency</p> <p>Scope: Guarantees reads lag behind writes by at most a specified time interval or number of versions (staleness window)</p> <p>Behavior:</p> <p>Reads might not see the most recent writes but will see all writes within the configured staleness bound.</p> <p>Provides global order of updates within the staleness window.</p> <p>Use Cases:</p> <p>Applications tolerating slightly stale data but requiring some ordering guarantees, e.g., leaderboards, social feeds.</p> <p>Performance: Moderate latency with stronger consistency guarantees than session.</p> </li> <li> <p>Consistent Prefix Consistency</p> <ul> <li>Scope: Guarantees that reads see writes in the order they were committed without gaps.</li> <li>Behavior:</li> <li>Reads never see out-of-order writes. For example, if updates A, B, and C occur in that order, a client will never see C before B.</li> <li>However, reads may lag and see earlier writes (e.g., A only), but never a later write without seeing all previous ones.</li> </ul> <p>Use Cases:</p> <p>Event processing pipelines, logging systems, or any workload that needs ordered events but can tolerate some staleness.</p> <p>Performance: Lower latency than bounded staleness, but no absolute freshness guarantee.</p> </li> </ol> <p>Summary Table</p> Consistency Level Guarantees Typical Use Cases Latency Session Monotonic reads/writes per session User sessions, personalization Low Bounded Staleness Reads lag behind writes by a fixed time/version window Leaderboards, social feeds Moderate Consistent Prefix Reads see writes in order, no gaps Event streams, logs Low to moderate <p>References</p> <ul> <li>Azure Cosmos DB Consistency Levels</li> <li>CAP Theorem and Distributed Consistency</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#8-what-is-multi-api-support-in-cosmos","title":"8. What is multi-api support in cosmos ?","text":"<p>Azure Cosmos DB is a multi-model, globally distributed database service that supports multiple APIs, enabling developers to use familiar data models and query languages while benefiting from Cosmos DB\u2019s scalability and low latency.</p> <p>What Does Multi-API Support Mean?</p> <p>Multi-API Support means that Cosmos DB allows you to interact with the same underlying database service using different APIs, each designed for a specific data model or developer ecosystem. This flexibility lets you:</p> <ul> <li>Use the APIs and tools you already know.</li> <li>Migrate existing applications to Cosmos DB with minimal changes.</li> <li>Work with multiple data models (document, graph, key-value, column-family) within a single service.</li> </ul> <p>Supported APIs in Azure Cosmos DB</p> API Name Data Model Description Use Cases Core (SQL) API Document (JSON) Native Cosmos DB API for querying JSON documents using SQL-like syntax. Web apps, mobile apps, IoT data MongoDB API Document (JSON) Enables applications written for MongoDB to run on Cosmos DB without code changes. Apps using MongoDB drivers &amp; tools Cassandra API Column-family Supports Cassandra Query Language (CQL) for wide-column data. Big data, time series, distributed apps Gremlin API Graph Enables graph database features using Gremlin traversal language. Social networks, recommendation engines Table API Key-Value Provides a key-value store compatible with Azure Table Storage API. Simple key-value storage, metadata <p>Benefits of Multi-API Support</p> <ul> <li>Flexibility: Choose the API that best fits your application requirements.</li> <li>Compatibility: Migrate existing workloads easily to Cosmos DB without rewriting application logic.</li> <li>Unified Service: All APIs benefit from Cosmos DB\u2019s global distribution, elastic scalability, and SLAs.</li> <li>Simplified Management: One backend service to manage multiple data models and APIs.</li> </ul> <p>Summary</p> <p>Azure Cosmos DB\u2019s multi-API support enables developers to use their preferred database models and query languages while leveraging Cosmos DB\u2019s globally distributed, scalable, and low-latency architecture \u2014 all within a single fully managed service.</p> <p>References</p> <ul> <li>Azure Cosmos DB APIs</li> <li>Choosing the Right API in Cosmos DB</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#9-explain-the-hierarchical-structure-of-cosmos-db","title":"9. Explain the hierarchical structure of Cosmos DB ?","text":"<p>Azure Cosmos DB organizes data and resources in a hierarchy to manage and scale globally distributed applications efficiently.</p> <p>Hierarchy Levels</p> <ol> <li> <p>Account</p> <ul> <li>The top-level resource.</li> <li>Represents a Cosmos DB service endpoint.</li> <li>Associated with one or more Azure regions for global distribution.</li> <li>Contains databases, offers throughput provisioning.</li> </ul> </li> <li> <p>Database</p> <ul> <li>Logical container for a set of containers (collections).</li> <li>Acts as a scope for provisioning throughput (manual or shared).</li> <li>Can contain multiple containers.</li> <li>Similar to a database in traditional database systems.</li> </ul> </li> <li> <p>Container</p> <ul> <li>Core unit of scalability and distribution.</li> <li>Also known as Collection, Table, or Graph depending on the API used.</li> <li>Stores JSON documents, key-value pairs, or graph data.</li> <li>Has a partition key to distribute data across physical partitions.</li> <li>Provisioned with throughput measured in Request Units (RUs).</li> </ul> </li> <li> <p>Item (Document/Row/Vertex/Edge)</p> <ul> <li>The individual data entity stored in a container.</li> <li>For Core (SQL) and MongoDB APIs, an item is a JSON document.</li> <li>For Cassandra API, an item is a row.</li> <li>For Gremlin API, items are vertices and edges.</li> <li>Each item is uniquely identified by an id (or primary key).</li> </ul> </li> </ol> <p>Visualization of the Hierarchy</p> <p>Cosmos DB Account \u2514\u2500\u2500 Database \u2514\u2500\u2500 Container (Collection/Table/Graph) \u2514\u2500\u2500 Items (Documents/Rows/Vertices/Edges)</p> <p>Additional Notes</p> <ul> <li>Partitioning: Containers use partition keys to shard data across multiple physical partitions for scalability.</li> <li>Throughput: Throughput (RU/s) can be provisioned at the account, database, or container level depending on configuration.</li> <li>Global Distribution: Cosmos DB accounts can replicate data across multiple regions transparently.</li> </ul> <p>References</p> <ul> <li>Azure Cosmos DB Core Concepts</li> <li>Data Model in Azure Cosmos DB</li> </ul>"},{"location":"Cloud/Azure/Azure_Cosmos_DB/#10-how-to-connect-to-cosmos-db-using-c-language","title":"10. How to connect to Cosmos DB using C# language ?","text":"<p>This guide shows you how to connect to Azure Cosmos DB using the Azure Cosmos DB .NET SDK in C#.</p> <p>Prerequisites</p> <ul> <li>An Azure Cosmos DB account (Core SQL API)</li> <li>Your connection string or URI and primary key</li> <li>.NET development environment (Visual Studio, .NET SDK)</li> <li>Install the Azure Cosmos SDK NuGet package:</li> </ul> <p><code>dotnet add package Microsoft.Azure.Cosmos</code></p> <pre><code>  using System;\n  using System.Threading.Tasks;\n  using Microsoft.Azure.Cosmos;\n  class Program\n  {\n  private static readonly string endpointUri = \"https://&lt;your-account&gt;.documents.azure.com:443/\";\n  private static readonly string primaryKey = \"&lt;your-primary-key&gt;\";\n  private CosmosClient cosmosClient;\n  private Database database;\n  private Container container;\n\n    private string databaseId = \"SampleDatabase\";\n    private string containerId = \"SampleContainer\";\n\n    public static async Task Main(string[] args)\n    {\n        try\n        {\n            Program p = new Program();\n            await p.ConnectCosmosAsync();\n        }\n        catch (CosmosException ex)\n        {\n            Console.WriteLine($\"Cosmos DB error: {ex.StatusCode} - {ex.Message}\");\n        }\n        catch (Exception e)\n        {\n            Console.WriteLine($\"Error: {e.Message}\");\n        }\n    }\n\n    public async Task ConnectCosmosAsync()\n    {\n        // Create a new CosmosClient instance\n        cosmosClient = new CosmosClient(endpointUri, primaryKey);\n\n        // Create the database if it does not exist\n        database = await cosmosClient.CreateDatabaseIfNotExistsAsync(databaseId);\n        Console.WriteLine($\"Created Database: {database.Id}\");\n\n        // Create the container if it does not exist\n        container = await database.CreateContainerIfNotExistsAsync(containerId, \"/partitionKey\");\n        Console.WriteLine($\"Created Container: {container.Id}\");\n\n        // Example: Add an item to the container\n        var newItem = new { id = Guid.NewGuid().ToString(), partitionKey = \"example\", name = \"Sample Item\" };\n        ItemResponse&lt;dynamic&gt; response = await container.CreateItemAsync(newItem, new PartitionKey(newItem.partitionKey));\n        Console.WriteLine($\"Created item with id: {response.Resource.id}\");\n    }\n  }\n</code></pre> <p>Explanation of Key Cosmos DB SDK Methods in C#</p> <ul> <li> <p>CosmosClient:   The main client class used to establish a connection to Azure Cosmos DB.</p> </li> <li> <p>CreateDatabaseIfNotExistsAsync:   Creates the database if it does not already exist in the Cosmos DB account.</p> </li> <li> <p>CreateContainerIfNotExistsAsync:   Creates the container (also called collection) within the database if it doesn\u2019t exist, specifying the partition key path.</p> </li> <li> <p>CreateItemAsync:   Adds a new item (document) to the specified container.</p> </li> </ul> <p>References</p> <ul> <li>Azure Cosmos DB .NET SDK documentation</li> <li>Quickstart: Use Azure Cosmos DB SDK for .NET</li> </ul>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/","title":"AzureFabricandMicroServices","text":""},{"location":"Cloud/Azure/Azure_Fabric_microservices/#1-why-automation-is-must-for-micro-services","title":"1. Why automation is must for Micro-services?","text":"<p>Microservices architecture breaks down applications into many small, independently deployable services. This distributed nature creates complexities that make automation essential.</p> <p>Key Reasons Automation is Crucial</p> <ol> <li> <p>Continuous Integration &amp; Continuous Deployment (CI/CD)</p> <ul> <li> <p>Frequent changes require automated build, test, and deployment pipelines.</p> </li> <li> <p>Ensures quick and reliable delivery of new features and bug fixes.</p> </li> <li>Reduces human errors during deployments.</li> </ul> </li> <li> <p>Service Scalability</p> <ul> <li>Automation enables dynamic scaling of individual microservices based on demand.</li> <li>Manual scaling is impractical and error-prone in distributed environments.</li> </ul> </li> <li> <p>Consistency Across Environments</p> <ul> <li>Automated provisioning and configuration management maintain consistency across dev, test, staging, and production.</li> <li>Avoids configuration drift and environment-specific bugs.</li> </ul> </li> <li> <p>Efficient Resource Management</p> <ul> <li>Automates resource allocation, monitoring, and cleanup.</li> <li>Improves cost efficiency and system reliability.</li> </ul> </li> <li> <p>Resilience and Recovery</p> <ul> <li>Automated health checks and self-healing mechanisms can detect and recover from failures.</li> <li>Minimizes downtime without manual intervention.</li> </ul> </li> <li> <p>Complexity Management</p> <ul> <li>Microservices often have many interdependent components.</li> <li>Automation orchestrates service dependencies, startup order, and updates seamlessly.</li> </ul> </li> </ol> <p>Summary</p> Challenge in Microservices How Automation Helps Frequent deployments Enables fast, reliable CI/CD Scaling Automates dynamic resource scaling Environment drift Ensures configuration consistency Service failures Provides automated recovery Complex inter-service workflows Orchestrates and manages dependencies <p>Automation is not just helpful \u2014 it\u2019s critical to manage, deploy, and maintain microservices efficiently and reliably.</p> <p>References</p> <ul> <li>Microservices Automation</li> <li>CI/CD for Microservices</li> </ul>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#2-what-are-the-infrastructure-issues-when-deploying-micro-services","title":"2. What are the infrastructure issues when deploying Micro-services?","text":"<p>Deploying microservices introduces several infrastructure challenges due to their distributed and decoupled nature.</p> <p>Key Infrastructure Issues</p> <ol> <li> <p>Service Discovery</p> <ul> <li>Microservices dynamically scale and move, requiring mechanisms to locate service instances.</li> <li>Without proper service discovery, services cannot communicate reliably.</li> </ul> </li> <li> <p>Load Balancing</p> <ul> <li>Efficiently distributing requests across multiple instances of a service is essential.</li> <li>Balancing load to avoid hotspots or downtime is complex.</li> </ul> </li> <li> <p>Network Complexity</p> <ul> <li>Increased inter-service communication leads to more network traffic.</li> <li>Handling latency, retries, failures, and network partitions becomes harder.</li> </ul> </li> <li> <p>Configuration Management</p> <ul> <li>Managing diverse configurations across many services.</li> <li>Propagating configuration changes without downtime is difficult.</li> </ul> </li> <li> <p>Security</p> <ul> <li>Securing communication between services (authentication, authorization, encryption).</li> <li>Managing secrets and credentials securely.</li> </ul> </li> <li> <p>Logging, Monitoring, and Tracing</p> <ul> <li>Collecting and correlating logs and metrics from many distributed services.</li> <li>Implementing distributed tracing to track requests end-to-end.</li> </ul> </li> <li> <p>Deployment and Orchestration</p> <ul> <li>Coordinating deployments of multiple interdependent services.</li> <li>Managing versioning, rollbacks, and zero-downtime deployments.</li> <li>Often requires container orchestration platforms like Kubernetes.</li> </ul> </li> <li> <p>Data Management</p> <ul> <li>Managing data consistency across distributed services.</li> <li>Handling distributed transactions or eventual consistency.</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Allocating compute, memory, and storage efficiently.</li> <li>Preventing resource contention and ensuring availability.</li> </ul> </li> </ol> <p>Summary Table</p> Infrastructure Issue Description Service Discovery Dynamically locating service instances Load Balancing Distributing traffic evenly Network Complexity Managing inter-service communication Configuration Management Handling configuration at scale Security Protecting data and services Logging &amp; Monitoring Centralizing logs and metrics Deployment &amp; Orchestration Managing service lifecycle and dependencies Data Management Ensuring data integrity and consistency Resource Management Efficient allocation and scaling of resources <p>References</p> <ul> <li>Microservices Architecture - Martin Fowler</li> <li>Kubernetes Documentation</li> <li>Service Mesh Concepts</li> </ul>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#3-define-azure-fabric-in-one-sentence","title":"3. Define Azure fabric in one sentence ?","text":"<p>Azure Service Fabric is a distributed systems platform that simplifies the packaging, deployment, and management of scalable and reliable microservices and containers in the cloud.</p>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#4-what-is-the-use-of-azure-fabric-sdk-and-local-cluster-manager-tool","title":"4. What is the use of Azure fabric SDK and Local cluster manager tool ?","text":"<ul> <li> <p>Azure Service Fabric SDK:   Provides developers with libraries, tools, and APIs to build, test, and deploy Service Fabric applications and microservices efficiently.</p> </li> <li> <p>Local Cluster Manager Tool:   Enables creating and managing a local Service Fabric cluster on a development machine to test and debug Service Fabric applications locally before deploying to the cloud.</p> </li> </ul>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#5-differentiate-between-stateful-and-stateless-microservices","title":"5. Differentiate between stateful and stateless microservices ?","text":"Aspect Stateful Microservices Stateless Microservices Definition Maintain and manage state (data) across multiple requests Do not retain any state between requests Data Storage Store data either in-memory or durable storage No internal data storage; rely on external storage if needed Use Case Applications that require data persistence, e.g., shopping carts, user sessions Services that perform independent operations, e.g., authentication, logging Scalability More complex due to state synchronization between nodes Easier to scale out since any instance can handle requests Failure Recovery Needs mechanisms to recover or replicate state Can recover easily by simply restarting without data loss Examples Databases, session management, chat applications Stateless APIs, microservices handling requests without data retention"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#6-explain-the-importance-of-configuration-project-in-azure-fabric-projects","title":"6. Explain the importance of configuration project in Azure fabric projects ?","text":"<p>The Configuration Project in Azure Service Fabric is crucial because it manages and centralizes all the configurable settings for your Service Fabric applications, such as connection strings, environment-specific variables, and feature toggles. This separation allows you to:</p> <ul> <li>Easily update settings without redeploying or changing the application code.</li> <li>Support multiple environments (development, testing, production) with different configurations.</li> <li>Maintain clean code by externalizing environment-specific data.</li> <li>Enable dynamic configuration updates to running services without downtime.</li> <li>Simplify deployment and management by keeping configuration organized and version-controlled separately from application logic.</li> </ul>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#7-explain-the-importance-of-statemanager-in-stateful-projects","title":"7. Explain the importance of \u201cStateManager\u201d in stateful projects ?","text":"<p>The <code>StateManager</code> is a key component in stateful Service Fabric applications that provides reliable, transactional storage for the service\u2019s state. Its importance lies in:</p> <ul> <li>Reliable State Persistence: Ensures that state data is durably stored and survives service restarts and failures.</li> <li>Transactional Consistency: Supports transactions that guarantee atomic updates to the state, preventing partial or corrupt data.</li> <li>Simplifies State Handling: Abstracts complex state management tasks, allowing developers to work with reliable collections (like dictionaries, queues) easily.</li> <li>Enables High Availability: Replicates state across multiple nodes to ensure data availability and fault tolerance.</li> <li>Facilitates Stateful Microservices: Makes it possible to build microservices that maintain and manage their own data reliably within the cluster.</li> </ul>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#8-where-is-web-application-url-configured","title":"8. Where is Web application URL configured ?","text":"<p>The Web Application URL is typically configured in one or more of the following places depending on the project setup:</p> <ul> <li> <p>In Azure Service Fabric projects:   The URL is specified in the ServiceManifest.xml file inside the <code>&lt;Endpoints&gt;</code> section, defining the port and protocol the service listens on.</p> </li> <li> <p>In configuration files:   Such as <code>appsettings.json</code> (for ASP.NET Core apps) or <code>web.config</code> (for older ASP.NET apps), where base URLs or API endpoints can be set.</p> </li> <li> <p>In the Azure Portal:   When deployed, the public-facing URL is configured as part of the Azure App Service or Azure Load Balancer settings.</p> </li> <li> <p>In code or environment variables:   Sometimes URLs are set programmatically or via environment variables for flexibility across environments.</p> </li> </ul> <p>In summary, the primary configuration for the web application URL in Service Fabric is inside the ServiceManifest.xml under the endpoint definitions.</p>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#9-differentiate-between-ltsc-and-sac","title":"9. Differentiate between LTSC and SAC ?","text":"Aspect LTSC (Long-Term Servicing Channel) SAC (Semi-Annual Channel) Release Frequency Every 2\u20133 years Twice a year (every 6 months) Support Duration 5 years of mainstream support + 5 years extended 18 months per release Use Case Stable environments needing long-term support, e.g., production servers Rapid innovation and feature testing environments Features Receives only security and critical updates Receives new features and improvements frequently Update Model In-place upgrades are rare; major upgrades at LTSC release Frequent feature updates and improvements Examples Windows Server 2019, Windows 10 Enterprise LTSC Windows Server versions released every 6 months"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#10-what-role-does-certificate-play-in-fabric","title":"10. What role does certificate play in fabric ?","text":"<p>Certificates in Azure Service Fabric are used to:</p> <ul> <li>Authenticate and secure communication between clients, nodes, and services within the cluster.</li> <li>Encrypt data in transit to ensure confidentiality and prevent eavesdropping.</li> <li>Enable cluster security by validating identities and establishing trust.</li> <li>Support mutual authentication between cluster nodes and external clients.</li> <li>Protect sensitive configuration and management operations by restricting access only to authorized parties.</li> <li>Facilitate secure management of the cluster via tools and APIs.</li> </ul> <p>Overall, certificates are critical for ensuring the security, integrity, and trustworthiness of the Service Fabric cluster and its services.</p>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#11-what-is-the-use-ad-user-in-fabric","title":"11. what is the use AD user in fabric ?","text":"<p>An AD User in Azure Service Fabric is used to:</p> <ul> <li>Authenticate and authorize users or services accessing the Service Fabric cluster, ensuring secure access control.</li> <li>Manage role-based access control (RBAC) by assigning permissions to AD users or groups for performing cluster management tasks.</li> <li>Integrate with enterprise identity systems, enabling seamless single sign-on (SSO) and centralized user management.</li> <li>Secure cluster operations by restricting administrative and operational actions to authenticated AD identities.</li> <li>Facilitate auditing and compliance by tracking user activities based on their AD credentials.</li> </ul> <p>In essence, AD users help enforce security policies and manage access within Service Fabric clusters in enterprise environments.</p>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#12-explain-azure-vault","title":"12. Explain Azure vault ?","text":"<p>Azure Key Vault is a cloud service that provides secure storage and management of sensitive information such as secrets, encryption keys, and certificates. It helps safeguard cryptographic keys and secrets used by cloud applications and services, enabling:</p> <ul> <li>Centralized secret management for passwords, API keys, connection strings, and certificates.</li> <li>Secure key management with hardware security modules (HSMs) for cryptographic keys.</li> <li>Access control and auditing using Azure Active Directory (AAD) and detailed logging.</li> <li>Simplified compliance with regulatory requirements by securely handling sensitive data.</li> <li>Seamless integration with other Azure services and custom applications for secure secret retrieval.</li> </ul> <p>Azure Key Vault helps improve security posture by reducing the risk of accidental exposure and simplifying secret lifecycle management.</p>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#13-how-to-publish-using-visual-studio-to-azure-fabric","title":"13. How to publish using visual studio to Azure fabric ?","text":"<ol> <li> <p>Open your Service Fabric project in Visual Studio.</p> </li> <li> <p>Right-click the project (usually the application project, not the service project) in Solution Explorer and select Publish.</p> </li> <li> <p>In the Publish Service Fabric Application dialog:</p> <ul> <li>Choose Azure Service Fabric Cluster as the target.</li> <li>Click Next.</li> </ul> </li> <li> <p>Select or enter the cluster connection details:</p> <ul> <li>Provide the cluster endpoint URL (e.g., <code>https://&lt;clustername&gt;.&lt;region&gt;.cloudapp.azure.com:19080</code>).</li> <li>Specify the management certificate or Azure Active Directory credentials for authentication.</li> </ul> </li> <li> <p>Choose the target application name on the cluster.</p> </li> <li> <p>Select the publish profile or create a new one for repeated use.</p> </li> <li> <p>Optionally configure advanced settings, such as:</p> <ul> <li>Application parameters.</li> <li>Upgrade modes (e.g., rolling upgrade).</li> <li>Timeout settings.</li> </ul> </li> <li> <p>Click Publish to build, package, and deploy your Service Fabric application to the Azure cluster.</p> </li> <li> <p>Monitor the output window for deployment progress and completion status.</p> </li> </ol> <p>Visual Studio simplifies the deployment by handling packaging, connection, and versioning automatically.</p>"},{"location":"Cloud/Azure/Azure_Fabric_microservices/#14-how-to-use-client-certificates-for-deployment-and-management","title":"14. How to use client certificates for deployment and management ?","text":"<p>Client certificates provide secure authentication for deployment and management operations in an Azure Service Fabric cluster, ensuring only authorized users or tools can perform sensitive actions.</p> <p>Steps to Use Client Certificates</p> <ol> <li> <p>Generate or Obtain a Client Certificate</p> <ul> <li>Create a certificate (e.g., using PowerShell, OpenSSL, or a certificate authority) that will identify the client (user or management tool).</li> </ul> </li> <li> <p>Upload the Client Certificate to the Service Fabric Cluster</p> <ul> <li>Add the client certificate\u2019s public key (thumbprint) to the cluster's security configuration (in <code>ClusterManifest.json</code> or via Azure portal) under the <code>ClientCertificateCommonNames</code> or <code>ClientCertificateThumbprints</code> section.</li> <li>This allows the cluster to recognize and trust the client certificate for authentication.</li> </ul> </li> <li> <p>Install the Client Certificate Locally</p> <ul> <li>Install the private key of the client certificate into the local certificate store on the machine that will run deployment or management commands (e.g., under Personal Certificates in Windows).</li> </ul> </li> <li> <p>Configure Client Tools to Use the Certificate</p> <ul> <li>For PowerShell or Service Fabric CLI tools, specify the client certificate when connecting to the cluster using parameters like <code>-FindType</code>, <code>-FindValue</code>, or by referencing the certificate\u2019s thumbprint.</li> <li>For Visual Studio publishing, configure the publish profile to use the client certificate.</li> </ul> </li> <li> <p>Perform Deployment and Management</p> <ul> <li>Use the client certificate authenticated session to deploy applications, manage the cluster, and perform administrative operations securely.</li> </ul> </li> </ol> <p>Benefits</p> <ul> <li>Provides mutual authentication between client and cluster.</li> <li>Enhances security by restricting access to authorized certificate holders.</li> <li>Supports non-interactive, automated deployment scenarios with certificates.</li> </ul>"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/","title":"Azure_Queues","text":"<p>Azure Queues , visibility timeouts ,Peek &amp; De-Queue.</p>"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#1-what-is-the-need-of-queues","title":"1. What is the need of Queues?","text":"<p>\ud83d\udd11 Purpose of Queues</p> <ul> <li> <p>Decouple application components:   Queues enable different parts of a system to communicate asynchronously without tight coupling.</p> </li> <li> <p>Buffering and load leveling:   They smooth out bursts of work by buffering requests, preventing system overload.</p> </li> <li> <p>Reliable message delivery:   Ensure that messages are stored durably until processed, even if consumers are temporarily unavailable.</p> </li> <li> <p>Enable asynchronous processing:   Producers can send messages and continue working without waiting for consumers to finish processing.</p> </li> <li> <p>Improve scalability:   Multiple consumers can process messages concurrently, improving throughput and scalability.</p> </li> </ul> <p>\ud83d\udccc Common Use Cases</p> <ul> <li>Task scheduling and background processing</li> <li>Communication between microservices</li> <li>Workload distribution in distributed systems</li> <li>Event-driven architectures and workflows</li> </ul> <p>\u2705 Summary</p> Need Explanation Decoupling Enables loose coupling between components Load leveling Buffers spikes in workload Reliability Guarantees message delivery Asynchronous processing Allows non-blocking communication Scalability Supports parallel and distributed processing"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#2-what-is-fifo-in-queues","title":"2. What is FIFO in Queues?","text":"<p>\ud83d\udd11 Definition</p> <ul> <li>FIFO stands for First-In, First-Out.</li> <li>It is a message processing order where the first message added to the queue is the first one to be processed.</li> <li>Ensures messages are handled in the exact order they were received.</li> </ul> <p>\u2699\ufe0f How FIFO Works in Queues</p> <ul> <li>Messages are enqueued (added) at the tail.</li> <li>Messages are dequeued (removed) from the head.</li> <li>Guarantees ordered processing, critical for workflows requiring sequence integrity.</li> </ul> <p>\ud83d\udccc Importance of FIFO</p> <ul> <li>Maintains data consistency when order matters.</li> <li>Prevents out-of-order processing that could cause errors or incorrect results.</li> <li>Vital for scenarios like:<pre><code>- Transaction processing\n- Event sourcing\n- Workflow orchestration\n</code></pre> </li> </ul> <p>\u2705 Summary</p> Term Meaning FIFO First-In, First-Out processing Purpose Process messages in order received"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#3-how-to-add-message-read-the-next-record","title":"3. How to add message read the next record?","text":"<p>\u2795 How to Add a Message and Read the Next Record in Azure Queue Storage (C#)</p> <p>1. Add a Message to the Queue</p> <pre><code>using Azure.Storage.Queues;\nusing System;\nusing System.Threading.Tasks;\n\nstring connectionString = \"&lt;your_connection_string&gt;\";\nstring queueName = \"myqueue\";\n\nvar queueClient = new QueueClient(connectionString, queueName);\n\n// Ensure queue exists\nawait queueClient.CreateIfNotExistsAsync();\n\nstring messageText = \"Hello, Azure Queue!\";\n\n// Add message to the queue\nawait queueClient.SendMessageAsync(messageText);\nConsole.WriteLine(\"Message added to queue.\");\n</code></pre> <p>2. Read the Next Message from the Queue</p> <pre><code>// Receive the next message (peek-lock)\nvar receivedMessage = await queueClient.ReceiveMessageAsync();\n\nif (receivedMessage.Value != null)\n{\n    Console.WriteLine($\"Dequeued message: {receivedMessage.Value.MessageText}\");\n\n    // Process message here...\n\n    // Delete message after processing to remove it from the queue\n    await queueClient.DeleteMessageAsync(receivedMessage.Value.MessageId, receivedMessage.Value.PopReceipt);\n}\nelse\n{\n    Console.WriteLine(\"No messages in queue.\");\n}\n</code></pre> <p>\ud83d\udcdd Notes</p> <ul> <li><code>ReceiveMessageAsync()</code> retrieves one message with a default visibility timeout (message becomes invisible temporarily).</li> <li>After processing, delete the message to prevent it from becoming visible again.</li> <li>You can receive multiple messages using <code>ReceiveMessagesAsync(int maxMessages)</code>.</li> </ul> <p>\ud83d\udccb Summary</p> Operation Method Add message <code>SendMessageAsync()</code> Read next message <code>ReceiveMessageAsync()</code> Delete processed message <code>DeleteMessageAsync()</code>"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#4-does-peekmessage-read-the-next-record","title":"4. Does PeekMessage read the next record?","text":"<p>\ud83d\udcd8 What is <code>PeekMessage</code>?</p> <ul> <li><code>PeekMessage</code> retrieves a message from the queue without changing its visibility.</li> <li>It does NOT dequeue the message; the message remains visible and available to other consumers.</li> <li>Useful for inspecting the next message without removing it from the queue.</li> </ul> <p>\u2699\ufe0f How It Differs from <code>ReceiveMessage</code></p> Method Behavior <code>ReceiveMessage</code> Retrieves and temporarily hides the message (locks it) for processing <code>PeekMessage</code> Retrieves the message without locking or hiding it <p>\ud83d\udd11 Key Points</p> <ul> <li><code>PeekMessage</code> lets you read the next message without affecting the queue state.</li> <li>It does not remove or mark the message as processed.</li> <li>The same message can be peeked multiple times by multiple consumers.</li> </ul> <p>\u2705 Summary</p> Method Reads Next Record? Removes from Queue? Changes Visibility? <code>PeekMessage</code> Yes No No <code>ReceiveMessage</code> Yes No (until deleted) Yes (locks message)"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#5-how-to-do-a-de-queue-in-queues","title":"5. How to do a De-queue in Queues?","text":"<p>Steps to Dequeue (Read &amp; Remove) a Message</p> <ol> <li>Receive the message (makes it invisible temporarily).</li> <li>Process the message as needed.</li> <li>Delete the message from the queue to complete dequeue.</li> </ol> <pre><code>using Azure.Storage.Queues;\nusing System;\nusing System.Threading.Tasks;\n\nstring connectionString = \"&lt;your_connection_string&gt;\";\nstring queueName = \"myqueue\";\n\nvar queueClient = new QueueClient(connectionString, queueName);\nawait queueClient.CreateIfNotExistsAsync();\n\n// Receive the next message\nvar receivedMessage = await queueClient.ReceiveMessageAsync();\n\nif (receivedMessage.Value != null)\n{\n    Console.WriteLine($\"Dequeued message: {receivedMessage.Value.MessageText}\");\n\n    // Process your message here\n\n    // Delete message to remove it permanently from the queue\n    await queueClient.DeleteMessageAsync(receivedMessage.Value.MessageId, receivedMessage.Value.PopReceipt);\n}\nelse\n{\n    Console.WriteLine(\"No messages available to dequeue.\");\n}\n</code></pre> <p>\ud83d\udcdd Notes</p> <ul> <li>The visibility timeout prevents other consumers from processing the same message simultaneously.</li> <li>If you don't delete the message after processing, it becomes visible again once the timeout expires.</li> </ul> <p>\ud83d\udccb Summary</p> Operation Method Receive message <code>ReceiveMessageAsync()</code> Delete message <code>DeleteMessageAsync()</code>"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#6-explain-visibility-time-out-concept","title":"6. Explain visibility time out concept?","text":"<p>\ud83d\udd11 What is Visibility Timeout?</p> <ul> <li>When a message is received (dequeued) from a queue, it becomes invisible to other consumers for a specific period called the visibility timeout.</li> <li>During this time, the receiver can process the message without other consumers seeing or processing it.</li> <li>If the message is deleted before the timeout expires, it is permanently removed from the queue.</li> <li>If not deleted within the timeout, the message becomes visible again and can be processed by other consumers.</li> </ul> <p>\u2699\ufe0f How It Works</p> <ol> <li>Message retrieved by a consumer using <code>ReceiveMessageAsync()</code>.</li> <li>Message is hidden (invisible) from other consumers for the visibility timeout duration.</li> <li>Consumer processes the message.</li> <li>Consumer deletes the message upon successful processing.</li> <li>If consumer fails to delete the message, it reappears in the queue after the timeout for retry.</li> </ol> <p>\ud83d\udccc Importance</p> <ul> <li>Prevents duplicate processing of the same message by multiple consumers.</li> <li>Supports reliable processing with automatic retries on failure.</li> <li>Allows scaling with multiple consumers processing messages independently.</li> </ul> <p>\ud83d\udcdd Default and Configurable</p> <ul> <li>Default visibility timeout is typically 30 seconds but can be configured up to 7 days.</li> <li>Can be set per message when receiving or updated dynamically.</li> </ul> <p>\u2705 Summary</p> Concept Description Visibility Timeout Period a message is hidden after being dequeued Purpose Prevents concurrent processing and ensures reliability Behavior Message reappears if not deleted within timeout"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#7-how-is-getmessage-different-from-peek","title":"7. How is GetMessage different from Peek?","text":"<p>\ud83d\udcd8 GetMessage</p> <ul> <li>Also known as: <code>ReceiveMessageAsync()</code> in Azure SDK.</li> <li>Purpose: Retrieves and locks the next message for processing.</li> <li>Behavior:</li> <li>Message becomes invisible to other consumers for the duration of the visibility timeout.</li> <li>Requires explicit deletion after processing to remove it from the queue.</li> <li>Use Case: When you want to process and then remove the message from the queue.</li> </ul> <p>\ud83d\udc40 PeekMessage</p> <ul> <li>Also known as: <code>PeekMessageAsync()</code></li> <li>Purpose: Inspects the next message without locking or removing it.</li> <li>Behavior:</li> <li>Message remains visible to other consumers.</li> <li>No effect on message state or visibility.</li> <li>Use Case: When you just want to look at the message without affecting queue processing.</li> </ul> <p>\u2705 Summary</p> Feature <code>GetMessage</code> (<code>ReceiveMessageAsync</code>) <code>PeekMessage</code> (<code>PeekMessageAsync</code>) Reads next message \u2705 Yes \u2705 Yes Locks message \u2705 Yes \u274c No Removes message \u274c No (until deleted manually) \u274c No Affects visibility \u2705 Temporarily hides message \u274c No impact Use case For processing and removing messages For inspecting messages only"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#8-how-to-read-bulk-messages-from-queues","title":"8. How to read bulk messages from Queues?","text":"<p>\ud83d\udd04 Use <code>ReceiveMessagesAsync(int maxMessages)</code> Method</p> <ul> <li>Azure Queue Storage allows you to retrieve up to 32 messages at a time using:</li> </ul> <pre><code>  ReceiveMessagesAsync(maxMessages: 32)\n  using Azure.Storage.Queues;\n  using System;\n  using System.Threading.Tasks;\n  string connectionString = \"&lt;your_connection_string&gt;\";\n  string queueName = \"myqueue\";\n  var queueClient = new QueueClient(connectionString, queueName);\n  await queueClient.CreateIfNotExistsAsync();\n  // Read up to 10 messages in one call\n  var messages = await queueClient.ReceiveMessagesAsync(maxMessages: 10);\n  foreach (var message in messages.Value)\n  {\n  Console.WriteLine($\"Message: {message.MessageText}\");\n\n    // Process and delete\n    await queueClient.DeleteMessageAsync(message.MessageId, message.PopReceipt);\n  }\n\n</code></pre> <p>\ud83d\udccc Notes</p> <ul> <li>Maximum <code>maxMessages</code> allowed: 32 per request.</li> <li>Each message becomes temporarily invisible (default: 30 seconds) unless deleted.</li> <li>Always call <code>DeleteMessageAsync()</code> after processing each message to prevent reprocessing.</li> </ul> <p>\ud83d\udccb Summary</p> Operation Method Read bulk messages <code>ReceiveMessagesAsync(maxMessages)</code> Delete each message <code>DeleteMessageAsync()</code>"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#9-by-default-in-getmessage-visibility-time-out-___seconds","title":"9. By default In GetMessage visibility time out ___seconds.","text":"<ul> <li>By default, when using <code>GetMessage</code> / <code>ReceiveMessageAsync()</code> in Azure Queue Storage:</li> <li>The visibility timeout is 30 seconds.</li> </ul> <p>\ud83d\udccc What This Means</p> <ul> <li>After receiving a message, it becomes invisible to other consumers for 30 seconds.</li> <li>If not deleted within that time, the message reappears in the queue and may be reprocessed.</li> </ul> <p>\u2705 Summary</p> Setting Default Value Visibility Timeout 30 seconds"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#10-how-to-update-a-message","title":"10. How to update a message?","text":"<p>\ud83d\udd04 How to Update a Message in Azure Queue Storage (C#)</p> <p>\ud83d\udee0\ufe0f Purpose</p> <ul> <li>Use <code>UpdateMessageAsync()</code> to modify the content of an existing message without removing it from the queue.</li> <li>You can also optionally reset the visibility timeout.</li> </ul> <p>\u2705 Example Code</p> <pre><code>using Azure.Storage.Queues;\nusing System;\nusing System.Threading.Tasks;\n\nstring connectionString = \"&lt;your_connection_string&gt;\";\nstring queueName = \"myqueue\";\n\nvar queueClient = new QueueClient(connectionString, queueName);\nawait queueClient.CreateIfNotExistsAsync();\n\n// Step 1: Get the message\nvar received = await queueClient.ReceiveMessageAsync();\n\nif (received.Value != null)\n{\n    string newContent = \"Updated message content\";\n\n    // Step 2: Update the message\n    await queueClient.UpdateMessageAsync(\n        received.Value.MessageId,\n        received.Value.PopReceipt,\n        newContent,\n        TimeSpan.FromSeconds(0) // Make it immediately visible again\n    );\n\n    Console.WriteLine(\"Message updated.\");\n}\n</code></pre> <p>\ud83d\udccc Notes</p> <ul> <li>You must provide both the MessageId and PopReceipt.</li> <li>The PopReceipt changes each time the message is accessed; use the most recent one.</li> <li>You can also update the visibility timeout to delay re-processing.</li> </ul> <p>\ud83d\udccb Summary</p> Operation Method Requirement Update a message <code>UpdateMessageAsync()</code> MessageId + PopReceipt"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#11-what-is-the-messageupdatefield-meant-for","title":"11. What is the MessageUpdateField meant for?","text":"<p>\ud83e\uddfe What is <code>MessageUpdateFields</code> Used For?</p> <p>\ud83d\udcd8 Definition</p> <ul> <li><code>MessageUpdateFields</code> is an enumeration in the Azure Queue SDK.</li> <li>It specifies which parts of a queue message should be updated when calling <code>UpdateMessageAsync()</code>.</li> </ul> <p>\ud83d\udd27 Options</p> Enum Value Meaning <code>None</code> Do not update any field <code>Content</code> Update the message text only <code>VisibilityTimeout</code> Update the visibility timeout only <code>All</code> Update both content and visibility timeout <p>\ud83d\udee0\ufe0f Usage Example</p> <pre><code>await queueClient.UpdateMessageAsync(\n    messageId: msg.MessageId,\n    popReceipt: msg.PopReceipt,\n    messageText: \"Updated content\",\n    visibilityTimeout: TimeSpan.FromSeconds(0),\n    MessageUpdateFields.All\n);\n</code></pre> <p>\ud83d\udccc Notes</p> <ul> <li><code>MessageUpdateFields</code> gives fine-grained control over what gets updated.</li> <li>Defaults to All if not specified.</li> </ul> <p>\u2705 Summary</p> Field Purpose <code>Content</code> Updates the message body <code>VisibilityTimeout</code> Changes the reappearance delay <code>All</code> Updates both content and timeout"},{"location":"Cloud/Azure/Azure_Queues%28Azure%20Queues%20%2C%20visibility%20timeouts%20%2CPeek%20%26%20De-Queue%29/#12-what-is-importance-of-messageid-and-popreceiptid","title":"12. What is importance of MessageId and Popreceiptid?","text":"<p>\ud83c\udd94 Importance of <code>MessageId</code> and <code>PopReceipt</code> in Azure Queues</p> <p>\ud83d\udcd8 What is <code>MessageId</code>?</p> <ul> <li>A unique identifier assigned to each message when it is added to the queue.</li> <li>Used to target a specific message for operations like delete or update.</li> </ul> <p>\ud83d\udd10 What is <code>PopReceipt</code>?</p> <ul> <li>A token (receipt) generated each time a message is retrieved using <code>ReceiveMessageAsync()</code>.</li> <li>Ensures that only the current processor of the message can delete or update it.</li> <li>Acts like a proof-of-access to prevent race conditions in distributed systems.</li> </ul> <p>\ud83d\udee0\ufe0f Why Both Are Required?</p> Reason Explanation Message identification <code>MessageId</code> tells which message to operate on Safe concurrency &amp; versioning <code>PopReceipt</code> ensures that only the latest owner of the message can change it Prevents accidental deletion If a message was retrieved earlier but not yet processed, a new <code>PopReceipt</code> is issued upon re-fetch <p>\u2705 Summary</p> Property Purpose <code>MessageId</code> Uniquely identifies a message <code>PopReceipt</code> Authorizes and secures update/delete"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/","title":"Azure_Services","text":"<p>Azure App services and Cloud serives</p>"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/#1-different-ways-of-publishing-on-azure","title":"1. Different ways of publishing on Azure","text":"<p>\ud83d\ude80 Different Ways of Publishing on Azure</p> <p>1. \ud83d\udd27 Visual Studio</p> <ul> <li>Direct deployment from IDE.</li> <li>Supports:</li> <li>Web Apps</li> <li>Azure Functions</li> <li>Azure App Service</li> <li>Easy for developers with built-in wizards.</li> </ul> <p>\u2705 Best For: Rapid dev/test deployments.</p> <p>** 2. \ud83c\udf10 Azure Portal**</p> <ul> <li>Manual upload or configuration via browser.</li> <li>Use for:</li> <li>ARM template deployments</li> <li>App Service ZIP Deploy</li> <li>Function App package uploads</li> </ul> <p>\u2705 Best For: One-time setups and admin tasks.</p> <p>3. \ud83e\uddf1 Azure CLI</p> <pre><code>az webapp up --name myapp --resource-group myrg --location eastus\n</code></pre> <ul> <li>Scriptable deployment via command line.</li> <li>Works on Linux, macOS, Windows.</li> </ul> <p>\u2705 Best For: DevOps, automation, scripting.</p> <ul> <li> <ol> <li>\ud83d\udce6 Azure DevOps (Pipelines)*</li> </ol> </li> <li> <p>CI/CD pipelines for:</p> </li> <li>Web Apps</li> <li>Azure Functions</li> <li>Containers</li> <li>Supports YAML &amp; Classic UI-based pipelines.</li> </ul> <p>\u2705 Best For: Enterprise-grade DevOps workflows.</p> <p>5. \ud83d\udc19 GitHub Actions</p> <ul> <li>CI/CD workflows using GitHub.</li> <li>Direct integration with Azure using actions like:</li> </ul> <pre><code>- uses: azure/webapps-deploy@v2\n</code></pre> <p>\u2705 Best For: GitHub-hosted repositories.</p> <p>6. \ud83d\udcc1 FTP/SFTP Deployment</p> <ul> <li>Use FTP credentials to manually upload files to Azure Web Apps or App Service.</li> </ul> <p>\u2705 Best For: Legacy or emergency hotfixes.</p> <p>7. \ud83d\udc33 Docker &amp; Containers</p> <ul> <li>Push container images to:</li> <li>Azure Container Registry (ACR)</li> <li>Docker Hub</li> <li>Deploy using:</li> <li>Azure App Service for Containers</li> <li>Azure Kubernetes Service (AKS)</li> </ul> <p>\u2705 Best For: Containerized microservices &amp; scale.</p> <p>8. \ud83e\uddfe ARM Templates / Bicep</p> <ul> <li>Infrastructure-as-Code (IaC) method.</li> <li>Declaratively deploy Azure resources.</li> <li>Repeatable and version-controllable.</li> </ul> <p>\u2705 Best For: Repeatable infra deployments, governance.</p> <p>9. \ud83d\udcdc Terraform</p> <ul> <li>Popular open-source IaC tool.</li> <li>Azure support via the <code>azurerm</code> provider.</li> </ul> <p>\u2705 Best For: Multi-cloud IaC setups.</p> <p>10. \ud83d\udef0\ufe0f Azure Resource Manager REST API</p> <ul> <li>Advanced programmatic deployments using RESTful API endpoints.</li> </ul> <p>\u2705 Best For: Custom apps or tools automating Azure.</p> <p>\u2705 Summary Table</p> Method Type Use Case Visual Studio Manual Dev/testing workflows Azure Portal Manual Admin tasks, monitoring, quick deploy Azure CLI Scripted Automation &amp; scripting Azure DevOps CI/CD Enterprise deployments GitHub Actions CI/CD GitHub-based automation FTP/SFTP Manual Legacy or emergency deploy Docker/Containers Container Microservices, cloud-native apps ARM Templates/Bicep IaC Repeatable, structured deployments Terraform IaC Multi-cloud infrastructure management REST API Programmatic Deep automation and integrations"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/#2-cloud-vs-app-services","title":"2. Cloud vs App services","text":"<p>\u2601\ufe0f Azure Cloud Services vs Azure App Services</p> <p>1. Azure Cloud Services</p> <ul> <li>Type: Platform as a Service (PaaS)</li> <li>Use Case: Hosting scalable, multi-tier web applications and background processing.</li> <li> <p>Architecture:</p> <pre><code>- Uses **Web Roles** (for web apps) and **Worker Roles** (for background tasks).\n- Supports full OS control and startup tasks.\n</code></pre> </li> <li> <p>Management: Requires managing VM instances, OS updates (though managed by Azure), and configuration.</p> </li> <li>Scaling: Manual or automatic scaling of instances.</li> <li>Deployment: Deploys via packages (.cspkg) and configuration (.cscfg).</li> <li>Customization: Higher control over environment and installed software.</li> <li>Ideal for: Legacy cloud apps needing OS-level access, or apps needing custom startup tasks.</li> </ul> <p>2. Azure App Services</p> <ul> <li>Type: Platform as a Service (PaaS)</li> <li>Use Case: Hosting web apps, REST APIs, mobile backends, and serverless functions.</li> <li> <p>Architecture:</p> <pre><code>    - Fully managed app hosting environment.\n    - No OS-level management required.\n</code></pre> </li> <li> <p>Management: Azure manages patching, scaling, and infrastructure.</p> </li> <li>Scaling: Built-in autoscaling and load balancing.</li> <li>Deployment: Supports Git, FTP, ZIP deploy, Azure DevOps, GitHub Actions.</li> <li>Customization: Limited OS access but easy integration with Azure services.</li> <li>Ideal for: Modern web apps, APIs, mobile backends needing fast deployment and easy scaling.</li> </ul> <p>\u2705 Summary Table</p> Feature Azure Cloud Services Azure App Services Service Type PaaS PaaS OS Control Yes No Scaling Manual or auto Built-in autoscaling Deployment Method Packages (.cspkg/.cscfg) Git, FTP, ZIP, DevOps, Actions Management Overhead Higher (VM and OS management) Lower (fully managed) Use Cases Legacy apps, complex setups Modern web apps, APIs, functions Customization High Moderate <p>\ud83d\udccc Notes</p> <ul> <li>Azure Cloud Services is considered a classic model, with many scenarios now better suited to App Services or Azure Kubernetes Service.</li> <li>App Services offer easier and faster deployment with less management effort.</li> </ul>"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/#3-impact-on-resource-groups","title":"3. Impact on resource groups","text":"<p>\ud83d\udd04 Impact on Resource Groups: Azure Cloud Services vs Azure App Services</p> <p>Azure Resource Groups Overview</p> <ul> <li>A Resource Group is a logical container in Azure that holds related resources for an application or workload.</li> <li>It helps manage, deploy, and monitor resources as a single entity.</li> </ul> <p>Impact on Resource Groups</p> Aspect Azure Cloud Services Azure App Services Resource Group Scope Cloud Services (web roles, worker roles) and associated VMs, storage, networking grouped together App Service plan and all related apps grouped within the resource group Granularity Multiple related resources (VMs, roles) bundled, sometimes across resource groups if not carefully planned Typically simpler, with App Service Plan + apps in one resource group Management Complexity Higher \u2014 more resources to manage within or across groups due to multiple roles and VMs Lower \u2014 fewer resource types, easier to organize and maintain Deployment Impact Updates may affect multiple related resources, requiring coordinated deployment App Services deploy as single unit, simplified deployment in resource group Monitoring &amp; Billing Can be complex due to multiple associated resources Easier aggregation of usage and costs per resource group Resource Dependencies Needs careful planning to avoid resource sprawl across groups Usually contained within fewer resources in a group <p>Summary</p> <ul> <li>Both Cloud Services and App Services use resource groups as containers, but Cloud Services often involve more complex resource structures.</li> <li>App Services simplify resource group management by consolidating app components under fewer resources.</li> <li>Proper planning of resource groups is important for cost management, access control, and deployment orchestration.</li> </ul>"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/#4-web-role-worker-roles-and-web-jobs","title":"4. Web role , Worker roles and Web jobs","text":"<p>1. Web Role</p> <ul> <li>Part of Azure Cloud Services (classic PaaS).</li> <li>Designed to host web applications and web APIs.</li> <li>Runs IIS (Internet Information Services) by default.</li> <li>Supports HTTP/HTTPS requests.</li> <li>Can be scaled by increasing the number of instances.</li> <li>Used for front-end web workloads.</li> </ul> <p>2. Worker Role</p> <ul> <li>Also part of Azure Cloud Services.</li> <li>Designed to run background processing or asynchronous tasks.</li> <li>Does not run IIS and cannot serve HTTP requests directly.</li> <li>Runs custom code continuously or triggered by a schedule.</li> <li>Typically used for tasks like:</li> <li>Queue processing</li> <li>Batch jobs</li> <li>Data processing</li> <li>Can be scaled independently of Web Roles.</li> </ul> <p>3. WebJobs</p> <ul> <li>Feature of Azure App Services (modern PaaS).</li> <li>Runs background or scheduled jobs within an App Service Plan.</li> <li>Supports continuous, triggered (on-demand), or scheduled execution.</li> <li>Can run any executable or script (e.g., .exe, PowerShell, Python).</li> <li>Easy to deploy and manage alongside web apps.</li> <li>Ideal for lightweight background tasks without managing separate roles or VMs.</li> </ul> <p>\u2705 Summary Table</p> Feature Web Role Worker Role WebJobs Platform Azure Cloud Services Azure Cloud Services Azure App Services Purpose Host web applications Background processing Background jobs/tasks Runs IIS? Yes No No Deployment Model Package-based (.cspkg) Package-based (.cspkg) Deployed within App Service Scalability Scalable instances Scalable instances Scales with App Service Plan Use Cases Web front-end apps Queues, batch jobs Scheduled/continuous tasks <p>\ud83d\udccc Notes</p> <ul> <li>Web Roles and Worker Roles belong to the classic Azure Cloud Services model and are less commonly used for new projects.</li> <li>WebJobs provide a simpler, more flexible way to run background tasks within App Services.</li> </ul>"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/#5-loosely-coupled-vs-tightly-couples-deployment","title":"5. Loosely coupled vs tightly couples deployment","text":"<p>1. Tightly Coupled Deployment</p> <ul> <li> <p>Definition: Components or services are highly dependent on each other.</p> </li> <li> <p>Characteristics:</p> <pre><code> - Changes in one component often require changes in others.\n - Deployment must be coordinated; often deployed together as a single unit.\n - Harder to scale or update parts independently.\n - Common in monolithic applications.\n</code></pre> </li> <li> <p>Pros:</p> <pre><code>- Simpler initial development.\n- Easier to test as a whole.\n</code></pre> </li> <li> <p>Cons:</p> <pre><code>- Difficult to maintain and evolve.\n- Higher risk of downtime during updates.\n- Poor scalability and flexibility.\n</code></pre> </li> </ul> <p>2. Loosely Coupled Deployment</p> <ul> <li> <p>Definition: Components or services operate independently with minimal dependencies.</p> </li> <li> <p>Characteristics:</p> <pre><code>- Each service/component can be deployed, scaled, updated independently.\n- Communicate through well-defined APIs or messaging.\n- Supports microservices architecture.\n</code></pre> </li> <li> <p>Pros:</p> <pre><code>- Greater flexibility and scalability.\n- Easier maintenance and faster deployments.\n- Fault isolation \u2014 one component failure less likely to impact others.\n</code></pre> </li> <li> <p>Cons:</p> <pre><code>- More complex to design and implement.\n- Requires robust inter-service communication mechanisms.\n</code></pre> </li> </ul> <p>\u2705 Summary Table</p> Aspect Tightly Coupled Loosely Coupled Dependency High dependency between components Minimal dependencies, independent Deployment Monolithic, coordinated deployment Independent deployments Scalability Limited, whole system scales Each component scales individually Flexibility Low, difficult to change parts High, easy to modify and evolve Fault Isolation Poor \u2014 one failure affects entire system Good \u2014 isolated failures Complexity Simpler initially More complex design and infrastructure <p>\ud83d\udccc Notes</p> <pre><code>    - Loosely coupled deployment is preferred for modern cloud-native and microservices applications.\n    - Tightly coupled deployment might still be used for simple or legacy systems.\n</code></pre>"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/#6-configuration-files-in-azure-deployment","title":"6. Configuration files in Azure deployment","text":"<p>Overview</p> <p>Configuration files store settings and parameters that control application behavior and deployment environment without changing code. Azure supports multiple types of configuration files depending on the service and deployment model.</p> <p>Common Configuration Files in Azure</p> File Type Purpose Typical Usage appsettings.json Application-specific settings (e.g., connection strings, API keys) ASP.NET Core and other modern apps web.config IIS and .NET Framework app configuration Traditional ASP.NET apps on Azure App Service ARM Templates Infrastructure-as-Code for deploying Azure resources Declarative deployment of resources and configs Bicep Files Simplified ARM templates Infrastructure deployments with more readable syntax .env files Environment variables for local development Local development and containerized app configuration applicationHost.config IIS server-level settings Azure App Service advanced IIS configuration host.json Azure Functions runtime configuration Configure triggers, logging, retry policies local.settings.json Local settings for Azure Functions development Local development environment variables service.json / config.json Custom app config files App-specific custom configurations <p>Deployment Configuration Approaches</p> <ul> <li>Slot Settings: In Azure App Services, deployment slots can have slot-specific settings to avoid overriding production values.</li> <li>Azure App Configuration: Centralized service to manage app settings and feature flags dynamically.</li> <li>Azure Key Vault: Secure storage for sensitive configuration data like secrets, keys, and certificates.</li> <li>Environment Variables: Supported in all Azure services for injecting configurations dynamically.</li> </ul> <p>Best Practices</p> <ul> <li>Keep sensitive data out of config files; use Key Vault or environment variables.</li> <li>Use separate config files or slots for dev, test, and prod environments.</li> <li>Use ARM/Bicep templates for infrastructure config versioning and automation.</li> <li>Validate configuration files locally before deployment.</li> </ul> <p>\ud83d\udccc Notes</p> <ul> <li>Many Azure SDKs and services have native support to load configs from these files or services.</li> <li>Configurations can be overridden at runtime via portal, CLI, or API for flexibility.</li> </ul>"},{"location":"Cloud/Azure/Azure_Services%28App%20services%20and%20Cloud%20services%29/#7-doing-rdp-in-virtual-machines","title":"7. Doing RDP in virtual machines.","text":"<p>\ud83d\udda5\ufe0f How to Use RDP to Connect to Azure Virtual Machines</p> <p>What is RDP?</p> <ul> <li>Remote Desktop Protocol (RDP) allows you to remotely connect to a Windows virtual machine (VM) and interact with its desktop environment.</li> </ul> <p>Prerequisites</p> <ul> <li>Azure VM must be running Windows OS.</li> <li>VM should have RDP port (TCP 3389) open in its Network Security Group (NSG).</li> <li>You need the VM's public IP address or DNS name.</li> <li>A valid username and password (or SSH key for Linux, but RDP is for Windows).</li> </ul> <p>Steps to Connect via RDP</p> <p>1. Enable RDP Access</p> <ul> <li>When creating the VM, ensure RDP port 3389 is allowed in the Inbound security rules of the VM\u2019s Network Security Group (NSG).</li> <li>If the VM is behind a firewall, make sure port 3389 is open.</li> </ul> <p>2. Obtain Connection Details</p> <ul> <li>Go to the Azure Portal.</li> <li>Navigate to Virtual Machines &gt; Your VM.</li> <li>Find the Public IP address or DNS name.</li> </ul> <p>3. Launch Remote Desktop Client</p> <ul> <li>On Windows: Use built-in Remote Desktop Connection (<code>mstsc.exe</code>).</li> <li>On macOS/Linux: Use Microsoft Remote Desktop app or compatible RDP client.</li> </ul> <p>4. Connect to the VM</p> <ul> <li>Open the RDP client.</li> <li>Enter the public IP address or DNS name.</li> <li>Enter your VM username and password.</li> <li>Click Connect.</li> </ul> <p>5. Accept the Security Certificate (if prompted)</p> <ul> <li>Confirm and proceed with the connection.</li> </ul> <p>Troubleshooting Tips</p> <ul> <li>Port blocked: Verify NSG and firewall settings allow inbound traffic on port 3389.</li> <li>Incorrect credentials: Make sure username and password are correct.</li> <li>VM not running: Ensure the VM is started.</li> <li>Public IP change: If the VM uses a dynamic IP, the address might have changed.</li> </ul> <p>Security Best Practices</p> <ul> <li>Use Just-In-Time (JIT) VM Access to limit RDP exposure.</li> <li>Restrict RDP access by IP address ranges.</li> <li>Use Azure Bastion for secure RDP access without exposing port 3389 publicly.</li> <li>Enable Network Security Groups (NSGs) properly.</li> </ul> <p>Summary</p> Step Description Enable port 3389 Open in NSG for inbound RDP traffic Get public IP/DNS From Azure portal VM overview Use RDP client Windows MSTSC or compatible client Connect &amp; login Provide credentials and accept cert Secure access Use JIT, Bastion, NSGs for security"},{"location":"Cloud/Azure/Azure_Tables/","title":"AzureTables","text":""},{"location":"Cloud/Azure/Azure_Tables/#1-define-azure-tables","title":"1. Define Azure Tables ?","text":"<p>Azure Tables is a NoSQL key-value store that allows applications to store and query large amounts of structured, non-relational data. It is part of Azure Table Storage and is optimized for fast access and high availability.</p> <p>\ud83d\udd0d Key Features</p> <ul> <li>Schema-less data storage: flexible and scalable</li> <li>Stores data as key-value pairs in a structured format (entities and properties)</li> <li>Supports OData protocol for querying</li> <li>PartitionKey and RowKey used for indexing and efficient access</li> <li>Integrated with Azure Storage Account</li> </ul> <p>\ud83e\uddf1 Core Concepts</p> Component Description Table A collection of entities Entity A set of properties, like a row in traditional database Property A name-value pair, like a column in traditional databases PartitionKey Determines the partition (used for scalability and performance) RowKey Uniquely identifies an entity within a partition <p>\ud83d\udce6 Use Cases</p> <ul> <li>Audit logs and telemetry data</li> <li>IoT device data storage</li> <li>User metadata storage</li> <li>Lightweight, scalable app backends</li> </ul> <p>\ud83d\udee0\ufe0f Sample Code (C# using Azure.Data.Tables SDK)</p> <pre><code>var client = new TableClient(\"UseYourConnectionString\", \"MyTable\");\nvar entity = new TableEntity(\"Partition1\", \"Row1\")\n{\n    { \"Name\", \"John Doe\" },\n    { \"Email\", \"john@example.com\" }\n};\nclient.AddEntity(entity);\n\n</code></pre> <p>\ud83d\udd17 Related Services</p> <ul> <li> <p>Azure Cosmos DB Table API (for global distribution and richer querying)</p> </li> <li> <p>Azure Blob Storage (for unstructured data)</p> </li> <li> <p>Azure Queue Storage (for messaging)</p> </li> </ul>"},{"location":"Cloud/Azure/Azure_Tables/#2-explain-the-importance-of-partition-and-row-key","title":"2. Explain the importance of Partition and Row key ?","text":"<p>\ud83d\udd11 Importance of PartitionKey and RowKey in Azure Tables</p> <p>Azure Table Storage uses PartitionKey and RowKey as a composite primary key to uniquely identify each entity and enable fast data access and scalability.</p> <p>\ud83d\udccc PartitionKey</p> <p>\u2705 What is it?</p> <ul> <li>A string value that groups entities within the same partition.</li> <li>Entities with the same PartitionKey are stored together, improving query performance.</li> </ul> <p>\ud83d\ude80 Why is it important?</p> <ul> <li>Enables horizontal partitioning (sharding) for scalability.</li> <li>Efficient range queries and batch transactions are only possible within the same partition.</li> <li>Controls data locality and read/write throughput.</li> </ul> <p>\ud83d\udccc RowKey</p> <p>\u2705 What is it?</p> <ul> <li>A unique identifier for an entity within a given partition.</li> <li>Acts like the primary key in a relational database.</li> </ul> <p>\ud83d\ude80 Why is it important?</p> <ul> <li>Together with PartitionKey, it ensures uniqueness of each entity.</li> <li>Used to directly retrieve a specific entity without scanning the entire table.</li> </ul> <p>\ud83e\udde0 Together: Composite Primary Key</p> Key Purpose Example Value PartitionKey Defines the partition/shard \"CustomerUSA\" RowKey Unique per entity in partition \"Cust12345\" <p>Composite Key = <code>\"CustomerUSA\"</code> + <code>\"Cust12345\"</code></p> <p>\ud83d\udcc8 Performance Benefits</p> <ul> <li>Fast point lookups using both keys.</li> <li>Optimal query routing to the correct partition.</li> <li>Supports batch operations on entities with the same PartitionKey.</li> </ul> <p>\u2757 Best Practices</p> <ul> <li>Design PartitionKey to balance load across partitions.</li> <li>Avoid hot partitions (many writes to the same PartitionKey).</li> <li>Keep PartitionKey and RowKey immutable if possible.</li> </ul> <p>\ud83d\udce6 Sample Retrieval Code (C#)</p> <pre><code>var entity = client.GetEntity&lt;TableEntity&gt;(\"CustomerUSA\", \"Cust12345\");\n</code></pre>"},{"location":"Cloud/Azure/Azure_Tables/#3-azure-tables-are-same-like-rdbms-database-true-or-false","title":"3. Azure tables are same like RDBMS database , true or false ?","text":"<p>\u2753 Are Azure Tables same as RDBMS?</p> <p>Answer: \u274c False</p> <p>Azure Tables and Relational Database Management Systems (RDBMS) serve different purposes and follow different data models.</p> <p>\ud83d\udd04 Key Differences</p> Feature Azure Table Storage RDBMS (e.g., SQL Server, MySQL) Data Model NoSQL, key-value pair (schema-less) Relational, schema-based (tables, rows) Schema Flexible, no enforced schema Fixed schema (defined columns, types) Query Language OData, REST, LINQ SQL (Structured Query Language) Transactions Limited (only within a partition) Full ACID support Relationships Not supported (no JOINs) Supported (foreign keys, joins) Scalability Horizontally scalable Typically vertically scalable Use Case Lightweight, fast-access data (e.g., IoT, logs) Complex relational operations Indexing PartitionKey + RowKey Custom indexes, primary &amp; foreign keys <p>\ud83d\udccc Conclusion</p> <p>Azure Tables are not the same as RDBMS. They are best suited for large-scale, unstructured, and semi-structured data where schema flexibility and horizontal scalability are important.</p>"},{"location":"Cloud/Azure/Azure_Tables/#4-explain-the-architecture-of-azure-tables","title":"4. Explain the architecture of Azure tables ?","text":"<p>\ud83c\udfd7\ufe0f Azure Table Storage Architecture</p> <p>Azure Table Storage is a NoSQL key-value datastore that provides highly available, scalable, and schema-less storage for structured data. It\u2019s part of the Azure Storage Account services.</p> <p>\ud83d\udd27 Core Components</p> <p>1.Storage Account</p> <ul> <li>The entry point for all storage services (Blob, Table, Queue, File).</li> <li>All tables live inside a single storage account.</li> </ul> <p>2. Table</p> <ul> <li>A collection of entities (similar to rows).</li> <li>Tables do not enforce schema; each entity can have different properties.</li> </ul> <p>3. Entity</p> <ul> <li>A single data item (like a row in a relational DB).</li> <li>Consists of a PartitionKey, RowKey, Timestamp, and other custom properties.</li> </ul> <p>4. PartitionKey</p> <ul> <li>Determines how entities are grouped and stored.</li> <li>Used for scalability and load balancing across servers.</li> </ul> <p>5. RowKey</p> <ul> <li>Uniquely identifies an entity within a partition.</li> <li>Together with PartitionKey forms a composite primary key.</li> </ul> <p>\ud83e\udde0 Logical Architecture Diagram</p> <pre><code>+----------------------+\n|  Azure Storage       |\n|  (Storage Account)   |\n+----------+-----------+\n           |\n        Tables\n           |\n    +------+-------+\n    |              |\n+-------+      +-------+\n| Table1|      | Table2|\n+---+---+      +---+---+\n    |              |\n+---v---+      +---v---+\n|Entity1|      |Entity1|\n| P: A  |      | P: B  |\n| R: 1  |      | R: 1  |\n+-------+      +-------+\n</code></pre> <p>How It Works</p> <ul> <li> <p>Client Application uses Azure SDK or REST API to interact with tables.</p> </li> <li> <p>PartitionKey routes the request to the correct partition server.</p> </li> <li> <p>RowKey locates the exact entity within that partition.</p> </li> <li> <p>Azure Storage Backend ensures durability, replication, and availability.</p> </li> </ul> <p>\ud83d\ude80 Key Features of the Architecture</p> <ul> <li> <p>Massive Scalability: Partitions distributed across many servers.</p> </li> <li> <p>High Availability: Automatic replication and geo-redundancy.</p> </li> <li> <p>Efficient Querying: Optimized for point lookups via PartitionKey + RowKey.</p> </li> <li> <p>Cost-Effective: Pay only for what you use (no complex schema or indexing).</p> </li> </ul> <p>\ud83d\udce6 Sample Use Case</p> <p>Azure Table Storage allows you to organize data using <code>PartitionKey</code> and <code>RowKey</code>. Here's a simple example:</p> PartitionKey RowKey Property Value \"USA-Cust\" \"001\" Name John Doe \"USA-Cust\" \"002\" Name Jane Smith \"EU-Cust\" \"003\" Name Erik Muller <p>Each row is uniquely identified and grouped logically by <code>PartitionKey</code>.</p> <p>\ud83d\udee1\ufe0f Security &amp; Access</p> <ul> <li>Supports Shared Access Signature (SAS) for granular delegated access.</li> <li>Integrates with Azure Role-Based Access Control (RBAC) for secure resource-level authorization.</li> <li>Uses HTTPS for all data transmission, ensuring encryption in transit.</li> </ul> <p>\ud83d\udd17 Integration</p> <p>Azure Table Storage can be seamlessly integrated with:</p> <ul> <li>\u2699\ufe0f Azure Functions \u2013 for serverless processing and triggers</li> <li>\ud83d\udd04 Azure Logic Apps \u2013 for low-code automation workflows</li> <li>\ud83c\udf10 Custom APIs and Web Apps \u2013 for building scalable backend services</li> </ul> <p>\u2705 Summary</p> <p>Azure Table Storage uses a partitioned, distributed architecture to store massive volumes of semi-structured data efficiently. It is:</p> <ul> <li>\ud83d\ude80 Fast \u2013 optimized for high-throughput operations</li> <li>\ud83d\udcc8 Scalable \u2013 automatically handles large datasets</li> <li>\ud83d\udcb0 Cost-effective \u2013 pay only for what you use</li> <li>\ud83e\udde9 Flexible \u2013 ideal for scenarios like logging, telemetry, user data, and IoT workloads</li> </ul>"},{"location":"Cloud/Azure/Azure_Tables/#5-how-to-connect-to-azure-tables-using-c-language","title":"5. How to connect to Azure tables using C# language ?","text":"<p>This guide demonstrates how to connect to Azure Table Storage using C# with the <code>Azure.Data.Tables</code> SDK.</p> <p>\ud83d\udce6 Prerequisites</p> <ul> <li>\u2714\ufe0f .NET SDK installed (preferably .NET 6 or later)</li> <li>\u2714\ufe0f An active Azure Storage Account</li> <li>\u2714\ufe0f A table created in Azure (e.g., <code>Customers</code>)</li> <li>\u2714\ufe0f NuGet package: <code>Azure.Data.Tables</code></li> </ul> <p>\ud83d\udce5 Install NuGet Package</p> <p>Use the following command to install the required SDK:</p> <pre><code>dotnet add package Azure.Data.Tables\n</code></pre> <p>Sample Code: Connect and Insert Data</p> <pre><code>using System;\nusing Azure;\nusing Azure.Data.Tables;\n\n// Define the entity model\npublic class CustomerEntity : ITableEntity\n{\n    public string PartitionKey { get; set; }\n    public string RowKey { get; set; }\n    public DateTimeOffset? Timestamp { get; set; }\n    public ETag ETag { get; set; }\n\n    // Custom properties\n    public string Name { get; set; }\n    public string Email { get; set; }\n}\n\nclass Program\n{\n    static void Main()\n    {\n        // \ud83d\udd10 Replace with your actual connection string from Azure Portal\n        string connectionString = \"&lt;Your_Storage_Account_Connection_String&gt;\";\n        string tableName = \"Customers\";\n\n        // Create the TableClient\n        var serviceClient = new TableServiceClient(connectionString);\n        var tableClient = serviceClient.GetTableClient(tableName);\n\n        // Create the table if it doesn't exist\n        tableClient.CreateIfNotExists();\n\n        // Create a new entity\n        var customer = new CustomerEntity\n        {\n            PartitionKey = \"USA\",\n            RowKey = Guid.NewGuid().ToString(),\n            Name = \"John Doe\",\n            Email = \"john@example.com\"\n        };\n\n        // Insert the entity into the table\n        tableClient.AddEntity(customer);\n\n        Console.WriteLine(\"Customer inserted successfully.\");\n    }\n}\n</code></pre> <p>\ud83d\udd10 How to Get the Connection String</p> <ul> <li> <p>Go to the Azure Portal</p> </li> <li> <p>Navigate to your Storage Account</p> </li> <li> <p>Select Access Keys under Security + networking</p> </li> <li> <p>Copy the Connection string</p> </li> </ul> <p>Other Useful Operations</p> <ul> <li>Update or Upsert an Entity</li> </ul> <pre><code>tableClient.UpsertEntity(customer);\n</code></pre> <p>\ud83d\udd0d Query Entities</p> <pre><code>var customers = tableClient.Query&lt;CustomerEntity&gt;(c =&gt; c.PartitionKey == \"USA\");\nforeach (var c in customers)\n{\n    Console.WriteLine($\"{c.Name} - {c.Email}\");\n}\n\n</code></pre> <p>\u274c Delete an Entity</p> <pre><code>tableClient.DeleteEntity(\"USA\", \"rowKeyValue\");\n\n</code></pre> <p>\u2705 Summary</p> <ul> <li> <p>Azure Table Storage is easy to access in C# using the Azure.Data.Tables SDK.</p> </li> <li> <p>You can perform full CRUD operations using simple, strongly-typed models.</p> </li> <li> <p>Use PartitionKey and RowKey for fast and scalable access patterns.</p> </li> </ul>"},{"location":"Cloud/Azure/Azure_Tables/#6-c-entity-classes-should-inherit-from-_-to-receive-azure-records","title":"6. C# Entity classes should inherit from _ to receive Azure records ?","text":"<p>C# entity classes should inherit from _ to receive Azure Table records?</p> <p>\u2705 Answer</p> <p>C# entity classes should implement the <code>ITableEntity</code> interface to work with Azure Table Storage.</p> <pre><code>public class MyEntity : ITableEntity\n{\n    public string PartitionKey { get; set; }\n    public string RowKey { get; set; }\n    public DateTimeOffset? Timestamp { get; set; }\n    public ETag ETag { get; set; }\n\n    // Custom properties\n    public string Name { get; set; }\n    public int Age { get; set; }\n}\n</code></pre> <p>Why ITableEntity?</p> <ul> <li> <p>Implementing ITableEntity enables the entity to:</p> </li> <li> <p>Map directly to a record in Azure Table Storage</p> </li> <li> <p>Support serialization/deserialization</p> </li> <li> <p>Include system properties:</p> <pre><code>- PartitionKey\n\n-RowKey\n\n-Timestamp\n\n-ETag\n</code></pre> </li> </ul> <p>Alternative: Use TableEntity (Dynamic)</p> <p>If you don\u2019t want a strongly-typed class, you can use the built-in TableEntity:</p> <pre><code>var entity = new TableEntity(\"Partition1\", \"Row1\")\n{\n    { \"Name\", \"John\" },\n    { \"Age\", 30 }\n};\n</code></pre> <p>Summary</p> <p>To receive and manipulate records from Azure Table Storage in a strongly-typed manner:</p> <pre><code>- Implement the ITableEntity interface\n\n- Define custom properties for your entity\n\n- Ensure PartitionKey and RowKey are always set\n</code></pre>"},{"location":"Cloud/Azure/Azure_Tables/#7-what-are-tablequery-classes-in-c","title":"7. What are TableQuery classes in C# ?","text":"<p>\ud83d\udd0d TableQuery Classes in C#</p> <p>\ud83d\udcd8 Overview</p> <p>In older Azure SDKs (like <code>Microsoft.Azure.Cosmos.Table</code> or <code>WindowsAzure.Storage</code>), the <code>TableQuery&lt;T&gt;</code> class was used to query entities from Azure Table Storage using LINQ-like expressions or filter strings.</p> <p>\u26a0\ufe0f In the latest SDK (<code>Azure.Data.Tables</code>), <code>TableQuery&lt;T&gt;</code> is no longer used. Instead, queries are made directly via <code>TableClient.Query&lt;T&gt;()</code>.</p> <p>\ud83e\uddfe TableQuery (Legacy SDK) <p>\u2705 Namespace</p> <pre><code>using Microsoft.Azure.Cosmos.Table;\n</code></pre> <p>\ud83e\uddf1 Example Entity</p> <pre><code>public class CustomerEntity : TableEntity\n{\n    public string Name { get; set; }\n    public string Email { get; set; }\n}\n</code></pre> <p>Example Query Using TableQuery</p> <pre><code>CloudTable table = tableClient.GetTableReference(\"Customers\");\n\nTableQuery&lt;CustomerEntity&gt; query = new TableQuery&lt;CustomerEntity&gt;()\n    .Where(TableQuery.GenerateFilterCondition(\"PartitionKey\", QueryComparisons.Equal, \"USA\"));\n\nforeach (CustomerEntity customer in table.ExecuteQuery(query))\n{\n    Console.WriteLine($\"{customer.Name} - {customer.Email}\");\n}\n</code></pre> <p>Deprecated in New SDK (Azure.Data.Tables)</p> <p>In the modern SDK, the approach is simpler and more LINQ-friendly:</p> <p>\u2705 Modern Query Example</p> <pre><code>var tableClient = new TableClient(connectionString, \"Customers\");\n\nvar customers = tableClient.Query&lt;CustomerEntity&gt;(entity =&gt; entity.PartitionKey == \"USA\");\n\nforeach (var customer in customers)\n{\n    Console.WriteLine($\"{customer.Name} - {customer.Email}\");\n}\n</code></pre> <p>\u2705 Summary: TableQuery vs TableClient.Query() Feature TableQuery TableClient.Query() SDK Legacy (<code>Microsoft.Azure.Cosmos.Table</code>) Modern (<code>Azure.Data.Tables</code>) Query Style Filter string or LINQ LINQ-style expression Recommended for New Apps? \u274c No \u2705 Yes"},{"location":"Cloud/Azure/Azure_Tables/#8-how-to-perform-insertupdate-and-delete-using-c-on-azure-tables","title":"8. How to perform insert,update and delete using C# on Azure tables?","text":"<p>\ud83d\udd04 Performing Insert, Update, and Delete Operations on Azure Tables Using C#</p> <p>This guide explains how to perform CRUD operations on Azure Table Storage using the Azure.Data.Tables SDK in C#.</p> <p>\ud83d\udce6 Prerequisites</p> <ul> <li>.NET SDK installed (e.g., .NET 6+)</li> <li>Azure Storage Account with Table service enabled</li> <li>NuGet package: <code>Azure.Data.Tables</code></li> </ul> <p>Install SDK</p> <pre><code>dotnet add package Azure.Data.Tables\n</code></pre> <p>Entity Definition Example</p> <pre><code>using Azure;\nusing Azure.Data.Tables;\nusing System;\n\npublic class CustomerEntity : ITableEntity\n{\n    public string PartitionKey { get; set; }\n    public string RowKey { get; set; }\n    public DateTimeOffset? Timestamp { get; set; }\n    public ETag ETag { get; set; }\n\n    public string Name { get; set; }\n    public string Email { get; set; }\n}\n\n</code></pre> <p>\u2795 Insert Entity</p> <pre><code>var tableClient = new TableClient(connectionString, \"Customers\");\ntableClient.CreateIfNotExists();\n\nvar newCustomer = new CustomerEntity\n{\n    PartitionKey = \"USA\",\n    RowKey = Guid.NewGuid().ToString(),\n    Name = \"John Doe\",\n    Email = \"john@example.com\"\n};\n\n// Insert the entity\ntableClient.AddEntity(newCustomer);\nConsole.WriteLine(\"Entity inserted successfully.\");\n</code></pre> <p>Update or Upsert Entity</p> <ul> <li> <p>Update: Use UpdateEntity to modify an existing entity.</p> </li> <li> <p>Upsert: Use UpsertEntity to insert if not exists, or update if exists.</p> </li> </ul> <pre><code>// Fetch existing entity or create new one\nvar customer = new CustomerEntity\n{\n    PartitionKey = \"USA\",\n    RowKey = \"existing-row-key\",\n    Name = \"John Smith\",\n    Email = \"john.smith@example.com\"\n};\n\n// Update existing entity - must provide ETag for concurrency control\ntableClient.UpdateEntity(customer, customer.ETag, TableUpdateMode.Replace);\n\n// Or Upsert (insert or update)\ntableClient.UpsertEntity(customer);\nConsole.WriteLine(\"Entity updated/upserted successfully.\");\n</code></pre> <p>\u274c Delete Entity</p> <pre><code>string partitionKey = \"USA\";\nstring rowKey = \"existing-row-key\";\n\n// Delete the entity\ntableClient.DeleteEntity(partitionKey, rowKey);\nConsole.WriteLine(\"Entity deleted successfully.\");\n\n</code></pre> <p>Notes</p> <ul> <li> <p>AddEntity throws if the entity already exists.</p> </li> <li> <p>UpdateEntity requires the current ETag for optimistic concurrency.</p> </li> <li> <p>UpsertEntity simplifies insert or update logic.</p> </li> <li> <p>DeleteEntity requires both PartitionKey and RowKey.</p> </li> </ul> <p>\u2705 Summary: Azure Table Storage CRUD Operations in C#</p> Operation Method Notes Insert <code>AddEntity()</code> Adds new entity, fails if exists Update <code>UpdateEntity()</code> Requires ETag, updates existing Upsert <code>UpsertEntity()</code> Insert or update without error Delete <code>DeleteEntity()</code> Deletes by PartitionKey + RowKey <p>Use the <code>Azure.Data.Tables</code> SDK for efficient and simple Azure Table Storage CRUD operations in C#.</p>"},{"location":"Cloud/Azure/Azure_Tables/#9-how-to-do-batch-inserts-using-azure-api","title":"9. How to do batch inserts using Azure API ?","text":"<p>\ud83d\ude80 How to Perform Batch Inserts Using Azure Table Storage API in C#</p> <p>Azure Table Storage supports batch operations to perform multiple insert, update, or delete operations in a single atomic transaction within the same partition.</p> <p>\ud83d\udccc Important Notes</p> <ul> <li>All entities in a batch must share the same PartitionKey.</li> <li>Maximum of 100 entities per batch.</li> <li>Batch operations are atomic: either all succeed or all fail.</li> </ul> <p>\ud83d\udce6 Prerequisites</p> <ul> <li>Azure.Data.Tables SDK installed</li> </ul> <pre><code>dotnet add package Azure.Data.Tables\n</code></pre> <p>Sample Code: Batch Insert</p> <pre><code>using Azure;\nusing Azure.Data.Tables;\nusing System;\nusing System.Collections.Generic;\n\nclass Program\n{\n    static void Main()\n    {\n        string connectionString = \"&lt;Your_Storage_Account_Connection_String&gt;\";\n        string tableName = \"Customers\";\n\n        var tableClient = new TableClient(connectionString, tableName);\n        tableClient.CreateIfNotExists();\n\n        // Create a list of entities with the same PartitionKey\n        var entities = new List&lt;TableEntity&gt;\n        {\n            new TableEntity(\"USA\", Guid.NewGuid().ToString())\n            {\n                { \"Name\", \"John Doe\" },\n                { \"Email\", \"john.doe@example.com\" }\n            },\n            new TableEntity(\"USA\", Guid.NewGuid().ToString())\n            {\n                { \"Name\", \"Jane Smith\" },\n                { \"Email\", \"jane.smith@example.com\" }\n            }\n            // Add more entities as needed (max 100)\n        };\n\n        // Create a batch\n        var batch = new List&lt;TableTransactionAction&gt;();\n\n        foreach (var entity in entities)\n        {\n            batch.Add(new TableTransactionAction(TableTransactionActionType.Add, entity));\n        }\n\n        // Submit the batch\n        tableClient.SubmitTransaction(batch);\n\n        Console.WriteLine(\"Batch insert completed successfully.\");\n    }\n}\n</code></pre> <p>\ud83d\udd0d Explanation</p> <ul> <li> <p>TableTransactionAction defines the action type (Add, Update, Delete, etc.) and the entity.</p> </li> <li> <p>Use SubmitTransaction() to execute the batch.</p> </li> <li> <p>All entities must share the same PartitionKey, otherwise the batch will fail.</p> </li> </ul>"},{"location":"Cloud/Azure/Azure_Tables/#10-what-is-the-consequence-of-not-writing-point-queries","title":"10. What is the consequence of not writing point queries ?","text":"<p>\ud83d\udd0d What Are Point Queries?</p> <ul> <li>Point queries retrieve an entity using its PartitionKey and RowKey.</li> <li>They provide direct, efficient access to a single entity without scanning the table.</li> </ul> <p>\u274c Consequences of Not Using Point Queries</p> Consequence Explanation Poor Performance Queries without PartitionKey and RowKey may scan many partitions, causing high latency and slow responses. Increased Cost Scanning large amounts of data leads to higher transaction and bandwidth costs. Limited Scalability Inefficient queries can overload partitions and reduce overall system scalability. Higher Resource Consumption Increased CPU, memory, and network usage on Azure servers. No Transactional Guarantees Complex queries across partitions can\u2019t benefit from atomic batch transactions. <p>\u2705 Best Practice</p> <p>Always design your queries to use PartitionKey and RowKey whenever possible to ensure:</p> <ul> <li>\u26a1 Fast, direct lookups</li> <li>\ud83d\udcb0 Cost-effective operations</li> <li>\ud83d\udcc8 High scalability and throughput</li> <li>\ud83d\udd12 Consistent and reliable data access</li> </ul> <p>\ud83d\udd17 Additional Notes</p> <ul> <li>If you must query by other properties, consider secondary indexing strategies or Azure Cosmos DB with richer query capabilities.</li> <li>Use filters that include PartitionKey for more efficient scans.</li> </ul>"},{"location":"Cloud/Azure/Azure_Tables/#11-how-does-duplicate-data-increase-search-performance-in-azure-tables","title":"11. How does duplicate data increase search performance in Azure tables ?","text":"<p>\ud83d\udd0d Concept Overview</p> <p>In Azure Table Storage, duplicate data (also known as denormalization) involves storing redundant copies of the same data in multiple entities or partitions to optimize query performance.</p> <p>\u2705 Benefits of Duplicate Data for Search Performance</p> Benefit Explanation Faster Queries Duplicate data enables queries to use PartitionKey + RowKey for direct lookups, avoiding costly scans. Improved Partitioning Data duplication across different partitions allows queries to target a specific partition efficiently. Reduced Joins/Complex Queries Since Azure Tables don\u2019t support joins, duplicating data eliminates the need to combine data from multiple tables. Optimized Access Patterns Data is stored in the shape most suitable for read operations, improving latency and throughput. <p>\u26a0\ufe0f Trade-offs</p> <ul> <li>Increased Storage Costs: More data stored means higher storage costs.</li> <li>Data Consistency Challenges: Keeping duplicated data in sync requires additional logic.</li> <li>Write Amplification: More write operations are needed when data changes.</li> </ul> <p>\ud83d\udccc When to Use Duplicate Data</p> <ul> <li>When your application requires fast read/query performance.</li> <li>When query patterns are known and predictable.</li> <li>When you can handle the additional complexity in data maintenance.</li> </ul> <p>\ud83d\udcdd Summary</p> <p>Denormalization via duplicate data is a common pattern in Azure Table Storage to optimize for fast, scalable searches by leveraging efficient partition keys and avoiding costly queries.</p>"},{"location":"Cloud/Azure/Azure_Tables/#12-how-does-storing-aggregate-data-benefit-in-terms-of-performance","title":"12. How does storing aggregate data benefit in terms of performance ?","text":"<p>\ud83d\udd0d Concept Overview</p> <p>Storing aggregate data means precomputing and saving summarized or combined data (e.g., totals, counts, averages) to optimize query performance and reduce computation overhead at runtime.</p> <p>\u2705 Performance Benefits of Storing Aggregate Data</p> Benefit Explanation Faster Queries Queries retrieving aggregate values are instantaneous, avoiding expensive real-time calculations. Reduced Compute Load Offloads aggregation work from the application or database to a pre-calculation step. Lower Latency Results are ready to serve, improving user experience in time-sensitive scenarios. Improved Scalability Reduces resource consumption on storage and compute layers, allowing handling of more requests. <p>\u26a0\ufe0f Considerations</p> <ul> <li>Aggregates must be updated carefully when underlying data changes to avoid stale results.</li> <li>Additional storage is required to maintain aggregate tables or fields.</li> <li>Logic complexity increases to keep aggregates in sync.</li> </ul> <p>\ud83d\udccc Use Cases</p> <ul> <li>Dashboards showing totals, averages, or counts.</li> <li>Real-time analytics.</li> <li>Reporting systems needing fast response times.   \ud83d\udcdd Summary</li> </ul> <p>Pre-storing aggregate data significantly boosts performance by enabling quick access to summarized information, reducing on-the-fly processing and resource usage.</p>"},{"location":"Cloud/Azure/Azure_Tables/#13-when-should-we-use-compound-key-in-azure-tables","title":"13. When should we use compound key in Azure tables ?","text":"<p>\ud83d\udccc What is a Compound Key?</p> <ul> <li>In Azure Table Storage, the compound key is the combination of:</li> <li>PartitionKey (defines the partition)</li> <li>RowKey (unique within the partition)</li> <li>Together, they uniquely identify each entity.</li> </ul> <p>\u2705 When to Use Compound Keys</p> Scenario Explanation Uniquely Identify Entities When you need a unique identifier that involves multiple attributes (e.g., region + user ID). Efficient Data Partitioning To logically group related entities while keeping uniqueness. Fast Lookups To enable point queries that are highly efficient and cost-effective. Batch Operations When you want to perform atomic batch transactions on entities in the same partition. Hierarchical Data Modeling Represent hierarchical or composite relationships via keys. <p>\u26a0\ufe0f Considerations</p> <ul> <li>Choose <code>PartitionKey</code> to maximize scalability by balancing workload across partitions.</li> <li>Choose <code>RowKey</code> to uniquely identify items within a partition.</li> <li>Avoid overly large partitions that can become performance bottlenecks.</li> </ul> <p>\ud83d\udcdd Summary</p> <p>Use compound keys (<code>PartitionKey</code> + <code>RowKey</code>) in Azure Tables whenever you need unique, efficient, and scalable entity identification that supports fast queries and batch transactions.</p>"},{"location":"Cloud/Azure/Azure_Tables/#14-what-is-egt-can-egt-be-done-across-tables","title":"14. What is EGT , can EGT be done across tables ?","text":"<p>\ud83d\udcd8 Definition</p> <ul> <li>ETag (Entity Tag) is a system-generated string value that represents the version of an entity in Azure Table Storage.</li> <li>It is used for optimistic concurrency control to prevent conflicts when multiple clients update the same entity concurrently.</li> </ul> <p>\ud83d\udd27 How ETag Works</p> <ul> <li>When an entity is retrieved, it comes with an ETag.</li> <li>When updating or deleting, the ETag is sent back to ensure the entity has not changed since retrieval.</li> <li>If the ETag does not match the current version in the table, the operation fails with a 412 Precondition Failed error.</li> </ul> <p>\u2753 Can ETag be Used Across Tables?</p> <ul> <li>No, ETag is scoped to a single entity in a specific table.</li> <li>It cannot be used across different tables because each table manages its own entities and versions independently.</li> <li>Concurrency control with ETag applies only to individual entities within their own tables.</li> </ul> <p>\u2705 Summary</p> Aspect Details What is ETag? A version identifier for concurrency control Purpose Prevents conflicting updates on the same entity Scope Single entity within a single table Cross-table use? \u274c Not supported <p>\ud83d\udd17 Additional Notes</p> <ul> <li>Use ETag with <code>UpdateEntity</code> or <code>DeleteEntity</code> operations to ensure safe concurrency.</li> <li>For cross-entity or cross-table transactions, consider application-level logic or alternative storage solutions.</li> </ul>"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/","title":"Azure_WebJobs","text":"<p>WebJob and background processing.</p>"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/#1-why-webjobs","title":"1. Why WebJobs?","text":"<p>\ud83e\udd14 Why Use Azure WebJobs?</p> <p>Azure WebJobs provide an easy way to run background tasks or scripts in the context of an Azure App Service (Web App). They simplify running scheduled or continuous jobs without managing separate infrastructure.</p> <p>Key Benefits of WebJobs</p> <ul> <li>Integrated with App Services: Runs inside the same environment as your web app, sharing resources and scaling.</li> <li> <p>Simple to Deploy: Deploy code or scripts alongside your web app without separate VM or service.</p> </li> <li> <p>Multiple Execution Modes:</p> <pre><code>- Continuous: Runs continuously in the background.\n- Triggered: Runs on demand or scheduled (using Azure Scheduler or TimerTriggers).\n</code></pre> </li> <li> <p>Supports Various Languages: Run .NET, PowerShell, Python, Node.js, and more.</p> </li> <li>Cost-Effective: No need for separate VMs or services for background processing.</li> <li>Easy Management: Use Azure Portal or CLI to monitor, start, stop WebJobs.</li> <li>Ideal for:<pre><code> - Processing queues or blobs.\n - Scheduled maintenance tasks.\n - Data import/export.\n - Any background processing tasks related to your web app.\n</code></pre> </li> </ul> <p>When to Use WebJobs?</p> <ul> <li>When you want to add background or scheduled jobs without extra infrastructure.</li> <li>When your background task is tightly coupled with your web app lifecycle.</li> <li>For lightweight to moderate background processing needs.</li> </ul> <p>Alternatives</p> <ul> <li> <p>For more complex or independent background processing, consider:</p> </li> <li> <p>Azure Functions (serverless)</p> </li> <li>Azure Logic Apps</li> <li>Azure Service Bus with worker roles or microservices</li> </ul>"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/#2-types-of-webjobs-triggered-continuos","title":"2. Types of Webjobs (Triggered, Continuos.)","text":"<p>\ud83c\udf00 Types of Azure WebJobs</p> <p>Azure WebJobs run background tasks in an Azure App Service environment and come in two main types:</p> <p>1. Continuous WebJobs</p> <ul> <li>Runs: Continuously in the background as long as the App Service is running.</li> <li> <p>Use Cases: Real-time processing, long-running tasks, listening to queues or events.</p> </li> <li> <p>Behavior:</p> <pre><code>- Starts automatically when the App Service starts.\n- Runs in an infinite loop or waits for events/messages.\n</code></pre> </li> <li> <p>Example: Monitoring a queue for incoming messages to process.</p> </li> </ul> <p>2. Triggered WebJobs</p> <ul> <li>Runs: On-demand or on a schedule.</li> <li> <p>Use Cases: Periodic tasks, batch jobs, maintenance scripts.</p> </li> <li> <p>Behavior:</p> <pre><code>- Manually triggered via Azure Portal, API, or CLI.\n- Scheduled using Azure Scheduler or TimerTrigger (if using WebJobs SDK).\n</code></pre> </li> <li> <p>Example: Nightly database cleanup or report generation.</p> </li> </ul> <p>Summary Table</p> Type Execution Mode Trigger Common Use Cases Continuous WebJob Always running Starts automatically Real-time queue processing Triggered WebJob Runs on demand Manual or scheduled Scheduled maintenance tasks <p>** Notes**</p> <ul> <li>Both types support various programming languages and scripts (.exe, .cmd, PowerShell, Python, etc.).</li> <li>Continuous jobs consume resources continuously, so use carefully to avoid excess cost.</li> </ul>"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/#3-view-logs-of-webjobs","title":"3. View logs of WebJobs.","text":"<p>\ud83d\udcc4 How to View Logs of Azure WebJobs</p> <p>Azure WebJobs provide built-in logging capabilities to monitor the execution and diagnose issues. Logs can be accessed through several methods.</p> <p>Ways to View WebJob Logs</p> <p>1. Azure Portal</p> <ul> <li>Navigate to your App Service in the Azure Portal.</li> <li>Select WebJobs under the Settings section.</li> <li> <p>Click on the specific WebJob.</p> </li> <li> <p>Use the Logs tab to view output logs, including:</p> <pre><code>- Invocation history\n- Console output\n- Error messages\n</code></pre> </li> </ul> <p>** 2. Kudu Console**</p> <ul> <li>Go to your App Service's Advanced Tools (Kudu) by navigating to:</li> </ul>"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/#4-cron-expressions","title":"4. CRON expressions","text":"<p>\u23f0 CRON Expressions Overview</p> <p>What is a CRON Expression?</p> <ul> <li>A CRON expression is a string used to define schedules for running tasks periodically.</li> <li>Commonly used in schedulers like Azure WebJobs, Azure Functions TimerTrigger, Linux cron jobs, Jenkins, etc.</li> </ul> <p>CRON Format (Standard 6 or 7 fields)</p> Field Allowed Values Description Seconds 0-59 Seconds Minutes 0-59 Minutes Hours 0-23 Hours (24-hour format) Day of Month 1-31 Day of the month Month 1-12 or JAN-DEC Month Day of Week 0-6 or SUN-SAT Day of the week (0=Sun) Year (optional) 1970\u20132099 Year (optional) <p>Special Characters</p> Character Meaning * All values , Value list separator - Range of values / Step values (increments) ? No specific value (Day of Month or Day of Week) L Last day of the week or month W Nearest weekday # Nth day of the week of the month <p>Example CRON Expressions</p> Expression Meaning <code>0 0 * * * *</code> Every hour at minute 0 and second 0 <code>0 0 12 * * ?</code> Every day at 12:00 PM <code>0 15 10 ? * MON-FRI</code> At 10:15 AM every weekday (Mon-Fri) <code>0 0/5 * * * *</code> Every 5 minutes <code>0 0 0 1 * ?</code> At midnight on the 1st day of every month <p>Notes</p> <ul> <li>Azure Functions TimerTrigger uses a 6-field CRON (seconds included).</li> <li>Make sure to test CRON expressions with online tools like crontab.guru.</li> </ul>"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/#5-always-on-importance-in-continous-webjob","title":"5. Always on importance in continous webjob","text":"<p>\ud83d\udd04 Importance of \"Always On\" in Continuous WebJobs</p> <p>What is \"Always On\"?</p> <ul> <li>\"Always On\" is an App Service setting that keeps your web app (and its associated WebJobs) running continuously, even when there is no HTTP traffic.</li> <li>By default, Azure may idle or sleep your app after a period of inactivity to save resources.</li> </ul> <p>Why is \"Always On\" Important for Continuous WebJobs?</p> <ul> <li> <p>Continuous WebJobs require the app to be always running to ensure the background job executes without interruption.</p> </li> <li> <p>If \"Always On\" is disabled:</p> <pre><code>- The App Service may go idle after inactivity.\n- Continuous WebJobs will **stop running** until the app is accessed again.\n- This leads to missed processing, delays, or failure in real-time tasks.\n</code></pre> </li> </ul> <p>Key Points</p> Aspect Explanation Keeps App Active Prevents app from unloading due to inactivity Ensures WebJob Runs Continuous WebJobs keep running as expected Required for Real-time Processing Necessary for tasks like queue monitoring, event processing Only Applies to App Service Plans Not available for Free or Shared tiers <p>How to Enable \"Always On\"</p> <ol> <li>Go to your App Service in the Azure Portal.</li> <li>Under Settings, select Configuration.</li> <li>Select the General settings tab.</li> <li>Set Always On to On.</li> <li>Save the changes.</li> </ol> <p>Summary</p> <ul> <li>For reliable execution of Continuous WebJobs, always enable Always On.</li> <li>Without it, your continuous background tasks may be paused, causing unexpected downtime.</li> </ul>"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/#6-storage-accounts-in-webjobs","title":"6. Storage accounts in Webjobs","text":"<p>\ud83d\udcbe Storage Accounts in Azure WebJobs</p> <p>Overview</p> <ul> <li>Azure WebJobs often rely on Azure Storage Accounts to enable messaging, state management, and logging.</li> <li> <p>Storage accounts provide:</p> </li> <li> <p>Queues: For triggering and coordinating WebJobs.</p> </li> <li>Blobs: For storing input/output data.</li> <li>Tables: For storing metadata or state information.</li> </ul> <p>Role of Storage Accounts in WebJobs</p> Feature Purpose Queue Storage WebJobs SDK can listen to Azure Queues to trigger job execution (e.g., process messages). Blob Storage Input/output bindings for file processing or status data. Table Storage Store execution logs, checkpoints, or metadata for WebJobs. Logging &amp; Monitoring WebJobs SDK uses storage accounts to persist logs and runtime status. <p>Why Use Storage Accounts with WebJobs?</p> <ul> <li>Triggering: Many WebJobs run in response to queue messages or blob changes.</li> <li>Scalability: Storage queues decouple producers and consumers for load leveling.</li> <li>Durability: Durable storage ensures reliable processing and retry mechanisms.</li> <li>Monitoring: Tracks job executions and failures for diagnostics.</li> </ul> <p>Configuration</p> <ul> <li>WebJobs SDK requires a connection string to an Azure Storage Account.</li> <li>This is usually set in the <code>appsettings.json</code> or Azure Portal under Application Settings as <code>AzureWebJobsStorage</code>.</li> <li>Example connection string format:</li> </ul> <p>DefaultEndpointsProtocol=https;AccountName=youraccount;AccountKey=yourkey;EndpointSuffix=core.windows.net</p> <p>Best Practices</p> <ul> <li>Use a separate storage account for WebJobs to isolate workload and improve security.</li> <li>Enable soft delete and lifecycle management on storage to protect and manage data.</li> <li>Monitor storage account usage and scale accordingly.</li> </ul> <p>Summary</p> Aspect Description Storage Types Queues, Blobs, Tables Purpose Triggering, state management, logging Configuration Connection string via <code>AzureWebJobsStorage</code> Best Practice Use dedicated storage accounts for WebJobs"},{"location":"Cloud/Azure/Azure_WebJobs%28WebJob%20and%20background%20processing.%29/#7-publish-website-and-webjob-using-vs","title":"7. Publish website and webjob using VS","text":"<p>\ud83d\ude80 Publish Website and WebJob Using Visual Studio</p> <p>Prerequisites</p> <ul> <li>Visual Studio installed with Azure development workload.</li> <li>Azure subscription with an existing App Service.</li> <li>Website and WebJob projects ready in your solution.</li> </ul> <p>Steps to Publish Website and WebJob</p> <p>1. Prepare Your WebJob</p> <ul> <li>Add your WebJob project to the solution (Console App recommended).</li> <li>Right-click the WebJob project &gt; Properties &gt; Ensure output type is Console Application.</li> <li> <p>Add the WebJob project as a WebJob to your Web App project:</p> <pre><code>- Right-click your **Web App project** &gt; **Add** &gt; **Existing Project as Azure WebJob** (in newer VS versions, or manually configure).\n</code></pre> </li> <li> <p>In Web App project\u2019s properties, under Azure WebJobs tab, ensure the WebJob is configured to deploy with the web app.</p> </li> </ul> <p>2. Publish the Web App and WebJob</p> <ul> <li>Right-click on your Web App project &gt; Publish.</li> <li>Choose Azure as the target.</li> <li>Select the appropriate App Service or create a new one.</li> <li>In the Publish settings, make sure the WebJob is included.</li> <li>Click Publish.</li> </ul> <p>3. Verify Deployment</p> <ul> <li>After publishing, go to the Azure Portal.</li> <li>Navigate to your App Service.</li> <li>Under Settings, click WebJobs.</li> <li>You should see your WebJob listed and its status (Running, Stopped).</li> <li>You can view logs, start, stop, or run the WebJob manually from the portal.</li> </ul> <p>Tips</p> <ul> <li>For Continuous WebJobs, ensure Always On is enabled in the App Service settings.</li> <li>For Triggered WebJobs, you can run them manually or schedule via Azure Scheduler.</li> <li>Use Application Settings in Azure Portal to configure connection strings or environment variables.</li> <li>Use Publish Profiles to save and reuse publish configurations.</li> </ul> <p>Summary Table</p> Step Description Add WebJob project Create and prepare console app as WebJob Link WebJob to Web App Configure in Web App project to deploy WebJob Publish Web App Use Visual Studio Publish wizard to deploy both Verify in Azure Portal Check WebJob status and logs"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/","title":"AzureBlobs(block,append,page)","text":""},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#1-what-are-the-different-types-of-blobs","title":"1. What are the different types of Blobs?","text":"<p>Azure Blob Storage supports different types of blobs optimized for various scenarios and performance characteristics.</p> <p>1. Block Blobs</p> <ul> <li>Purpose: Store text and binary data.</li> <li>Use Cases: Documents, images, videos, backups, etc.</li> <li>Characteristics:<pre><code>- Composed of blocks of data.\n- Supports efficient uploads and updates by uploading blocks independently.\n- Maximum size: up to about 190.7 TiB.\n- Ideal for streaming and storing files.\n</code></pre> </li> </ul> <p>2. Append Blobs</p> <ul> <li>Purpose: Optimized for append operations.</li> <li>Use Cases: Logging, auditing, telemetry data.</li> <li>Characteristics:<pre><code>- Composed of blocks but optimized for append-only operations.\n- New data can only be added to the end of the blob.\n- Supports efficient append with low latency.\n- Maximum size: up to about 195 GiB.\n</code></pre> </li> </ul> <p>3. Page Blobs</p> <ul> <li>Purpose: Store random-access files.</li> <li>Use Cases: Virtual hard disks (VHDs), databases, or any scenario requiring frequent read/write operations.</li> <li>Characteristics:<pre><code>- Composed of 512-byte pages.\n- Supports random read/write operations.\n-  Maximum size: up to 8 TiB.\n- Suitable for scenarios requiring frequent updates.\n</code></pre> </li> </ul> <p>\u2705 Summary</p> Blob Type Use Case Max Size Key Feature Block Blob General file storage ~190.7 TiB Efficient block uploads Append Blob Append-only data (logs) ~195 GiB Optimized for append operations Page Blob Random read/write (VHDs, DBs) Up to 8 TiB Supports random access pages <p>\ud83d\udd17 Additional Info</p> <ul> <li>Blob storage is scalable, durable, and accessible via REST APIs, SDKs, and Azure Portal.</li> <li>Choose blob type based on your application needs.</li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#2-in-which-scenarios-we-should-use-which-type-of-blobs","title":"2. In which scenarios we should use which type of blobs?","text":"<p>Azure Blob Storage offers Block Blobs, Append Blobs, and Page Blobs, each suited to different scenarios.</p> <ol> <li>Block Blobs</li> </ol> <p>Use When:</p> <ul> <li>You need to store large files like images, videos, documents, backups.</li> <li>Your workload involves uploading or downloading entire files.</li> <li>You want efficient uploads with support for resumable transfers.</li> <li>You are streaming media or files.</li> <li>File size can be very large (up to ~190.7 TiB).</li> </ul> <p>Common Scenarios:</p> <ul> <li>Media storage (videos, photos)</li> <li>Document storage (PDFs, Word files)</li> <li>Backup and restore systems</li> <li>Content delivery</li> </ul> <p>2. Append Blobs</p> <p>Use When:</p> <ul> <li>You need to append data frequently without modifying existing content.</li> <li>You want to store logs, audit trails, or telemetry data.</li> <li>Data grows by adding new records continuously.</li> <li>Order of records is important.</li> </ul> <p>Common Scenarios:</p> <ul> <li>Application logging</li> <li>Event and telemetry capture</li> <li>Audit and compliance trails</li> </ul> <p>3. Page Blobs</p> <p>Use When:</p> <ul> <li>You require random read/write access to the data.</li> <li>You need to store virtual hard disks (VHDs) for Azure Virtual Machines.</li> <li>Your application involves frequent updates to specific byte ranges.</li> <li>Low-latency I/O operations are necessary.</li> </ul> <p>Common Scenarios:</p> <ul> <li>Azure VM disks (VHD storage)</li> <li>Databases requiring page-level updates</li> <li>Random access files that need frequent modifications</li> </ul> <p>\u2705 Summary Table</p> Blob Type Best Use Case Key Feature Block Blob Large file storage, streaming Efficient block uploads Append Blob Logging, appending data Append-only writes Page Blob Random read/write, VHD storage Random access pages <p>\ud83d\udd0d Tips</p> <ul> <li>Choose Block Blobs for most general-purpose file storage.</li> <li>Use Append Blobs only if you need append-only semantics.</li> <li>Use Page Blobs when low-latency random access or VM disks are required.</li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#3-can-you-explain-the-working-of-block-blobs","title":"3. Can you explain the working of block blobs ?","text":"<p>\ud83d\udd0d What Are Block Blobs?</p> <ul> <li>Block blobs store text and binary data as a collection of blocks.</li> <li>Each block is identified by a block ID (a Base64 string).</li> <li>Blocks can be uploaded independently and in parallel.</li> </ul> <p>\u2699\ufe0f How Block Blobs Work</p> <p>1. Uploading Blocks</p> <ul> <li>The client splits the blob data into smaller chunks called blocks.</li> <li>Each block is uploaded separately with a unique block ID.</li> <li>Blocks can be uploaded in any order and in parallel, improving upload speed.</li> </ul> <p>2. Committing Blocks</p> <ul> <li>After uploading all blocks, the client sends a commit block list request.</li> <li>The commit operation assembles the blocks into the final blob in the specified order.</li> <li>Only committed blocks form the visible blob.</li> </ul> <p>3. Downloading Blobs</p> <ul> <li>When a blob is requested, Azure serves the committed blocks in order.</li> <li>Supports streaming large blobs efficiently.</li> </ul> <p>4. Modifying Blobs</p> <ul> <li>To modify a block blob, upload new blocks and commit an updated block list.</li> <li>Uncommitted blocks expire after a week if not committed.</li> </ul> <p>\u2705 Benefits of Block Blob Architecture</p> <ul> <li>Efficient Uploads: Upload large blobs in parallel smaller chunks.</li> <li>Resumable Uploads: If upload is interrupted, only missing blocks need to be uploaded.</li> <li>Flexible: Update parts of a blob by committing new block lists.</li> <li>Scalable: Supports blobs up to approximately 190.7 TiB.</li> </ul> <p>\ud83d\udd17 Summary</p> Step Description Upload Blocks Upload blocks independently with IDs Commit Blocks Assemble blocks into final blob Download Blob Read committed blocks in order Modify Blob Upload new blocks and commit list <p>Block blobs are ideal for storing files where large size, efficient upload, and streaming are important.</p>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#4-what-is-the-size-of-individual-block-blob","title":"4. What is the size of individual block blob ?","text":"<p>\u2705 Key Size Limits</p> Aspect Limit Maximum size of a single block 4000 MiB (4,000 MiB or ~4 GB) Maximum number of blocks per blob 50,000 blocks Maximum size of a block blob Approximately 190.7 TiB (50,000 blocks \u00d7 4 MiB each) <p>\ud83d\udd0d Explanation</p> <ul> <li>A block blob is made up of multiple blocks.</li> <li>Each block can be up to 4000 MiB in size.</li> <li>You can upload up to 50,000 blocks per blob.</li> <li>Therefore, the maximum total size of a block blob is roughly 190.7 TiB.</li> <li>Blocks are combined when committing the blob.</li> </ul> <p>\ud83d\udccc Notes</p> <ul> <li>For optimal performance, Azure recommends using smaller blocks (e.g., 4 MiB).</li> <li>Large block sizes reduce the number of blocks but may affect upload reliability.</li> <li>Uncommitted blocks expire after 7 days.</li> </ul> <p>\ud83d\udd17 Summary</p> Limit Value Max block size 4000 MiB (4 GB) Max number of blocks 50,000 Max block blob size ~190.7 TiB"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#5-how-many-block-blobs-can-be-accommodated-in-one-blob","title":"5. How many block blobs can be accommodated in one blob ?","text":"<p>\ud83d\udccc Answer</p> <ul> <li>A single block blob in Azure Blob Storage can contain up to 50,000 blocks.</li> <li>Each block can be up to 4000 MiB (approximately 4 GiB) in size.</li> </ul> <p>\ud83e\uddfe Calculation</p> Parameter Value Max blocks/blob 50,000 Max size/block 4000 MiB (~4 GiB) Max blob size 50,000 \u00d7 4000 MiB = ~190.7 TiB <p>\ud83e\udde0 Note: You do not store multiple block blobs inside another blob. Each block blob is a single object made up of multiple blocks.</p> <p>\u2753 Clarification</p> <ul> <li>A block blob \u2260 a container of multiple blobs.</li> <li>Rather, it is a single blob object composed of multiple blocks.</li> <li>You can store multiple block blobs in a blob container, not inside each other.</li> </ul> <p>\u2705 Summary</p> Concept Value Max blocks per block blob 50,000 Max block size 4000 MiB (\u22484 GiB) Max total blob size ~190.7 TiB Block blobs inside a blob \u274c Not possible (1 blob = 1 object)"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#6-explain-the-hierarchal-structure-of-account-container-and-blobs","title":"6. Explain the hierarchal structure of account , container and blobs ?","text":"<p>\ud83c\udfd7\ufe0f Hierarchical Structure of Azure Blob Storage</p> <p>Azure Blob Storage is organized in a three-level hierarchy to manage and access data efficiently.</p> <p>\ud83d\udcd8 1. Storage Account</p> <ul> <li>The top-level namespace for all your Azure Storage data.</li> <li>Each storage account provides a unique namespace in Azure for your data.</li> <li>It can contain multiple containers.</li> <li>URL format:https://.blob.core.windows.net/ <p>\ud83d\udcc2 2. Container</p> <ul> <li>A container organizes a set of blobs.</li> <li>It's similar to a directory or folder in traditional file systems.</li> <li>All blobs must be stored in a container.</li> <li>You can define access levels at the container level: Private, Blob (public read), or Container (full public read).</li> <li>URL format: https://.blob.core.windows.net/ <p>\ud83d\udcc4 3. Blob</p> <ul> <li>The actual file or object you store in Azure Blob Storage.</li> <li>Can be of types: Block Blob, Append Blob, or Page Blob.</li> <li>Blobs are stored inside containers and can be large binary/text files.</li> <li>Each blob is uniquely identified within its container by its name.</li> <li>URL format: https://.blob.core.windows.net// <pre><code>Storage Account\n\u2502\n\u251c\u2500\u2500 Container 1\n\u2502 \u251c\u2500\u2500 blob-a.txt\n\u2502 \u251c\u2500\u2500 image1.jpg\n\u2502 \u2514\u2500\u2500 video.mp4\n\u2502\n\u251c\u2500\u2500 Container 2\n\u2502 \u251c\u2500\u2500 log-2024.txt\n\u2502 \u2514\u2500\u2500 snapshot.json\n\u2502\n\u2514\u2500\u2500 Container 3\n\u2514\u2500\u2500 invoice.pdf\n</code></pre> <p>\u2705 Summary Table</p> Level Description Example URL Storage Account Root namespace in Azure <code>https://mystorage.blob.core.windows.net/</code> Container Logical grouping of blobs <code>https://mystorage.blob.core.windows.net/photos/</code> Blob Actual data file (object) <code>https://mystorage.blob.core.windows.net/photos/image.jpg</code> <p>\ud83d\udd10 Access Control</p> <ul> <li>Authentication and authorization are typically applied at the account or container level.</li> <li>You can use Shared Access Signatures (SAS), Azure RBAC, or access policies.</li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#7-explain-private-container-and-blob-access-levels","title":"7. Explain private , container and blob access levels ?","text":"<p>Azure Blob Storage provides three levels of public access for containers and blobs to control visibility and security.</p> <p>1. \ud83d\udeab Private (No Anonymous Access)</p> <ul> <li> <p>Description:   No anonymous access is allowed. Only authorized users (via Azure AD, SAS tokens, or account keys) can access data.</p> </li> <li> <p>Use Case:   Ideal for secure enterprise data, private backups, confidential documents.</p> </li> <li> <p>Who Can Access?   Only users/applications with proper credentials.</p> </li> </ul> <p>2. \ud83d\udcc2 Container (Full Public Read Access for Container and Blobs)</p> <ul> <li> <p>Description:   Allows anonymous read access to both the blobs and the list of blobs in the container.</p> </li> <li> <p>Use Case:   Useful for public media galleries, static websites, or shared public datasets.</p> </li> <li> <p>Who Can Access?   Anyone with the container URL can:</p> </li> <li>List all blobs in the container.</li> <li>Read blob content.</li> </ul> <p>3. \ud83d\udcc4 Blob (Public Read Access for Blobs Only)</p> <ul> <li> <p>Description:   Allows anonymous read access to blobs, but not to the container metadata or list of blobs.</p> </li> <li> <p>Use Case:   Share specific files (like PDFs or images) without exposing the full container.</p> </li> <li> <p>Who Can Access?   Anyone with the blob\u2019s direct URL can read the blob. They cannot list blobs in the container.</p> </li> </ul> <p>\u2705 Summary Table</p> Access Level Container Listing Blob Read Access Authentication Required Private \u274c No \u274c No \u2705 Yes Container \u2705 Yes \u2705 Yes \u274c No (anonymous allowed) Blob \u274c No \u2705 Yes \u274c No (anonymous allowed) <p>\ud83d\udd10 How to Set Access Level</p> <p>You can configure access levels:</p> <ul> <li>From Azure Portal (Container \u2192 Change access level)</li> <li>Using Azure CLI:   <code>bash   az storage container set-permission --name &lt;container-name&gt; --public-access &lt;level&gt;</code></li> </ul> <p>\ud83d\udee1\ufe0f Best Practice</p> <ul> <li> <p>Use Private for sensitive and internal data.</p> </li> <li> <p>Use Blob for publicly shared files.</p> </li> <li> <p>Use Container with caution, only for fully public use cases.</p> </li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#8-what-are-the-broader-steps-to-create-blobs","title":"8. What are the broader steps to create blobs ?","text":"<p>Creating blobs in Azure involves setting up the environment, organizing your storage hierarchy, and uploading your data.</p> <p>\u2705 Step-by-Step Overview 1. \ud83d\udd10 Create an Azure Storage Account</p> <ul> <li>Go to Azure Portal.</li> <li>Click on Storage Accounts \u2192 Create.</li> <li>Choose a resource group, provide a unique name, region, and performance tier.</li> <li>Select BlobStorage or General-purpose v2 as the account type.</li> </ul> <p>2. \ud83d\udcc2 Create a Blob Container</p> <ul> <li>Navigate to the storage account you just created.</li> <li>Select Containers \u2192 + Container.</li> <li>Provide a container name (lowercase only).</li> <li>Set the access level: <code>Private</code>, <code>Blob</code>, or <code>Container</code>.</li> </ul> <p>3. \ud83d\udce4 Upload a Blob</p> <ul> <li>Go to the container.</li> <li>Click Upload.</li> <li>Select the file you want to upload (supports images, videos, docs, etc.).</li> <li>Choose block blob (default) unless you need append/page blob.</li> <li>Click Upload.</li> </ul> <p>4. \ud83d\udd17 Access or Secure Your Blob</p> <ul> <li> <p>Once uploaded, you\u2019ll get a URL like: https://.blob.core.windows.net// <li> <p>Use SAS tokens, Azure RBAC, or container policies to secure or share the blob.</p> </li> <p>\ud83d\udccc Optional: Programmatic Upload (C# Example)</p> <pre><code>var blobServiceClient = new BlobServiceClient(\"&lt;connection-string&gt;\");\nvar containerClient = blobServiceClient.GetBlobContainerClient(\"mycontainer\");\nvar blobClient = containerClient.GetBlobClient(\"myfile.txt\");\n\nusing FileStream uploadFileStream = File.OpenRead(\"myfile.txt\");\nblobClient.Upload(uploadFileStream, true);\nuploadFileStream.Close();\n</code></pre> <p>\ud83d\udccb Summary \u2013 Steps to Create Blobs in Azure</p> Step Description 1 Create a storage account 2 Create a container 3 Upload blob (file) 4 Access or secure the blob URL"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#9-what-is-the-importance-of-singleblobthreshholdinbytes","title":"9. What is the importance of \u201cSingleBlobThreshHoldInBytes\u201d ?","text":"<p>\ud83d\udcd8 What Is It?</p> <p><code>SingleBlobUploadThresholdInBytes</code> is a configuration setting in Azure SDKs that determines:</p> <p>\ud83d\udd27 The maximum size (in bytes) of a blob that can be uploaded in a single operation. If the blob size exceeds this threshold, it is automatically uploaded in blocks (i.e., multipart upload).</p> <p>\u2699\ufe0f How It Works</p> <ul> <li>If a file is smaller than or equal to the threshold:</li> <li>It is uploaded as a single PUT operation.</li> <li>If a file is larger than the threshold:</li> <li>It is uploaded in multiple blocks using <code>PutBlock</code> + <code>PutBlockList</code>.</li> </ul> <p>\u2705 Why It's Important</p> Benefit Explanation Optimized Performance Uploading small files in one request reduces overhead. Automatic Block Uploads Automatically switches to block upload for large files. Better Reliability Large uploads benefit from retry logic per block, improving fault tolerance. Customization Developers can tune this value to balance performance and reliability. <p>\ud83d\udccc Default Value</p> <ul> <li>Typically: 256 MiB (268,435,456 bytes)</li> <li>Can be configured in the SDK for performance tuning:   <code>csharp   new BlobClientOptions   {       SingleBlobUploadThreshold = 100 * 1024 * 1024 // 100 MB   };</code> \ud83d\udd10 Best Practice</li> <li> <p>For large files: Use lower thresholds to enable block uploads with retry support.</p> </li> <li> <p>For small files: Use higher thresholds to avoid overhead of block management.</p> </li> </ul> <p>\ud83d\udcdd Summary \u2013 SingleBlobUploadThresholdInBytes</p> Setting Name <code>SingleBlobUploadThresholdInBytes</code> Purpose Switch between single PUT and block upload automatically Default ~256 MiB Benefit Optimizes performance, fault tolerance, and control"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#10-what-happens-when-we-specify-streamwritesizeinbytes","title":"10. What happens when we specify \u201cStreamWriteSizeInBytes\u201d ?","text":"<p>\ud83d\udcd8 Definition</p> <p><code>StreamWriteSizeInBytes</code> is a configuration property used in Azure Blob Storage SDKs to define:</p> <p>\ud83d\udca1 The size of data chunks written to the blob when uploading a stream (e.g., FileStream or MemoryStream).</p> <p>\u2699\ufe0f How It Works</p> <ul> <li>When uploading a blob from a stream (e.g., <code>UploadAsync(Stream)</code>), Azure writes the stream in chunks of size = <code>StreamWriteSizeInBytes</code>.</li> <li>This value controls the buffer size used internally during the upload process.</li> </ul> <p>\u2705 Why It Matters</p> Impact Explanation Performance Tuning Larger sizes reduce number of write calls and can improve upload speed. Memory Consumption Larger buffer sizes use more memory per upload thread. Network Efficiency Helps balance between network throughput and responsiveness. Parallel Uploads Can be combined with concurrency settings for better performance. <p>\ud83d\udccc Typical Values</p> <ul> <li>Common default: <code>4 MiB</code> to <code>8 MiB</code> (depends on SDK version)</li> <li>Recommended range: 2 MiB to 100 MiB</li> <li>Example in C#:</li> </ul> <p><code>csharp   var options = new BlockBlobClientOptions   {       TransferOptions = new StorageTransferOptions       {           InitialTransferSize = 8 * 1024 * 1024,           // 8 MiB           MaximumTransferSize = 8 * 1024 * 1024,           // 8 MiB           MaximumConcurrency = 4       }   };</code></p> <p>\u26a0\ufe0f Important Notes</p> <ul> <li>Increasing this too much may lead to high memory usage in concurrent uploads.</li> <li>Too small can result in slower uploads due to too many small network operations.</li> </ul> <p>\ud83d\udcdd Summary \u2013 StreamWriteSizeInBytes</p> Setting Name <code>StreamWriteSizeInBytes</code> Purpose Chunk size for writing blob data from streams Default (SDK-dependent) ~4\u20138 MiB Effect Impacts upload performance and memory usage Best Practice Tune based on file size, memory, and network"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#11-differentiate-between-singleblobthreshholdinbytes-vs-streamwritesizeinbytes","title":"11. Differentiate between \u201cSingleBlobThreshHoldInBytes\u201d VS \u201cStreamWriteSizeInBytes\u201d ?","text":"<p>\ud83d\udd0d Difference Between <code>SingleBlobUploadThresholdInBytes</code> vs <code>StreamWriteSizeInBytes</code></p> <p>These two settings control how data is uploaded to Azure Blob Storage, but they serve different purposes.</p> <p>\ud83d\udccc Key Differences</p> Aspect <code>SingleBlobUploadThresholdInBytes</code> <code>StreamWriteSizeInBytes</code> Purpose Determines when to switch from single upload to block upload Sets the chunk size when writing from a stream Applies To Entire blob upload Streaming uploads (e.g., FileStream, MemoryStream) Triggers Block Upload \u2705 Yes \u2013 if blob size exceeds threshold \u274c No \u2013 just controls buffer size per chunk Performance Impact Impacts upload mode selection Impacts memory usage and write performance Typical Use Case Full file uploads (e.g., <code>UploadAsync(filePath)</code>) Uploading large blobs via streams Default Value ~256 MiB ~4\u20138 MiB Risk of High Memory Use Low \u2013 switches upload method High \u2013 if set too large in multi-threaded scenarios <p>\ud83d\udcdd Summary</p> <ul> <li>Use <code>SingleBlobUploadThresholdInBytes</code> to decide when Azure uses block upload.</li> <li>Use <code>StreamWriteSizeInBytes</code> to control buffer size when uploading from a stream.</li> <li>Both settings can be fine-tuned for performance and scalability.</li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#12-when-should-we-use-putblock-and-putblocklist","title":"12. When should we use \u201cPutBlock\u201d and \u201cPutBlockList\u201d ?","text":"<p>\u2699\ufe0f When to Use <code>PutBlock</code> and <code>PutBlockList</code> in Azure Blob Storage</p> <p>Azure Block Blob uploads use two key operations when working with blocks:</p> <p>1. \ud83d\udccc <code>PutBlock</code></p> <ul> <li>Purpose: Uploads a single block of data to Azure Blob Storage.</li> <li> <p>Use Case:</p> <pre><code> - When you want to upload a **part (block)** of a large blob independently.\n - Supports **parallel or chunked uploads** of large files.\n - Blocks can be uploaded **in any order**.\n</code></pre> </li> <li> <p>Note:</p> <pre><code> - Blocks uploaded with `PutBlock` are **not committed** until `PutBlockList` is called.\n - Uncommitted blocks expire after 7 days if not committed.\n</code></pre> </li> </ul> <p>2. \ud83d\udccc <code>PutBlockList</code></p> <ul> <li>Purpose: Commits a list of blocks to assemble the final blob.</li> <li> <p>Use Case:</p> <pre><code>- After all blocks are uploaded via `PutBlock`, call `PutBlockList` to **commit** the blocks.\n- Defines the **final order** of blocks in the blob.\n- Makes the blob **available and readable**.\n</code></pre> </li> <li> <p>Note:</p> <pre><code>- Only blocks included in the `PutBlockList` become part of the committed blob.\n- Blocks not included remain uncommitted and will expire.\n</code></pre> </li> </ul> <p>\ud83d\udd04 Workflow Summary</p> Step Action 1. Upload Blocks Use <code>PutBlock</code> to upload individual blocks in any order 2. Commit Blocks Use <code>PutBlockList</code> to commit the block list in desired order <p>\u2705 When to Use</p> Scenario Operation Upload large blobs in chunks <code>PutBlock</code> Finalize and assemble the blob <code>PutBlockList</code> Resume interrupted uploads Use <code>PutBlock</code> to upload missing blocks, then <code>PutBlockList</code> <p>\ud83d\udcdd Summary</p> <ul> <li><code>PutBlock</code> uploads blocks independently (uncommitted).</li> <li><code>PutBlockList</code> commits the blocks to form the final blob.</li> <li>Together they enable reliable, resumable, and parallel uploads of large blobs.</li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#13-how-can-we-get-committed-and-uncommitted-blobs","title":"13. How can we get committed and uncommitted blobs ?","text":"<p>Azure Blob Storage allows you to manage and inspect both committed and uncommitted blocks within a block blob.</p> <p>Definitions</p> <ul> <li>Committed blocks: Blocks that have been committed by a <code>PutBlockList</code> operation and form the visible blob.</li> <li>Uncommitted blocks: Blocks uploaded by <code>PutBlock</code> but not yet committed. They are invisible until committed and expire after 7 days.</li> </ul> <p>\u2699\ufe0f Retrieving Blocks</p> <p>Using Azure SDK (C# Example)</p> <pre><code>var blobClient = containerClient.GetBlockBlobClient(\"myblob.txt\");\nvar blockList = await blobClient.GetBlockListAsync(BlockListTypes.All);\n\nConsole.WriteLine(\"Committed Blocks:\");\nforeach (var block in blockList.Value.CommittedBlocks)\n{\n    Console.WriteLine($\"Block ID: {block.Name}, Size: {block.Size}\");\n}\n\nConsole.WriteLine(\"Uncommitted Blocks:\");\nforeach (var block in blockList.Value.UncommittedBlocks)\n{\n    Console.WriteLine($\"Block ID: {block.Name}, Size: {block.Size}\");\n}\n</code></pre> <p>Using REST API</p> <ul> <li> <p>Use the Get Block List operation with blocklisttype=all query parameter.</p> </li> <li> <p>The response contains two sections:  and . <p>\u2705 Summary Table</p> Block Type Description Visibility Lifetime Committed Blocks Part of the committed blob Visible to reads Persist until overwritten Uncommitted Blocks Uploaded but not yet committed blocks Invisible Expire after 7 days if uncommitted <p>\ud83d\udcdd Notes</p> <ul> <li>Use block list info to manage resumable uploads.</li> <li>Uncommitted blocks consume storage but are not visible.</li> <li>Commit blocks with <code>PutBlockList</code> to finalize the blob.</li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#14-how-can-we-download-block-blob","title":"14. How can we download block blob ?","text":"<p>You can download a block blob from Azure Blob Storage using various SDKs. Below is a common example using C# Azure SDK.</p>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#using-azure-sdk-for-net-c","title":"Using Azure SDK for .NET (C#)","text":"<pre><code>using Azure.Storage.Blobs;\nusing System.IO;\nusing System.Threading.Tasks;\n\npublic async Task DownloadBlockBlobAsync(string connectionString, string containerName, string blobName, string downloadFilePath)\n{\n    // Create BlobServiceClient\n    BlobServiceClient blobServiceClient = new BlobServiceClient(connectionString);\n\n    // Get the container client\n    BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient(containerName);\n\n    // Get the blob client\n    BlobClient blobClient = containerClient.GetBlobClient(blobName);\n\n    // Download the blob's contents to a local file\n    await blobClient.DownloadToAsync(downloadFilePath);\n}\n</code></pre> <p>Steps Explained</p> <ul> <li> <p>Create a BlobServiceClient with your storage account connection string.</p> </li> <li> <p>Get the container client using the container name.</p> </li> <li> <p>Get the blob client using the blob name.</p> </li> <li> <p>Call DownloadToAsync() to download the blob content to a local file path.</p> </li> </ul> <p>Notes</p> <ul> <li> <p>You can also download to a stream instead of a file by using DownloadToAsync(Stream).</p> </li> <li> <p>For large blobs, consider using asynchronous or chunked downloads for better performance.</p> </li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#15-explain-the-importance-of-streamminimumreadsizeinbytes","title":"15. Explain the importance of \u201cStreamMinimumReadSizeInBytes\u201d ?","text":"<p><code>StreamMinimumReadSizeInBytes</code> is a configuration setting used in Azure Blob Storage SDKs that defines:</p> <p>\ud83d\udd27 The minimum size of data (in bytes) to read from the blob stream in one operation.</p> <p>\u2699\ufe0f How It Works</p> <ul> <li>When reading data from a blob stream, the SDK reads data in chunks.</li> <li>This setting controls the minimum buffer size used for each read operation.</li> <li>Helps optimize the balance between number of read calls and memory usage.</li> </ul> <p>\u2705 Why It Matters</p> Benefit Explanation Performance Optimization Larger read sizes reduce the number of network calls, improving throughput. Memory Management Controls how much memory is allocated for buffering during reads. Smooth Streaming Ensures efficient streaming of blob data with fewer pauses. <p>\ud83d\udccc Typical Values</p> <ul> <li>Default values vary by SDK but usually range from 256 KB to 4 MB.</li> <li>Can be adjusted based on application needs and available memory.</li> </ul> <p>\ud83d\udd10 Best Practice</p> <ul> <li>Increase this value for large sequential reads to improve performance.</li> <li>Decrease it if memory usage needs to be minimized or for small reads.</li> </ul> <p>\ud83d\udcdd Summary</p> Setting Name <code>StreamMinimumReadSizeInBytes</code> Purpose Minimum size to read from blob stream per call Effect Balances network calls and memory use Default (SDK-dependent) Typically 256 KB to 4 MB Best Practice Tune based on workload size and memory constraints"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#16-how-to-use-appendblockblobs","title":"16. How to use AppendBlockBlobs ?","text":"<p>What is an Append Blob?</p> <ul> <li>Append blobs are optimized for append operations.</li> <li>You can only add data to the end of the blob.</li> <li>Ideal for logging, audit trails, and telemetry data.</li> </ul> <p>Using Azure SDK for .NET</p> <pre><code>using Azure.Storage.Blobs.Specialized;\nusing System.Text;\nusing System.Threading.Tasks;\n\nvar connectionString = \"&lt;your_connection_string&gt;\";\nvar containerName = \"mycontainer\";\nvar blobName = \"logfile.txt\";\n\nvar blobServiceClient = new BlobServiceClient(connectionString);\nvar containerClient = blobServiceClient.GetBlobContainerClient(containerName);\n\n// Get the Append Blob client\nvar appendBlobClient = containerClient.GetAppendBlobClient(blobName);\n</code></pre> <p>Create the Append Blob (if not exists)</p> <pre><code>await appendBlobClient.CreateIfNotExistsAsync();\n</code></pre> <p>Append Data to the Blob</p> <pre><code>string logMessage = \"This is a new log entry.\\n\";\nbyte[] byteArray = Encoding.UTF8.GetBytes(logMessage);\n\nusing var stream = new MemoryStream(byteArray);\nawait appendBlobClient.AppendBlockAsync(stream);\n</code></pre> <p>\ud83d\udcdd Notes</p> <ul> <li>Append blobs only support append operations; you cannot modify existing content.</li> <li>Each append operation adds data to the end of the blob.</li> <li>Append blobs have a maximum size of about 195 GiB.</li> </ul> <p>\ud83d\udccb Summary</p> Operation Method Create Append Blob <code>CreateIfNotExistsAsync</code> Append Data <code>AppendBlockAsync</code>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#17-can-we-update-appendblockblob","title":"17. Can we update appendBlockBlob ?","text":"<p>\u274c Can We Update Append Blobs?</p> <ul> <li>No, append blobs do not support updating or modifying existing data once written.</li> <li>You can only append new data to the end of an append blob using <code>AppendBlockAsync</code>.</li> <li>To modify content, you must:<pre><code>- Download the entire blob,\n- Make changes locally,\n- Then upload a new blob (replace the old one).\n</code></pre> </li> </ul> <p>\ud83d\udd04 Summary</p> Operation Supported? Append data \u2705 Yes Update existing data \u274c No Delete data Only by deleting the entire blob <p>Append blobs are designed primarily for append-only scenarios like logs and audit trails.</p>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#18-explain-writepages-and-read-methods-of-page-blobs","title":"18. Explain \u201cWritePages\u201d and \u201cRead\u201d methods of page blobs ?","text":"<p>\ud83d\udcc4 What is a Page Blob?</p> <ul> <li>A Page Blob stores data optimized for random read/write operations.</li> <li>Data is organized in 512-byte pages.</li> <li>Ideal for virtual machine disks, databases, or scenarios requiring frequent partial updates.</li> </ul> <p>\u270d\ufe0f <code>WritePages</code> Method</p> <ul> <li>Purpose: Writes data to specific pages (offsets) within the page blob.</li> <li>Key Features:</li> <li>Allows random writes at specified byte offsets.</li> <li>Requires data length and offset to be aligned to 512-byte boundaries.</li> <li>Efficient for updating parts of large blobs without rewriting the entire blob.</li> <li>Usage Example:</li> </ul> <pre><code>// Parameters\nlong offset = 0; // must be multiple of 512\nbyte[] data = ...; // data to write, multiple of 512 bytes\n\nusing MemoryStream stream = new MemoryStream(data);\nawait pageBlobClient.WritePagesAsync(stream, offset);\n</code></pre> <p>\ud83d\udcd6 Read Method</p> <ul> <li> <p>Purpose: Reads data from specified ranges (offset and length) of the page blob.</p> </li> <li> <p>Key Features:</p> <pre><code>    - Supports random reads from any offset.\n\n    - Reads can be partial, allowing efficient access to portions of large blobs.\n\n    - Offset and length should align to 512-byte boundaries for best performance.\n</code></pre> </li> </ul> <p>Usage Example:</p> <pre><code>long offset = 0;    // byte offset to start reading from\nlong length = 512;  // number of bytes to read\n\nvar response = await pageBlobClient.ReadAsync(new HttpRange(offset, length));\nusing Stream stream = response.Value.Content;\n// Process stream as needed\n\n</code></pre> <p>\u2705 Summary</p> Method Purpose Key Points WritePages Write data to specific pages Random, aligned writes in 512-byte units Read Read data from specific ranges Random reads, efficient partial access <p>\ud83d\udccc Notes</p> <ul> <li>Both methods require 512-byte alignment for offsets and lengths.</li> <li>Page blobs support efficient random read/write, unlike block blobs which are optimized for sequential writes.</li> </ul>"},{"location":"Cloud/Azure/Azure_blobs%28block%2Cappend%2Cpage%29/#19-what-does-seek-method-do-of-page-blob","title":"19. What does \u201cSeek\u201d method do of page blob ?","text":"<ul> <li>The <code>Seek</code> method is used when working with streams that represent page blobs.</li> <li>It changes the current position of the stream pointer to a specified location.</li> <li>This allows reading from or writing to different parts of the page blob without sequential access.</li> </ul> <p>\u2699\ufe0f How It Works</p> <ul> <li> <p>The method takes an offset and a reference point (origin) which can be:</p> </li> <li> <p><code>Begin</code> \u2014 offset is from the start of the stream.</p> </li> <li><code>Current</code> \u2014 offset is relative to the current position.</li> <li> <p><code>End</code> \u2014 offset is from the end of the stream.</p> </li> <li> <p>After <code>Seek</code>, the next read or write operation will start from the new position.</p> </li> </ul> <p>\ud83d\udccc Importance in Page Blob Operations</p> <ul> <li>Page blobs support random access read/write.</li> <li>Using <code>Seek</code> lets you efficiently navigate to the exact page (512-byte aligned position) you want to read or write.</li> <li>Enables partial updates without needing to load or rewrite the entire blob.</li> </ul> <p>\ud83d\udcdd Example (C#)</p> <pre><code>Stream pageBlobStream = await pageBlobClient.OpenWriteAsync(true);\npageBlobStream.Seek(1024, SeekOrigin.Begin); // Move to byte offset 1024\n// Next write will start at byte 1024\n</code></pre> <p>\u2705 Summary</p> Method Purpose Seek Moves the stream position to a specified offset, enabling random access to the blob data"},{"location":"Cloud/Azure/Azure_storage/","title":"AzureStorage","text":""},{"location":"Cloud/Azure/Azure_storage/#1-differentiate-between-resource-manager-and-classic","title":"1. Differentiate between resource manager and classic ?","text":"<p>Azure provides two deployment models for resources: Resource Manager (ARM) and Classic. Below is a comparison of the two models:</p> <ol> <li>Overview</li> </ol> Feature Resource Manager (ARM) Classic Deployment Model Introduced 2014 (Modern model) Pre-2014 (Legacy model) Deployment Method Declarative templates (ARM templates) Manual or scripted (PowerShell, CLI) Resource Group Resources are grouped logically using Resource Groups No concept of Resource Groups Management Unified and consistent management via Azure Portal, PowerShell, CLI, SDKs Limited and inconsistent management <ol> <li>Key Differences</li> </ol> Aspect Resource Manager (ARM) Classic Deployment Grouping Resources are deployed and managed as a group Resources are managed individually RBAC Support Supports Role-Based Access Control (RBAC) Limited RBAC (applies at subscription level) Tagging Support Tags can be applied to resources and groups No tagging support Template Support Supports Infrastructure-as-Code with ARM templates No native template support Consistency Consistent and predictable deployments Less consistent across services Dependencies Supports defining dependencies between resources No dependency handling Resource Locks Supports locking resources to prevent changes Not available Policy Integration Supports Azure Policy Not supported <ol> <li>Use Cases</li> </ol> <p>ARM Model:</p> <ul> <li>Recommended for all new resources and services</li> <li>Supports CI/CD, DevOps, automation, and modern governance3</li> </ul> <p>Classic Model:</p> <ul> <li>Legacy workloads or services not yet migrated</li> <li> <p>Not recommended for new deployments</p> </li> <li> <p>Migration</p> </li> </ul> <p>Microsoft encourages migration from Classic to ARM using tools like:</p> <ul> <li>Azure Migrate</li> <li>Classic to ARM migration tools</li> <li>Manual re-deployment via ARM templates</li> </ul> <p>\u26a0\ufe0f Note: Some services no longer support the classic model and may be retired.</p> <ol> <li>Conclusion</li> </ol> <p>ARM provides a modern, scalable, and manageable way of working with Azure resources, enabling automation, consistency, and governance. The classic model is deprecated and should be avoided in new projects.</p> <p>References</p> <ul> <li>Microsoft Learn: Azure Resource Manager Overview</li> <li>Migration from Classic to ARM</li> </ul>"},{"location":"Cloud/Azure/Azure_storage/#2-explain-the-difference-between-blobs-files-queues-and-table","title":"2. Explain the difference between blobs , files , queues and table ?","text":"<p>Azure Storage offers different services to cater to various storage needs. Below is a breakdown of the key differences among Blob Storage, File Storage, Queue Storage, and Table Storage.</p> <ol> <li>Overview</li> </ol> Storage Type Description Blob Storage Stores unstructured data like documents, images, videos, and backups File Storage Shared file system accessible via SMB protocol, similar to a file share Queue Storage Messaging store for reliable inter-service communication Table Storage NoSQL key-value storage for structured, non-relational data <ol> <li>Key Differences</li> </ol> Feature Blob Storage File Storage Queue Storage Table Storage Use Case Images, backups, videos, logs Shared file access, lift-and-shift apps Messaging between services Storing large sets of structured data Data Type Unstructured File-based Text messages Structured data in key-value format Access Protocol HTTP/HTTPS via REST API SMB (Server Message Block) REST API REST API File Structure Containers \u2192 Blobs Shares \u2192 Directories \u2192 Files Queues \u2192 Messages Tables \u2192 Entities Max Message Size N/A N/A 64 KB per message Up to 1 MB per entity Max Storage Size Depends on tier (up to petabytes) Up to 100 TiB per share Up to 500 TiB Up to 500 TiB Access Control Azure RBAC, Shared Access Signatures Azure RBAC, Shared Access Signatures Shared Access Signatures (SAS) Shared Access Signatures (SAS) Latency Low Low Very Low (used for message queuing) Low <ol> <li>When to Use What?</li> </ol> <p>\u2705 Blob Storage</p> <ul> <li>Storing images, videos, PDFs, large logs</li> <li>Backup and disaster recovery</li> <li>Data lake for analytics (with ADLS Gen2)</li> </ul> <p>\u2705 File Storage</p> <ul> <li>Migrating legacy apps that use file shares</li> <li>Centralized file storage accessible via SMB</li> <li>Lift-and-shift file-based workloads</li> </ul> <p>\u2705 Queue Storage</p> <ul> <li>Asynchronous task messaging</li> <li>Decoupling services in distributed systems</li> <li>Processing orders, logs, or events in the background</li> </ul> <p>\u2705 Table Storage</p> <ul> <li>Large-scale structured data with simple query needs</li> <li>Device logs, metadata storage, user profiles</li> <li> <p>Cheaper alternative to full database if joins aren't needed</p> </li> <li> <p>Summary</p> </li> </ul> Storage Type Best For Key Benefit Blob Media files, unstructured data Cost-effective, scalable File SMB file share replacement Easy migration of legacy systems Queue Service communication Reliable async messaging Table NoSQL structured storage Fast and cheap key-value access <ol> <li> <p>References</p> </li> <li> <p>Azure Blob Storage Documentation</p> </li> <li>Azure Files Documentation</li> <li>Azure Queue Storage Documentation</li> <li>Azure Table Storage Documentation</li> </ol>"},{"location":"Cloud/Azure/Azure_storage/#3-differentiate-between-general-storage-v1-vs-v2-vs-blob","title":"3. Differentiate between general storage v1 vs v2 vs blob ?","text":"<p>Azure offers different types of storage accounts to meet various needs. The three primary types are:</p> <ul> <li>General Purpose v1 (GPv1)</li> <li>General Purpose v2 (GPv2)</li> <li>Blob Storage Account</li> </ul> <p>Below is a detailed comparison:</p> <ol> <li>Overview</li> </ol> Storage Account Type Description GPv1 Legacy account supporting all storage services with older pricing model GPv2 Recommended default; supports latest features, access tiers, and pricing Blob Storage Specialized for blob data only, with access tiers but limited services <ol> <li>Feature Comparison</li> </ol> Feature / Capability GPv1 GPv2 (Recommended) Blob Storage Account Supported Services Blobs, Files, Queues, Tables Blobs, Files, Queues, Tables Blobs only Access Tiers Not supported Hot, Cool, Archive Hot, Cool, Archive Performance Tiers Standard, Premium (limited) Standard, Premium (full) Standard only Features (Soft delete, etc.) Limited Full support (latest features) Partial (Blob-related only) Pricing Model Older pricing Latest pricing (cost-effective) Latest pricing Lifecycle Management Not supported Supported Supported Redundancy Options LRS, GRS, RA-GRS, ZRS LRS, GRS, RA-GRS, ZRS LRS, GRS, RA-GRS Use Cases Legacy systems New apps, general workloads Blob-heavy workloads <ol> <li>Key Differences</li> </ol> <p>\u2705 General Purpose v1</p> <ul> <li>Older version with limited support for new features</li> <li>No access tiers (hot/cool/archive)</li> <li>Higher transaction costs compared to GPv2</li> <li>Use only if backward compatibility is required</li> </ul> <p>\u2705 General Purpose v2 (GPv2)</p> <ul> <li>Default and recommended option</li> <li>Supports all storage types with full feature set</li> <li>Includes access tiers, lifecycle management, soft delete, and Azure Data Lake Gen2</li> <li>Cost-optimized pricing model</li> </ul> <p>\u2705 Blob Storage Account</p> <ul> <li>Only supports blob data</li> <li>Includes support for access tiers (hot, cool, archive)</li> <li>Does not support queues, files, or tables</li> <li> <p>Good for scenarios dealing only with blobs like media storage, backup, archives</p> </li> <li> <p>When to Use Which?</p> </li> </ul> Scenario Recommended Account Type General purpose storage (blobs, files, etc.) General Purpose v2 (GPv2) Blob-only workloads (e.g., backup/archive) Blob Storage Legacy systems requiring older features General Purpose v1 (GPv1) <ol> <li>Summary Table</li> </ol> Feature / Type GPv1 GPv2 (Recommended) Blob Storage Supports Blob Storage \u2705 \u2705 \u2705 Supports File Storage \u2705 \u2705 \u274c Supports Queue Storage \u2705 \u2705 \u274c Supports Table Storage \u2705 \u2705 \u274c Access Tiers (Hot/Cool/Archive) \u274c \u2705 \u2705 Pricing Model Legacy Latest (Flexible) Latest Lifecycle Management \u274c \u2705 \u2705 Azure Data Lake Gen2 \u274c \u2705 \u274c <ol> <li> <p>Conclusion</p> </li> <li> <p>Always use GPv2 for most new workloads \u2014 it's the most flexible and future-proof.</p> </li> <li>Use Blob Storage Account only for blob-specific needs when no other storage services are required.</li> <li> <p>Avoid GPv1 unless you're working with legacy systems or need compatibility.</p> </li> <li> <p>References</p> </li> <li> <p>Azure Storage Account Overview</p> </li> <li>Azure Storage Pricing</li> </ol>"},{"location":"Cloud/Azure/Azure_storage/#4-when-should-we-select-hot-access-tier-or-cold-access-tier","title":"4. When should we select hot access tier or cold access tier ?","text":"<p>Azure Blob Storage provides multiple access tiers to optimize cost based on how frequently your data is accessed. Choosing the right tier can significantly reduce storage costs.</p> <p>\ud83d\udd25 Hot Access Tier</p> <p>\u2705 When to Use:</p> <ul> <li>Data is accessed frequently (read/write)</li> <li>Requires low latency and high throughput</li> <li>Mission-critical, real-time processing applications</li> </ul> <p>\ud83d\udce6 Typical Use Cases:</p> <ul> <li>Active application data (e.g., media content, transaction logs)</li> <li>Data in use by websites, mobile apps, and business processes</li> <li>Databases and frequently updated logs</li> <li>Real-time analytics and dashboarding</li> </ul> <p>\ud83d\udcb0 Cost Characteristics:</p> <ul> <li>Higher storage cost</li> <li>Lower access (read/write) cost</li> </ul> <p>\u2744\ufe0f Cool (Cold) Access Tier</p> <p>\u2705 When to Use:</p> <ul> <li>Data is infrequently accessed, but still needs to be retrieved occasionally</li> <li>Data must be stored for at least 30 days</li> </ul> <p>\ud83d\udce6 Typical Use Cases:</p> <ul> <li>Backup files</li> <li>Long-term storage for compliance</li> <li>Historical logs and infrequently queried datasets</li> <li>Data awaiting processing</li> </ul> <p>\ud83d\udcb0 Cost Characteristics:</p> <ul> <li>Lower storage cost</li> <li>Higher access cost (read/write operations cost more)</li> <li>Early deletion fees (minimum 30-day retention)</li> </ul> <p>\u2757 Key Decision Factors</p> Factor Hot Tier Cool Tier Access Frequency High Low Latency Low (faster access) Medium (slightly slower access) Storage Cost per GB Higher Lower Access Cost per Operation Lower Higher Minimum Retention Duration None 30 days Early Deletion Charges No Yes <p>\ud83d\udcca Summary</p> Use Case Recommended Tier Frequently used app data Hot Infrequently accessed backups Cool Log files stored for 6+ months Cool Videos for a streaming platform Hot Archived customer invoices Cool <p>\ud83d\udd04 Other Access Tiers for Reference</p> Tier Best For Min Retention Access Speed Cost Trend Hot Frequent access None Fast High storage, low access Cool Infrequent access (\u2265 30 days) 30 days Moderate Low storage, high access Archive Rare access (\u2265 180 days) 180 days Slow (rehydration needed) Lowest storage, highest access <p>\ud83d\udd17 References</p> <ul> <li>Azure Blob Storage Access Tiers</li> <li>Azure Storage Pricing</li> </ul>"},{"location":"Cloud/Azure/Azure_storage/#5-when-should-we-choose-standard-vs-premium","title":"5. When should we choose standard vs premium ?","text":"<p>Azure Storage offers two performance tiers:</p> <ul> <li>Standard Tier</li> <li>Premium Tier</li> </ul> <p>Selecting the right tier depends on your application's performance, cost, and latency requirements.</p> <p>\ud83d\udfe2 Standard Tier</p> <p>\u2705 When to Use:</p> <ul> <li>General-purpose workloads with moderate performance needs</li> <li>Applications where latency is not critical</li> <li>Cost-sensitive scenarios with large data volumes</li> </ul> <p>\ud83d\udce6 Typical Use Cases:</p> <ul> <li>Backups and archives</li> <li>Media content storage (images, videos)</li> <li>General-purpose file shares</li> <li>Database BLOBs with low IOPS</li> <li>Web apps, APIs, dev/test environments</li> </ul> <p>\u2699\ufe0f Technical Characteristics:</p> <ul> <li>Backed by HDDs or standard SSDs</li> <li>Lower IOPS and throughput</li> <li>Higher latency (~ms range)</li> </ul> <p>\ud83d\udcb0 Cost:</p> <ul> <li>Low cost per GB</li> <li>Higher latency</li> <li>Pay-per-transaction (higher for frequent access)</li> </ul> <p>\ud83d\udd34 Premium Tier</p> <p>\u2705 When to Use:</p> <ul> <li>Workloads requiring low-latency, high throughput, and high IOPS</li> <li>Applications with performance-critical operations</li> </ul> <p>\ud83d\udce6 Typical Use Cases:</p> <ul> <li>Virtual machines with heavy I/O (e.g., SQL Server, Oracle)</li> <li>OLTP databases</li> <li>High-performance file shares (e.g., FSLogix profiles)</li> <li>Enterprise apps with sub-millisecond latency requirements</li> </ul> <p>\u2699\ufe0f Technical Characteristics:</p> <ul> <li>Backed by high-performance SSDs</li> <li>Very high IOPS &amp; throughput</li> <li>Lower latency (~sub-millisecond)</li> </ul> <p>\ud83d\udcb0 Cost:</p> <ul> <li>Higher cost per GB</li> <li>Predictable performance</li> <li>Flat rate pricing (vs. transactional)</li> </ul> <p>\ud83d\udd0d Side-by-Side Comparison</p> Feature Standard Tier Premium Tier Performance Medium HDD / Standard SSD SSD Latency Milliseconds Sub-millisecond Throughput Moderate High IOPS Low to moderate High Cost Lower (pay-as-you-go) Higher (flat rate) Use Case Fit Archive, backup, general use Mission-critical, high IOPS Scalability High High <p>\u2705 Choosing Recommendation</p> Scenario Recommended Tier Backups and archives Standard Media content delivery Standard Dev/test environments Standard Mission-critical database (SQL, Oracle) Premium High-performance virtual machine disks Premium File shares for FSLogix profiles (VDI) Premium <p>\ud83d\udd17 References</p> <ul> <li>Azure Storage Performance Tiers</li> <li>Azure Premium vs Standard Disks</li> </ul>"},{"location":"Cloud/Azure/Azure_storage/#6-differentiate-between-sdd-vs-hdd","title":"6. Differentiate between SDD vs HDD ?","text":"<p>Both SSD (Solid State Drive) and HDD (Hard Disk Drive) are storage devices used in computers and servers. Below is a comparison based on speed, durability, cost, and use cases.</p> <p>\ud83e\udde0 Overview</p> Feature SSD (Solid State Drive) HDD (Hard Disk Drive) Technology Flash memory (no moving parts) Magnetic spinning disk + moving head Access Time Fast (0.1 ms - 0.3 ms) Slow (5 ms - 15 ms) Durability More durable (no mechanical parts) Prone to damage from drops/shocks Noise Silent operation Audible spinning/clicking sounds Power Usage Lower power consumption Higher power consumption Cost/GB Higher Lower Lifespan (writes) Limited write cycles (wear leveling) Can degrade over time (mechanical wear) Boot Time 10\u201315 seconds 30\u201340 seconds <p>\u2699\ufe0f Performance Comparison</p> Criteria SSD HDD Read/Write Speed 500 MB/s to 7,000 MB/s (NVMe) 80 MB/s to 160 MB/s Data Transfer Rate Very high Moderate to low IOPS (Input/Output) 10x to 100x more than HDD Lower IOPS <p>\ud83d\udce6 Use Case Comparison</p> Use Case Recommended Drive Operating system / boot drive SSD Gaming or media editing SSD Archival or long-term data storage HDD Large file media libraries HDD Performance-critical applications SSD Budget-friendly large storage HDD <p>\ud83e\uddfe Cost &amp; Capacity Comparison</p> Feature SSD HDD Price per GB Higher Lower Typical Capacity 128 GB \u2013 8 TB 500 GB \u2013 20 TB Affordability Expensive for large sizes Cost-effective for bulk <p>\ud83d\udcca Summary Table</p> Feature SSD HDD Speed Much faster Slower Durability More resistant to damage Mechanical, fragile Noise Silent Audible Cost Higher Lower Lifespan Limited write cycles Mechanical wear over time Energy Usage Low High <p>\ud83e\udde0 Conclusion</p> <ul> <li>Choose SSD if you need speed, reliability, and performance (e.g., boot drives, databases, gaming).</li> <li>Choose HDD if you need large storage capacity at low cost (e.g., backups, media libraries).</li> </ul> <p>\ud83d\udd17 References</p> <ul> <li>What is an SSD?</li> <li>SSD vs HDD Comparison \u2013 Microsoft Docs</li> </ul>"},{"location":"Cloud/Azure/Azure_storage/#7-differentiate-between-lrs-zrs-grs-and-ra-grs","title":"7. Differentiate between LRS , ZRS , GRS and RA-GRS ?","text":"<p>Azure provides multiple data redundancy options to ensure high availability, durability, and disaster recovery of your data. These include:</p> <ul> <li>LRS (Locally Redundant Storage)</li> <li>ZRS (Zone-Redundant Storage)</li> <li>GRS (Geo-Redundant Storage)</li> <li> <p>RA-GRS (Read-Access Geo-Redundant Storage)</p> </li> <li> <p>\ud83e\udde0 Definitions</p> </li> </ul> Redundancy Option Description LRS Replicates data 3 times within a single datacenter in one region. ZRS Replicates data across 3 availability zones in the same region. GRS Replicates data to a secondary region hundreds of miles away (with LRS in both regions). RA-GRS Same as GRS but allows read-only access to the secondary region. <ol> <li>\ud83d\udce6 Use Case Comparison</li> </ol> Scenario Recommended Redundancy Low-cost storage with basic durability LRS High availability within a region ZRS Disaster recovery across regions (write-only) GRS Read-only access during regional outage RA-GRS <ol> <li>\ud83d\udd01 Replication Scope</li> </ol> Redundancy Intra-Region Inter-Zone Cross-Region Read Access to Secondary LRS \u2705 \u274c \u274c \u274c ZRS \u2705 \u2705 \u274c \u274c GRS \u2705 \u274c \u2705 \u274c RA-GRS \u2705 \u274c \u2705 \u2705 <p>\ud83d\udcb0 Cost &amp; Availability</p> Feature LRS ZRS GRS RA-GRS Cost Lowest Medium High Highest Durability (9s) 11 9s 12 9s 16 9s 16 9s Availability SLA 99.9% 99.999% 99.9% 99.9% Disaster Recovery Ready \u274c \u274c \u2705 \u2705 Read Access in Failover \u274c \u274c \u274c \u2705 <p>\ud83d\udd0d Summary Table</p> Feature LRS ZRS GRS RA-GRS Redundancy Type Single DC replication Multi-zone replication Cross-region replication Cross-region + read access Number of Copies 3 3+ 6 (3 in each region) 6 (3 in each region) Availability Zone \u274c \u2705 \u274c \u274c Geo Replication \u274c \u274c \u2705 \u2705 Read from Secondary \u274c \u274c \u274c \u2705 Best For Low-cost local storage High-availability within region Disaster recovery DR + read access to secondary <p>\u2705 Recommendations</p> Need Use Basic redundancy, dev/test workloads LRS Zone-level high availability (e.g., production apps) ZRS DR capability for mission-critical apps GRS DR with read access (e.g., reporting, analytics) RA-GRS <p>\ud83d\udd17 References</p> <ul> <li>Azure Storage Redundancy</li> <li>Choose a Storage Redundancy Option</li> </ul>"},{"location":"Cloud/Azure/Azure_storage/#8-how-can-azure-storage-explorer-make-your-life-easy","title":"8. How can azure storage explorer make your life easy ?","text":"<p>Azure Storage Explorer is a free, standalone GUI tool from Microsoft that simplifies working with Azure Storage resources like Blobs, Queues, Tables, and File Shares.</p> <p>\ud83d\udee0\ufe0f What is Azure Storage Explorer?</p> <p>Azure Storage Explorer is a cross-platform desktop app (Windows, macOS, Linux) that lets you easily manage and interact with your Azure storage accounts without needing to write code or scripts.</p> <p>\u2705 Key Benefits of Azure Storage Explorer</p> <ol> <li> <p>Visual Interface</p> </li> <li> <p>Browse containers, directories, and blobs like a file explorer.</p> </li> <li>Easy drag-and-drop for uploads and downloads.</li> <li> <p>No need for complex PowerShell or Azure CLI commands.</p> </li> <li> <p>Multi-Storage Support</p> </li> <li> <p>Manage multiple storage accounts across subscriptions and tenants.</p> </li> <li> <p>Access Azure Blob, Queue, Table, and File storage, plus Azure Data Lake.</p> </li> <li> <p>Local Emulator Support</p> </li> <li> <p>Integrates with Azure Storage Emulator or Azurite for local development.</p> </li> <li> <p>Helps in testing apps offline before cloud deployment.</p> </li> <li> <p>Access via Different Authentication Modes</p> </li> <li> <p>Azure AD (RBAC), Shared Access Signature (SAS), Connection Strings, or Account Keys.</p> </li> <li> <p>Connect to public, private, or even on-premises Azure Stack storage.</p> </li> <li> <p>Blob and File Management</p> </li> <li> <p>Upload, download, rename, move, and delete blobs or files.</p> </li> <li> <p>Preview file types like text, JSON, images right in the app.</p> </li> <li> <p>Queue and Table Explorer</p> </li> <li> <p>View and modify queue messages.</p> </li> <li> <p>Browse, filter, add, edit, or delete rows in Azure Tables (NoSQL DB).</p> </li> <li> <p>SAS Token and Shared Access</p> </li> <li> <p>Generate and manage SAS URLs to securely share blobs, files, or containers.</p> </li> <li> <p>Easily control permissions (read, write, delete, list, etc.).</p> </li> <li> <p>Cross-Region and Cloud Integration</p> </li> <li> <p>Manage Azure storage across multiple regions.</p> </li> <li> <p>Support for Azure Government, China, and Azure Stack clouds.</p> </li> <li> <p>Time-Saving for DevOps &amp; Admins</p> </li> <li> <p>Easy to inspect storage used by web apps, functions, and containers.</p> </li> <li> <p>Faster troubleshooting and real-time monitoring of storage data.</p> </li> <li> <p>Snapshot and Versioning Support</p> </li> <li> <p>View and restore blob snapshots and versions.</p> </li> <li>Helpful for backup and disaster recovery scenarios.</li> </ol> <p>\ud83d\udce6 Typical Use Cases</p> Role Use Case Developer Upload test files, debug queue messages Data Engineer Manage and inspect blob datasets and file shares Admin Monitor storage usage and permissions QA Tester Interact with emulators for offline testing <p>\ud83d\udd17 Resources</p> <ul> <li>Download Azure Storage Explorer</li> <li>Azure Storage Explorer Docs</li> </ul> <p>\ud83e\udde0 Conclusion</p> <p>Azure Storage Explorer makes your life easy by offering a GUI-based, no-code, cross-platform way to work with Azure storage. It e</p>"},{"location":"Cloud/Azure/DTU_EDTU/","title":"DTU_EDTU","text":""},{"location":"Cloud/Azure/DTU_EDTU/#1-what-is-the-problem-of-mapping-work-load-with-azure-configuration","title":"1. What is the problem of mapping work load with Azure configuration ?","text":"<p>Problem of Mapping Workload with Azure Configuration</p> <p>Mapping workloads correctly with Azure configurations is critical to ensure optimal performance, cost-efficiency, and reliability. However, there are several challenges and problems that organizations face during this process:</p> <p>\u26a0\ufe0f Common Problems</p> <p>1. Incorrect Sizing of Resources</p> <ul> <li>Issue: Choosing VM sizes or App Service plans that are too small or too large.</li> <li>Impact: Under-provisioning leads to performance bottlenecks; over-provisioning causes unnecessary costs.</li> </ul> <p>2. Lack of Understanding of Workload Characteristics</p> <ul> <li>Issue: Not analyzing CPU, memory, I/O, and scaling behavior of the workload.</li> <li>Impact: Results in misaligned service selection, such as using an App Service when a Function App would be more cost-effective.</li> </ul> <p>3. Improper Use of Service Tiers</p> <ul> <li>Issue: Selecting wrong pricing tiers (e.g., Basic instead of Premium).</li> <li>Impact: Missing out on features like auto-scaling, staging slots, or high availability.</li> </ul> <p>4. Ignoring Network and Storage Needs</p> <ul> <li>Issue: Workload requires specific networking or storage configurations not accounted for.</li> <li>Impact: Leads to network bottlenecks or data latency issues.</li> </ul> <p>5. Overlooking Compliance and Security Requirements</p> <ul> <li>Issue: Not mapping workloads to services that meet regulatory or security needs.</li> <li>Impact: Non-compliance with industry standards (e.g., HIPAA, GDPR).</li> </ul> <p>\ud83d\udee0 Best Practices for Mapping</p> <ul> <li>Use Azure Advisor: Get real-time suggestions for performance and cost optimizations.</li> <li>Perform Workload Assessment: Analyze workload behavior using tools like Azure Migrate.</li> <li>Right-size Continuously: Use autoscaling and cost monitoring tools to adjust as usage changes.</li> <li>Align with Business SLAs: Choose configurations that support your uptime, latency, and DR requirements.</li> <li>Leverage Azure Well-Architected Framework: Follow guidelines to optimize reliability, performance, and cost.</li> </ul> <p>\u2705 Conclusion</p> <p>Mapping workloads with the correct Azure configurations is not a one-time task. It requires continuous assessment and alignment with workload behavior, business goals, and cost considerations. Failure to do so can lead to degraded performance, security issues, and increased cloud bills.</p>"},{"location":"Cloud/Azure/DTU_EDTU/#2-explain-dtu-and-edtu","title":"2. Explain DTU and EDTU ?","text":"<p>Understanding DTU and eDTU in Azure SQL Database</p> <p>When using Azure SQL Database, it's important to understand how Microsoft defines and allocates performance using DTUs and eDTUs. These units simplify resource management but can be confusing without proper context.</p> <p>\ud83d\udca1 What is a DTU?</p> <p>DTU (Database Transaction Unit) is a blended measure of:</p> <ul> <li>CPU</li> <li>Memory</li> <li>Reads and Writes (I/O)</li> </ul> <p>It represents the overall performance level for a single Azure SQL database in the DTU-based purchasing model.</p> <p>\ud83d\udcac Think of it as a performance \"package\" that wraps CPU, memory, and storage IOPS into one simplified number.</p> <p>\ud83d\udd22 Example DTU Tiers:</p> Tier DTUs Max Storage Basic 5 2 GB Standard 10\u2013300 250 GB Premium 125\u20134000 1 TB <p>\ud83d\udcd8 What is an eDTU?</p> <p>eDTU (Elastic DTU) is the equivalent of DTU but applied to Elastic Pools.</p> <ul> <li>Elastic Pools are collections of databases that share resources.</li> <li>eDTUs are the total pooled performance capacity available to all databases within the pool.</li> </ul> <p>\ud83e\udde0 eDTU = Shared DTU capacity across all databases in a pool</p> <p>\ud83d\udd04 Key Difference:</p> <ul> <li>DTU = For single database</li> <li>eDTU = For elastic pool of databases</li> </ul> <p>\ud83c\udd9a DTU vs vCore Model</p> Feature DTU Model vCore Model Unit of measure DTU (bundled) vCore (CPU + memory separately) Flexibility Less granular More control over resources Use case Simpler workloads, cost limits Enterprise workloads, predictable scaling Transparency Lower Higher (you know exact CPU/memory) <p>\u2705 When to Use</p> Scenario Choose You want simple pricing and don't need fine-grained control DTU model You want transparency, control, and scalability vCore model You have multiple databases with unpredictable usage patterns eDTU with Elastic Pool <p>\ud83d\udccc Summary</p> <ul> <li>DTU: Performance metric for a single database, combining CPU, memory, and I/O.</li> <li>eDTU: Shared DTU performance metric for elastic pools (multiple databases).</li> <li>Useful for simplified performance planning, but less flexible than vCore-based pricing.</li> </ul>"},{"location":"Cloud/Azure/DTU_EDTU/#3-on-which-factors-does-dtu-depend","title":"3. On which factors does DTU depend ?","text":"<p>\ud83d\udd0d Factors on Which DTU Depends</p> <p>A DTU (Database Transaction Unit) in Azure SQL Database is a pre-configured blend of three core resources:</p> <ol> <li> <p>CPU (Compute Power)</p> </li> <li> <p>Represents the processing capability of the database engine.</p> </li> <li>DTU includes CPU cycles for:</li> <li>Query execution</li> <li>Indexing</li> <li>Stored procedure processing</li> <li>Complex joins or calculations</li> </ol> <p>\u26a0\ufe0f CPU-bound queries can max out DTUs quickly.</p> <ol> <li> <p>Memory (RAM)</p> </li> <li> <p>Used for:</p> </li> <li>Query caching</li> <li>Data buffering</li> <li>Sorting and hashing operations</li> <li>Execution plans</li> </ol> <p>\ud83d\udca1 Efficient use of memory can significantly reduce I/O and boost performance. 3. Disk I/O (Reads and Writes)</p> <ul> <li>Includes:</li> <li>Transaction log writes</li> <li>Data page reads/writes</li> <li>TempDB usage</li> <li>I/O-heavy operations (bulk inserts, frequent reads) consume more DTUs.</li> </ul> <p>\ud83d\udcc9 Poor indexing or large table scans increase I/O load and DTU usage. \ud83d\udd01 DTU = Blended Metric</p> <p>The DTU is essentially a performance throttle that balances: DTU = f(CPU, Memory, Disk I/O) If any one of the three resources hits its threshold, your DTU usage approaches 100%, throttling further performance.</p> <p>\ud83d\udcca What Affects DTU Consumption?</p> Factor Impact on DTU Complex or nested queries High CPU usage Missing indexes Increased I/O Large result sets More memory and I/O High transaction volume CPU + I/O Long-running or blocking queries CPU + Memory Poor query design All 3 (CPU, Mem, I/O) <p>\ud83d\udee0 Tools to Monitor DTU Usage</p> <ul> <li>Azure Portal \u2192 SQL Database \u2192 Monitoring \u2192 DTU %</li> <li>Query Performance Insight</li> <li>Extended Events or Query Store</li> </ul> <p>\u2705 Optimization Tips</p> <ul> <li>Use proper indexing and query tuning</li> <li>Reduce I/O operations by filtering and batching</li> <li>Enable automatic tuning in Azure SQL</li> <li>Consider scaling to a higher DTU or switch to vCore model for more control</li> </ul> <p>\ud83d\udccc Summary</p> <p>DTU depends on:</p> <ul> <li>CPU usage</li> <li>Memory pressure</li> <li>Disk I/O operations</li> </ul> <p>Understanding workload behavior helps you choose the right DTU tier and optimize performance effectively.</p>"},{"location":"Cloud/Azure/DTU_EDTU/#4-how-to-calculate-dtu","title":"4. How to calculate DTU ?","text":"<p>\ud83d\udcd0 How to Calculate DTU (Database Transaction Units)</p> <p>Azure does not provide a direct formula to calculate DTUs because they are abstract performance units, blending CPU, memory, and I/O. However, Microsoft offers tools and guidance to help estimate DTU requirements based on your on-premise or existing workloads.</p> <p>\u2699\ufe0f What is DTU?</p> <p>DTU = A blended measure of CPU + Memory + IOPS It helps you choose the right Azure SQL Database performance level (Basic, Standard, Premium)</p> <p>\ud83e\uddee How to Calculate or Estimate DTU Requirements</p> <p>\u2705 Option 1: Use Microsoft DTU Calculator (Recommended)</p> <p>Microsoft provides a tool: \ud83d\udc49 DTU Calculator</p> <p>\ud83d\udccb Steps:</p> <ol> <li>Run a workload on your existing SQL Server (on-premise or VM).</li> <li>Collect performance metrics using Performance Monitor (PerfMon) for:</li> <li>Processor: <code>% Processor Time</code></li> <li>Logical Disk: <code>Disk Reads/sec</code>, <code>Disk Writes/sec</code></li> <li>SQL Server: <code>Batch Requests/sec</code></li> <li>Export the data to a CSV.</li> <li>Upload to the DTU Calculator.</li> <li>The tool estimates the minimum DTU tier needed for Azure SQL Database.</li> </ol> <p>\ud83d\udcca Example Metrics Collected</p> Metric Description % Processor Time CPU utilization Disk Reads/sec Read I/O load Disk Writes/sec Write I/O load Batch Requests/sec Workload intensity <p>\u2705 Option 2: Use Azure SQL Query Performance Insight</p> <p>If you're already in Azure:</p> <ul> <li>Go to SQL Database \u2192 Monitoring \u2192 Query Performance Insight</li> <li>Check DTU usage over time</li> <li>Analyze top-consuming queries</li> </ul> <p>\u26a0\ufe0f Notes</p> <ul> <li>DTUs are only relevant in the DTU-based pricing model.</li> <li>For vCore-based models, resource estimation is done separately (vCPU, RAM, IOPS).</li> <li>DTUs do not map 1:1 with any physical resource; they are relative units.</li> </ul> <p>\ud83d\udccc Summary</p> Method Use When DTU Calculator Migrating from on-prem or VM to Azure SQL Azure Monitor &amp; Insights Already in Azure environment Manual Estimation (not recommended) Lacks precision without real metrics <p>\ud83e\udde0 DTU calculation is estimative, not exact. Always monitor after deployment to adjust tier if needed.</p>"},{"location":"Cloud/Azure/DTU_EDTU/#5-how-to-measure-the-five-factors-which-derive-dtu","title":"5. How to measure the five factors which derive DTU ?","text":"<p>\ud83d\udccf How to Measure the Five Factors That Drive DTU</p> <p>DTU (Database Transaction Unit) is a composite measure of five core performance metrics related to:</p> <ol> <li>CPU</li> <li>Memory</li> <li>Data Reads</li> <li>Data Writes</li> <li>Transaction Log Writes</li> </ol> <p>These metrics help estimate the workload intensity and choose the correct DTU tier. \ud83e\udde0 Why Measure These?</p> <p>Azure bundles CPU, memory, and I/O into DTUs. Understanding the individual contributors lets you:</p> <ul> <li>Right-size your Azure SQL tier</li> <li>Optimize queries and performance</li> <li>Migrate on-prem workloads accurately using the DTU Calculator</li> </ul> <p>\ud83d\udda5\ufe0f Tools to Use</p> <ul> <li>Performance Monitor (PerfMon) for on-prem SQL Server</li> <li>Azure Monitor / Query Performance Insight for Azure SQL</li> <li>SQL Server Management Studio (SSMS) for DMVs</li> </ul> <p>\ud83d\udd0d Factor-by-Factor Measurement</p> <p>1. CPU Usage</p> <p>Metric: <code>% Processor Time</code> (instance-wide)</p> <p>How to Measure:</p> <ul> <li>PerfMon counter: <code>Processor(_Total)\\% Processor Time</code></li> <li>DMV:   <code>sql   SELECT record_id, SQLProcessUtilization FROM sys.dm_os_ring_buffers</code></li> </ul> <p>2. Memory Usage</p> <p>Metric: Buffer cache usage / memory grants</p> <p>How to Measure:</p> <ul> <li>PerfMon counter: <code>SQLServer:Memory Manager\\Target Server Memory</code> &amp; <code>Total Server Memory</code></li> <li>DMV:   <code>sql   SELECT total_physical_memory_kb, available_physical_memory_kb FROM sys.dm_os_sys_memory;</code></li> </ul> <p>3. Data Reads (Logical/Physical)</p> <p>Metric: <code>Disk Reads/sec</code>, <code>Page Reads/sec</code></p> <p>How to Measure:</p> <ul> <li>PerfMon counter: <code>LogicalDisk(_Total)\\Disk Reads/sec</code></li> <li>DMV:   <code>sql   SELECT * FROM sys.dm_io_virtual_file_stats(NULL, NULL);</code></li> </ul> <p>4. Data Writes</p> <p>Metric: <code>Disk Writes/sec</code></p> <p>How to Measure:</p> <ul> <li>PerfMon counter: <code>LogicalDisk(_Total)\\Disk Writes/sec</code></li> <li>DMV:   <code>sql   SELECT * FROM sys.dm_io_virtual_file_stats(NULL, NULL);</code></li> </ul> <p>5. Transaction Log Writes</p> <p>Metric: Log I/O throughput</p> <p>How to Measure:</p> <ul> <li>PerfMon: <code>SQLServer:Databases\\Log Bytes Flushed/sec</code></li> <li>DMV:   <code>sql   SELECT database_id, log_write_percent, log_bytes_flushed FROM sys.dm_db_log_space_usage;</code>   \ud83d\udcca Suggested PerfMon Counters for DTU Estimation</li> </ul> Category Counter CPU <code>% Processor Time</code> Memory <code>Total Server Memory (KB)</code> Data Read I/O <code>Disk Reads/sec</code> Data Write I/O <code>Disk Writes/sec</code> Log Writes <code>Log Bytes Flushed/sec</code> Workload Intensity <code>Batch Requests/sec</code> <p>\ud83e\uddea Using the DTU Calculator</p> <ol> <li>Collect PerfMon logs with above counters over typical workload.</li> <li>Export the data to CSV.</li> <li>Upload to Azure DTU Calculator</li> <li>View estimated DTU requirements and recommended pricing tier.</li> </ol> <p>\u2705 Summary</p> Factor Measurement Tool PerfMon Counter / DMV CPU PerfMon, DMV <code>% Processor Time</code>, <code>SQLProcessUtilization</code> Memory PerfMon, DMV <code>Total/Target Server Memory</code>, <code>dm_os_sys_memory</code> Data Reads PerfMon, DMV <code>Disk Reads/sec</code>, <code>dm_io_virtual_file_stats</code> Data Writes PerfMon, DMV <code>Disk Writes/sec</code>, <code>dm_io_virtual_file_stats</code> Log Writes PerfMon, DMV <code>Log Bytes Flushed/sec</code>, <code>dm_db_log_space_usage</code> <p>\ud83d\udca1 Tip: Monitor during peak usage periods to get the most accurate DTU estimation.</p>"},{"location":"Cloud/Azure/DTU_EDTU/#6-how-can-you-create-sql-server-db-on-azure","title":"6. How can you create SQL Server DB on Azure ?","text":"<p>\ud83d\udee0\ufe0f How to Create a SQL Server Database on Azure</p> <p>You can create an Azure SQL Database (PaaS) via the Azure Portal, Azure CLI, ARM Templates, or PowerShell. Below is the most common and beginner-friendly method: Azure Portal.</p> <p>\ud83c\udf10 Option 1: Create via Azure Portal (GUI) \u2705 Step-by-Step</p> <p>1. Sign in to Azure Portal</p> <p>Go to \ud83d\udc49 https://portal.azure.com</p> <p>Create a SQL Server (Logical Server)</p> <p>-Search for <code>SQL Server</code> in the search bar.</p> <ul> <li>Click Create &gt; SQL Server.</li> </ul> <p>1. Fill in the following:</p> <ul> <li>Server Name: globally unique</li> <li>Admin Username &amp; Password</li> <li> <p>Region: where the server will reside</p> </li> <li> <p>Click Review + Create \u2192 Create</p> </li> </ul> <p>Create SQL Database</p> <ul> <li> <p>Go to the SQL databases service.</p> </li> <li> <p>Click Create \u2192 SQL Database.</p> </li> </ul> <p>Fill in:</p> <ul> <li>Database Name</li> <li>Subscription and Resource Group</li> <li>Select SQL Server (use the one you just created)</li> <li>Compute + Storage: choose DTU or vCore model</li> <li> <p>Backup and Geo-redundancy options</p> </li> <li> <p>Click Review + Create \u2192 Create</p> </li> </ul> <p>Configure Firewall &amp; Networking</p> <ul> <li> <p>After creation, go to the SQL Server resource.</p> </li> <li> <p>Open Networking &gt; Firewalls and virtual networks.</p> </li> <li> <p>Add your client IP address to allow external access.</p> </li> <li> <p>Save changes.</p> </li> </ul> <p>Connect to the Database</p> <ul> <li>Use SQL Server Management Studio (SSMS) or Azure Data Studio</li> <li>Server name format: .database.windows.net <li>Use the admin username/password you provided.</li> <p>\u2699\ufe0f Option 2: Create via Azure CLI</p> <pre><code> Create Resource Group\naz group create --name myResourceGroup --location eastus\n\n Create SQL Server\naz sql server create \\\n--name my-sql-server \\\n--resource-group myResourceGroup \\\n--location eastus \\\n--admin-user myadmin \\\n--admin-password MyP@ssword123\n\n Create SQL Database\naz sql db create \\\n--resource-group myResourceGroup \\\n--server my-sql-server \\\n--name mySampleDatabase \\\n--service-objective S0\n</code></pre>"},{"location":"Cloud/Azure/DTU_EDTU/#7-error-while-connecting-to-the-azure-database-from-ssms","title":"7. Error while connecting to the azure database from SSMS","text":"<p>ERROR</p> <pre><code>An instance-specific error occurred while establishing a connection to SQL Server.\nConnection was denied since Deny Public Network Access is set to\nYes (https://docs.microsoft.com/azure/azure-sql/database/connectivity-settings#deny-public-network-access).\nTo connect to this server, use the Private Endpoint from inside your virtual network (https://docs.microsoft.com/azure/sql-database/sql-database-private-endpoint-overview#how-to-set-up-private-link-for-azure-sql-database). (Microsoft SQL Server, Error: 47073)\n</code></pre> <p>Azure SQL Connectivity Setup</p> <p>This document outlines how to configure connectivity to an Azure SQL Server for Production and Development/Test environments.</p> <p>\ud83d\udd12 Production: Secure Access via Private Endpoint</p> <p>\u2705 Use Case: For production environments where public access is not permitted.</p> <p>\ud83d\udd27 Steps:</p> <p>1. Create a Private Endpoint</p> <ul> <li>Go to the Azure Portal \u2192 SQL Server (not database).</li> <li>Navigate to Networking \u2192 Private endpoint connections.</li> <li>Click + Private endpoint and follow the wizard.</li> <li>Associate it with the correct Virtual Network (VNet) and subnet.</li> </ul> <p>2. Configure Private DNS</p> <ul> <li>Use Azure Private DNS Zone: <code>privatelink.database.windows.net</code></li> <li>Link it to your VNet.</li> <li>Ensure the DNS resolves the SQL Server name to the private IP.</li> </ul> <p>3. Access from Azure Resources</p> <ul> <li>Ensure clients (VMs, App Services, Functions) are in the same VNet or peered VNet.</li> <li>Use the regular server name (e.g., <code>yourserver.database.windows.net</code>) \u2014 it will resolve to the private IP.</li> </ul> <p>4. Test Connection</p> <ul> <li>RDP into a VM within the VNet (or use VNet-integrated app).</li> <li>Use SSMS or app connection string to verify access.</li> </ul> <p>5. Ensure Public Access is Disabled</p> <ul> <li>Navigate to SQL Server \u2192 Networking.</li> <li>Set Public network access to Deny.</li> </ul> <p>\ud83d\udcd8 Private Link Setup Guide</p> <p>\ud83e\uddea Development/Test: Controlled Public Access</p> <p>\u26a0\ufe0f Use Case: For local development or testing scenarios where secure access is relaxed.</p> <p>\ud83d\udd27 Steps:</p> <p>1. Allow Public Network Access</p> <ul> <li>Go to SQL Server \u2192 Networking.</li> <li>Set Public network access to Yes.</li> </ul> <p>3. Add Client IP to Firewall Rules</p> <ul> <li>Under Firewall and virtual networks, add your current IP address.</li> <li>Use <code>0.0.0.0 - 255.255.255.255</code> only for testing (not recommended).</li> </ul> <p>3. Test SQL Connection</p> <ul> <li>Use SSMS, Azure Data Studio, or your application with:</li> <li>Server: <code>yourserver.database.windows.net</code></li> <li>Authentication: SQL Auth / Azure AD / Managed Identity</li> </ul> <p>4. Restrict When Done</p> <ul> <li>Remove test IPs and reset Public network access to Deny when testing is complete.</li> </ul> <p>\ud83d\udcd8 Firewall Configuration Guide</p> <p>\ud83e\udde0 Best Practices</p> <ul> <li>\u2705 Always use Private Endpoints in production.</li> <li>\u2705 Monitor and alert on unexpected public access attempts.</li> <li>\u2705 Periodically review firewall rules and audit access.</li> <li>\ud83d\udeab Never use <code>Allow All</code> IP range in production.</li> </ul>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/","title":"IASS_PASS_SAAS","text":""},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#1-define-ssas-paas-and-iaas","title":"1. Define SSAS, PAAS and IAAS ?","text":"<p>Cloud computing offers different levels of service models to fit various needs. The three primary models are:</p> Feature SaaS PaaS IaaS Use Case End-user applications App development &amp; deployment Full infrastructure control Managed By Provider Shared (provider + developer) Mostly user User Manages Just usage (no backend) Code &amp; app config OS, middleware, app, data Examples Gmail, Zoom, Salesforce Heroku, App Engine, Azure AppSvc EC2, Azure VM, GCP Compute <p>\u2705 SaaS is best for end users. \ud83d\udc68\u200d\ud83d\udcbb PaaS is for developers. \ud83d\udee0\ufe0f IaaS is for sysadmins and architects.</p>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#2-how-is-cloud-different-from-normal-webhosting","title":"2. How is cloud different from normal webhosting ?","text":"Feature Traditional Hosting Cloud Hosting Infrastructure Single physical server Cluster of virtual servers Scalability Limited/manual Auto-scalable Uptime Low (server failure = down) High (failover built-in) Cost Fixed pricing Pay-as-you-use Performance Affected by neighbors Load-balanced Best For Small/static sites Scalable, dynamic apps"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#3-explain-the-2-big-os-in-cloud","title":"3. Explain the 2 big O\u2019s in cloud ?","text":"Concept Description Real-World Analogy On-Demand Provision resources instantly when needed Like streaming movies (no DVDs) Operatinal Expenditure Pay for usage instead of upfront investment Like paying utility bills"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#4-what-is-a-resource-and-why-do-we-need-resource-groups","title":"4. What is a resource and why do we need Resource groups ?","text":"<ul> <li>Understanding Resources and Resource Groups in Cloud Computing</li> </ul> <p>What is a Resource?</p> <p>Definition: A resource is any manageable item available in your cloud environment. These are individual services that you create, manage, and pay for.</p> <p>Examples of Resources:</p> <ul> <li>Virtual Machines (VMs)</li> <li>Storage Accounts</li> <li>SQL Databases</li> <li>Web Apps</li> <li>Virtual Networks (VNet)</li> <li>Kubernetes Clusters</li> </ul> <p>Each resource represents a single service that performs a specific function.</p> <p>What is a Resource Group?</p> <p>Definition: A Resource Group is a logical container that holds related cloud resources. It helps organize and manage them as a unit.</p> <p>Why Do We Need Resource Groups?</p> <p>1. Logical Organization</p> <p>Group resources by application, environment (dev/test/prod), or project. Makes it easier to locate and manage related items.</p> <p>2. Unified Management</p> <p>Apply permissions, tags, and policies at the group level. This ensures consistency across all resources inside it.</p> <p>3. Simplified Monitoring &amp; Billing</p> <p>Monitor usage, set budgets, and track costs at the resource group level.</p> <p>4. Efficient Deployment</p> <p>Deploy, update, or delete all resources in a group together using templates (e.g., ARM or Bicep templates).</p> <p>5. Access Control</p> <p>Assign role-based access (RBAC) to an entire group, ensuring users can only interact with the resources they\u2019re allowed to manage.</p> <p>Example</p> <p>Let\u2019s say you\u2019re building a web application called <code>MyApp</code>.</p> <p>You could create a resource group named <code>MyApp-RG</code>, and include:</p> <ul> <li><code>MyApp-VM</code> (Virtual Machine)</li> <li> <p><code>MyApp-SQL</code> (Database)</p> </li> <li> <p><code>MyApp-Storage</code> (Blob Storage)</p> </li> <li><code>MyApp-WebApp</code> (App Service)</li> </ul> <p>All of these resources will be grouped logically, and you can manage them together.</p> <p>Summary</p> Term Description Example Resource A cloud service instance Azure VM, SQL DB, Blob Storage Resource Group A logical container for related resources Group for <code>MyApp</code> services <p>Think of a Resource Group as a folder, and Resources as the files inside it.</p>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#5-what-is-the-importance-of-resource-group-location","title":"5. What is the importance of Resource group location ?","text":"<p>When creating a Resource Group in Azure, you're asked to specify a location (region) \u2014 but why does it matter?</p> <p>What Is a Resource Group Location?</p> <p>The location of a resource group refers to the region where the metadata for that group is stored. It does not dictate where all the resources inside must reside.</p> <p>Why Is Resource Group Location Important?</p> <p>Metadata Storage</p> <p>The region determines where the deployment data, management info, and tags for the resource group are stored.</p> <p>Resource Deployment</p> <ul> <li>Resources can be deployed in any region, regardless of the resource group's location.</li> <li>But some services (like Azure Managed Identity, automation, or policy enforcement) may rely on the resource group\u2019s region.</li> </ul> <p>Compliance &amp; Governance</p> <p>If your organization has data residency or compliance requirements, storing metadata in a specific region may be necessary.</p> <p>Availability &amp; Disaster Recovery</p> <p>In case of a regional outage:</p> <ul> <li>The ability to manage or update resources might be affected if the resource group\u2019s location is down, even if the actual resource is in a healthy region.</li> </ul> <p>Consistency in Deployment Templates</p> <p>Some ARM templates or scripts may use the resource group\u2019s location as a default value for deploying resources.</p> <p>Example Scenario</p> <p>You create a resource group <code>MyApp-RG</code> in East US.</p> <ul> <li>You deploy a VM in West US and a SQL DB in East US 2 \u2014 both are valid.</li> <li>However, metadata like access policies or tags for <code>MyApp-RG</code> are stored in East US.</li> <li>If East US is down, you might not be able to update or delete the group until it's restored.</li> </ul> <p>Best Practice</p> <ul> <li>Choose a stable and compliant region for the resource group location.</li> <li>If most of your resources are in a specific region, align the resource group with that region to reduce latency and simplify deployment.</li> </ul> <p>Summary Table</p> Aspect Impact of Resource Group Location Metadata Storage Determines where group metadata is stored Resource Deployment Resources can live in any region Outage Impact Managing group may fail if location is down Compliance Important for legal/data governance Template Defaults Used in scripts to default resource location <p>Note: Resource group location \u2260 resource location \u2014 but it still plays a key role in stability and governance.</p>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#6-what-are-app-services","title":"6. What are app services ?","text":"<p>Azure App Service is a fully managed Platform-as-a-Service (PaaS) offering from Microsoft Azure that allows developers to build, deploy, and scale web applications and APIs quickly and efficiently.</p> <p>It supports multiple programming languages and frameworks, including:</p> <ul> <li>ASP.NET / .NET Core</li> <li>Java</li> <li>Node.js</li> <li>PHP</li> <li>Python</li> <li>Ruby</li> <li>Custom containers (Docker, Linux)</li> </ul> <p>\u2705 Key Benefits</p> Feature Description Fully Managed Hosting No need to manage infrastructure \u2014 Microsoft handles OS, server patching, scaling, and security updates. Built-in CI/CD Integration with GitHub, Azure DevOps, and Bitbucket for continuous integration and deployment. Scaling Options Automatically scale your application based on demand or schedule. Global Availability Deploy to data centers around the world for high availability. Custom Domains &amp; SSL Easily configure your own domain name and secure your app with HTTPS. Monitoring and Logging Built-in tools for diagnostics, logging, and performance monitoring using Application Insights. Authentication &amp; Authorization Built-in user authentication with Azure AD, Facebook, Google, Twitter, etc. <p>\ud83e\uddf1 App Service Types</p> Type Description Web Apps Host websites and web applications using .NET, Java, Node.js, Python, PHP, etc. API Apps Host RESTful APIs with Swagger/OpenAPI support. Mobile Apps Backend services for mobile applications with offline sync, push notifications. WebJobs Run background tasks triggered by events or on schedules. <p>\ud83d\udee0\ufe0f Hosting Plans</p> Plan Use Case Free (F1) Basic development and testing Shared (D1) Shared resources for simple workloads Basic (B1-B3) Dedicated compute for dev/test Standard (S1-S3) Production-ready with autoscaling and SSL Premium (P1v3-P3v3) High performance with VNET support Isolated (I1-I3) Apps needing high security, compliance, and network isolation <p>\ud83d\ude80 Use Cases</p> <ul> <li>Hosting public-facing websites and portals</li> <li>Deploying RESTful APIs</li> <li>Running internal line-of-business applications</li> <li>Building backend systems for mobile or IoT apps</li> <li>Migrating on-premise .NET web apps to the cloud</li> </ul> <p>\ud83d\udce6 Supported Deployment Methods</p> <ul> <li>Visual Studio / VS Code</li> <li>Azure CLI / PowerShell</li> <li>GitHub Actions / Azure DevOps Pipelines</li> <li>FTP / FTPS</li> <li>Zip deploy</li> <li>Container Registry (ACR / Docker Hub)</li> </ul> <p>\ud83d\udd10 Security Features</p> <ul> <li>Managed Identity integration for secure Azure resource access</li> <li>VNET integration (Premium/Isolated tiers)</li> <li>TLS/SSL certificates with automatic renewal</li> <li>App-level authentication and RBAC</li> <li>Azure Key Vault support for secret management</li> </ul> <p>\ud83d\udcc8 Monitoring and Diagnostics</p> <p>Azure App Services integrates with:</p> <ul> <li>Azure Monitor</li> <li>Application Insights</li> <li>Log Analytics</li> </ul> <p>Provides insights into:</p> <ul> <li>Request/response times</li> <li>Error tracking</li> <li>Memory and CPU usage</li> <li>Dependency tracking (e.g., DB calls)</li> </ul> <p>Summary</p> <p>Azure App Services is ideal for developers and teams who want to focus on code, not infrastructure. It delivers a fast, secure, and scalable environment for hosting modern web applications and APIs.</p>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#7-which-appservice-will-you-choose-to-host-a-website","title":"7. Which appservice will you choose to host a website ?","text":"<p>Azure App Service provides different types of hosting plans to suit a variety of web application needs. When choosing an App Service plan to host a website, consider factors such as performance, scalability, traffic, budget, and required features.</p> <p>\u2705 Recommended App Service Plan for Hosting a Website</p> <p>App Service Plan: Basic / Standard / Premium (Windows or Linux)</p> <ul> <li>Use Case: Hosting production websites with moderate to high traffic, requiring custom domains, SSL, scaling, and deployment slots.</li> <li>Recommendation: Use Standard (S1/S2/S3) or Premium (P1v3/P2v3) for best performance and features.</li> </ul> <p>\ud83d\udd0d App Service Plan Options</p> Plan Type Description Ideal For Free (F1) Shared compute; limited resources, no custom domains or SSL. Testing, learning, non-public Shared (D1) Shared infrastructure; basic scaling. Low-traffic dev/test apps Basic (B1/B2/B3) Dedicated VMs, custom domain support. Small production workloads Standard (S1/S2/S3) Autoscaling, staging slots, custom domains + SSL. Business websites, web APIs Premium (P1v3/P2v3) Faster processors, VNet integration, autoscaling, more slots. High-traffic websites Isolated (I1/I2/I3) Runs in VNet with dedicated environment (ASE). Secure, compliance-heavy apps Container (Linux Docker) Host web apps in Docker containers using App Service for Linux. Microservices &amp; container-based apps <p>\ud83c\udfaf Hosting Static Sites?</p> <ul> <li>Use Azure Static Web Apps if you are hosting a static site (HTML/CSS/JS) with optional backend via Azure Functions.</li> <li>Benefits: Global CDN, GitHub/Azure DevOps integration, custom domain &amp; SSL.</li> </ul> <p>\u2705 Example: Ideal App Services for Common Website Types</p> Website Type Recommended Plan Personal blog Free or Basic Business landing page Basic or Standard E-commerce website Standard or Premium High-traffic CMS Premium or Isolated React/Angular SPA + API Static Web App + Premium Plan <p>\ud83d\udccc Additional Considerations</p> <ul> <li>Platform: Choose Linux for Node.js, Python, PHP apps or Dockerized apps. Use Windows for .NET Framework apps.</li> <li>Scaling: Standard and above support Auto Scaling and Deployment Slots.</li> <li>Networking: Premium and Isolated plans support VNet Integration.</li> </ul> <p>\ud83d\udca1 Final Recommendation</p> <p>If you're deploying a typical production-ready website (e.g. using React, Angular, or ASP.NET), go with:</p> <pre><code>App Service Plan: **Standard S1** or **Premium P1v3**\nOS: **Linux** (preferred for container support and lower cost)\n\n</code></pre>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#8-what-is-the-importance-of-service-plan-and-pricing-tier","title":"8. What is the importance of Service plan and pricing tier?","text":"<p>When deploying a web application on Azure App Services, choosing the right App Service Plan and Pricing Tier is crucial. It directly impacts your app's performance, scalability, availability, cost, and feature set.</p> <p>\ud83d\udccc 1. What is an App Service Plan?</p> <p>An App Service Plan defines:</p> <ul> <li>How much your app pays (pricing tier)</li> <li>What kind of resources are available (CPU, memory, storage)</li> <li>How your app runs (Windows/Linux, regional deployment)</li> <li>How many apps you can host in the same plan</li> </ul> <p>\ud83d\udca1 2. Importance of Choosing the Right Pricing Tier</p> <p>\u2705 Performance and Resources</p> <ul> <li>Higher tiers offer more CPU, memory, and storage.</li> <li>Directly affects how many users your site can handle without crashing.</li> </ul> Tier CPU &amp; Memory Scaling Ideal For Free Very Low None Learning/testing Basic Low Manual Low-traffic apps Standard Medium Auto Production apps Premium High Auto High-traffic or enterprise apps Isolated Very High Auto Secure, VNet-integrated workloads <p>\ud83d\udd10 Features and Capabilities</p> <ul> <li>Only certain tiers support custom domains, SSL, auto-scaling, staging slots, VNet integration, etc.</li> </ul> Feature Free Basic Standard Premium Isolated Custom Domain \u274c \u2705 \u2705 \u2705 \u2705 SSL Support \u274c \u2705 \u2705 \u2705 \u2705 Auto-scaling \u274c \u274c \u2705 \u2705 \u2705 Staging Slots \u274c \u274c \u2705 (5) \u2705 (20) \u2705 (20) VNet Integration \u274c \u274c \u2705 \u2705 \u2705 <p>\ud83d\udcb0 Cost Optimization</p> <ul> <li>Underprovisioning can lead to performance issues, while overprovisioning leads to unnecessary cost.</li> <li>Pricing tier affects billing per hour whether the app is being used or not.</li> </ul> <p>\u26a0\ufe0f You are billed for the entire App Service Plan, not per app. All apps hosted under the same plan share the same resources.</p> <p>\ud83d\udd04 Scalability</p> <ul> <li>Only Standard and higher tiers support horizontal scaling (adding more instances).</li> <li>Needed when your website gets high traffic or needs high availability.</li> </ul> <p>\ud83d\udee0\ufe0f DevOps and CI/CD Support</p> <ul> <li>Deployment slots (Standard+) allow zero-downtime deployments and easy rollback.</li> <li>Premium tiers support more slots, ideal for multi-stage environments (dev/stage/prod).</li> </ul> <p>\ud83c\udfaf Final Thoughts</p> <p>Choosing the right App Service Plan and Pricing Tier ensures:</p> <ul> <li>\u2705 Optimal user experience</li> <li>\u2705 Reliable performance under load</li> <li>\u2705 Cost-effective infrastructure</li> <li>\u2705 Access to enterprise-level features</li> </ul> <p>\ud83d\udd3d Recommendation: For most production web apps, start with Standard S1 or Premium P1v3, and scale based on usage and growth.</p>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#9-how-to-upload-a-site-by-using-ftp-on-azure","title":"9. How to upload a site by using FTP on Azure ?","text":"<p>Azure App Service provides FTP/S access to deploy website files manually. This method is especially useful for uploading static content or performing quick file edits.</p> <p>\ud83d\udcc1 Prerequisites</p> <ul> <li>An Azure App Service Web App is already created.</li> <li>Your website files (e.g., HTML, CSS, JS, etc.) are ready.</li> <li>An FTP client like FileZilla, WinSCP, or Visual Studio Code with FTP extension.</li> </ul> <p>\ud83d\udd10 Step 1: Get FTP Deployment Credentials</p> <ol> <li>Go to the Azure Portal \u2192 App Services \u2192 Select your Web App.</li> <li>Under Deployment &gt; click Deployment Center (or FTP tab).</li> <li>Click on User Credentials or Application Scope Credentials.</li> <li>Take note of:</li> <li>FTP Hostname</li> <li>FTP Username</li> <li>Set or view your FTP Password</li> </ol> <p>Alternatively, under \"Overview\" \u2192 \"Essentials\", note down the FTP/FTPS hostname.</p> <p>\ud83d\udcbe Step 2: Prepare FTP Client (Example: FileZilla)</p> <ol> <li>Open FileZilla.</li> <li> <p>Go to File \u2192 Site Manager and add a new site:</p> </li> <li> <p>Protocol: FTP or FTPS (recommended)</p> </li> <li>Host: <code>&lt;yourappname&gt;.ftp.azurewebsites.windows.net</code></li> <li>Port: 21 (FTP) or 990 (FTPS)</li> <li>Logon Type: Normal</li> <li>User: <code>\\&lt;app-name&gt;\\$\\&lt;username&gt;</code> (from Azure)</li> <li> <p>Password: Your FTP password</p> </li> <li> <p>Click Connect.</p> </li> </ol> <p>\ud83d\udcc2 Step 3: Upload Website Files</p> <ol> <li>Once connected, navigate to: /site/wwwroot/</li> </ol> <p>This is the root folder of your web app.</p> <ol> <li> <p>On the left panel, browse your local site files.</p> </li> <li> <p>Drag and drop your website files into the <code>/site/wwwroot</code> directory.</p> </li> </ol> <p>\u26a0\ufe0f Uploading files here overwrites existing content. Be cautious when deploying to a live site.</p> <p>\u2705 Step 4: Verify the Upload</p> <ul> <li> <p>Go to your web app's URL in a browser: This is the root folder of your web app.</p> </li> <li> <p>On the left panel, browse your local site files.</p> </li> <li> <p>Drag and drop your website files into the <code>/site/wwwroot</code> directory.</p> </li> </ul> <p>\u26a0\ufe0f Uploading files here overwrites existing content. Be cautious when deploying to a live site.</p> <p>\u2705 Step 4: Verify the Upload</p> <ul> <li> <p>Go to your web app's URL in a browser: https://.azurewebsites.net <li> <p>You should see your uploaded site running live.</p> </li> <p>\ud83d\udd04 Optional: Enable FTP Logs</p> <p>If facing issues, enable diagnostic logs:</p> <ol> <li>Go to App Service \u2192 Monitoring \u2192 App Service logs.</li> <li>Enable Application Logging (Filesystem) and FTP Logs for troubleshooting.</li> </ol> <p>\ud83d\udccc Notes</p> <ul> <li>FTP is convenient for quick uploads but not recommended for production CI/CD workflows.</li> <li>Use Azure DevOps, GitHub Actions, or ZipDeploy for automated and secure deployments.</li> <li>Ensure you use FTPS over FTP for encrypted upload.</li> </ul> <p>\ud83d\udcda Related Docs</p> <ul> <li>Deploy via FTP</li> <li>Configure deployment credentials</li> </ul> <p>\u2705 Summary</p> Step Action 1 Get FTP credentials from Azure Portal 2 Connect via FTP client 3 Upload files to <code>/site/wwwroot/</code> 4 Test website"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#10-what-is-the-importance-of-wwwroot-folder","title":"10. What is the importance of \u201cwwwroot\u201d folder ?","text":"<p>The <code>wwwroot</code> folder is a critical directory in Azure App Service and many web frameworks (like ASP.NET Core). It serves as the public root of your web application.</p> <p>\ud83d\udcc1 What is <code>wwwroot</code>?</p> <p><code>wwwroot</code> is the web root directory for a hosted web app.</p> <ul> <li>All files placed inside <code>wwwroot</code> are publicly accessible via HTTP/S.</li> <li>It is the default location where Azure App Service looks to serve content for incoming web requests.</li> </ul> <p>\ud83d\udd17 Example: If your file <code>index.html</code> is located in <code>/site/wwwroot/index.html</code>, it will be accessible at:</p> <p><code>https://&lt;yourappname&gt;.azurewebsites.net/index.html</code></p> <p>\ud83c\udfaf Why is <code>wwwroot</code> Important?</p> <ol> <li> <p>\ud83d\udce4 Deployment Target</p> </li> <li> <p>All FTP, zip deployment, or build pipelines upload your app to <code>wwwroot</code>.</p> </li> <li> <p>It\u2019s the entry point for your app on Azure App Service.</p> </li> <li> <p>\ud83c\udf0d Public Access Area</p> </li> <li> <p>Files in <code>wwwroot</code> are exposed to users.</p> </li> <li>Ideal for placing:</li> <li>Static HTML files</li> <li>JavaScript files</li> <li>CSS</li> <li>Images</li> <li>Frontend bu</li> <li>\ud83d\uddc2\ufe0f Directory Structure in Azure App Service   When connecting via FTP or Kudu (Advanced Tools), you typically see this structure:   /site   \u251c\u2500\u2500 deployments   \u251c\u2500\u2500 diagnostics   \u251c\u2500\u2500 locks   \u2514\u2500\u2500 wwwroot \u2190 Your application\u2019s actual web content lives here</li> </ol>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#11-how-can-you-go-to-the-console-of-azure","title":"11. How can you go to the console of Azure ?","text":"<p>Azure provides a built-in Kudu Console (also known as SCM) for App Services. This powerful tool gives you terminal-like access to your app's environment.</p> <p>\ud83e\udded Steps to Access the Azure Console for App Service</p> <p>\u2705 Method 1: Using Azure Portal</p> <ol> <li> <p>Login to Azure Portal    Go to https://portal.azure.com</p> </li> <li> <p>Navigate to App Services</p> </li> <li> <p>Click on App Services from the left menu.</p> </li> <li> <p>Select your Web App (e.g., <code>my-web-app</code>).</p> </li> <li> <p>Go to Advanced Tools (Kudu)</p> </li> <li> <p>In the left pane, scroll to Development Tools.</p> </li> <li> <p>Click on Advanced Tools &gt; Click Go.</p> </li> <li> <p>Kudu Portal Opens in a New Tab    You\u2019ll be redirected to: https://.scm.azurewebsites.net <li> <p>Open Console</p> </li> <li> <p>In the top menu, click on Debug Console \u2192 CMD or PowerShell.</p> </li> <li> <p>\u2705 You now have terminal access to your app's file system and environment.</p> </li> <p>\u2705 Method 2: Direct URL Access to Kudu</p> <p>You can directly access the console using this URL format:</p> <p>https://.scm.azurewebsites.net/DebugConsole <p>\ud83d\udccc Replace <code>&lt;yourappname&gt;</code> with your actual Azure Web App name.</p> <p>\ud83d\udd27 What Can You Do in Azure Console?</p> <ul> <li>Browse and edit files (e.g., wwwroot)</li> <li>Run PowerShell or CMD commands</li> <li>Deploy apps manually (zip deploy)</li> <li>View environment variables</li> <li>Inspect logs and temp files</li> </ul> <p>\ud83d\udeab Limitations</p> <ul> <li>Only available for App Service (not for VMs or AKS).</li> <li>Has access only to the App Service sandboxed environment.</li> <li>Cannot access full Azure environment or other services (e.g., DB, storage directly).</li> </ul> <p>\ud83d\udee1\ufe0f Security Note</p> <p>Only users with proper Azure portal access (e.g., Contributor or higher role) can access the console.</p> <p>\u2705 Summary</p> Feature Description Tool Used Kudu (Advanced Tools) Console Access Method Azure Portal \u2192 App Service \u2192 Advanced Tools \u2192 Go Direct URL <code>https://&lt;appname&gt;.scm.azurewebsites.net/DebugConsole</code> Modes CMD or PowerShell Use Cases File edits, logs, manual deploys, environment checks"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#12-what-is-the-need-to-app-service-editor","title":"12. What is the need to App Service editor ?","text":"<p>\u270d\ufe0f What is the Need for Azure App Service Editor?</p> <p>The App Service Editor in Azure is a browser-based IDE that allows you to edit your web app\u2019s code directly in the cloud \u2014 without needing to re-deploy from a local machine.</p> <p>\ud83d\ude80 What is App Service Editor?</p> <ul> <li>A lightweight web-based code editor, built on Monaco (same engine as VS Code).</li> <li>Runs directly inside your App Service environment.</li> <li>Allows real-time edits to your files under <code>/site/wwwroot</code>.</li> </ul> <p>\ud83e\udde0 Why Do You Need It?</p> <p>\u2705 1. Quick Fixes in Production</p> <p>You can:</p> <ul> <li>Instantly update HTML, CSS, JavaScript, or config files.</li> <li>Apply minor bug fixes without redeploying from GitHub or DevOps.</li> </ul> <p>\u26a0\ufe0f Not recommended for large production changes, but useful for hotfixes or debugging.</p> <p>\u2705 2. Real-Time Editing</p> <ul> <li>Live preview and edit of files deployed in your app.</li> <li>Immediate effect without rebuilding or publishing.</li> </ul> <p>\u2705 3. No Local Setup Required</p> <ul> <li>No need to clone repo or set up a development environment.</li> <li>Works entirely in the browser from any location.</li> </ul> <p>\u2705 4. Debug and Inspect Files</p> <ul> <li>Browse the complete file system of your app.</li> <li>Useful for checking deployed builds or log files.</li> </ul> <p>\u2705 5. Developer Convenience</p> <ul> <li>Ideal for small frontend changes in static sites or single-page apps.</li> <li>Especially useful for demos, proof-of-concepts, and MVP apps.</li> </ul> <p>\ud83d\udd13 How to Open App Service Editor</p> <ol> <li>Go to Azure Portal \u2192 App Services</li> <li>Select your App \u2192 Under Development Tools, click App Service Editor</li> <li>Click Go, and it will open:https://.scm.azurewebsites.net/dev <p>\u26a0\ufe0f Limitations</p> Limitation Notes Not version controlled Manual changes won't be in Git or DevOps pipelines No CI/CD integration Bypasses standard deployment pipelines Not ideal for .NET Editing compiled backends like ASP.NET Core is limited Not scalable Not recommended for large or team-based projects <p>\u2705 When to Use App Service Editor</p> Scenario Use App Service Editor? Fixing a typo in HTML/CSS \u2705 Yes Adding a console log in JS \u2705 Yes Changing <code>appsettings.json</code> \u26a0\ufe0f Temporary, not ideal Refactoring backend logic \u274c No (use IDE + deploy) Managing large projects \u274c No <p>\ud83d\udccc Summary</p> <p>The App Service Editor is a lightweight, web-based code editor useful for:</p> <ul> <li>Quick edits</li> <li>Debugging</li> <li>Fast fixes without redeploy</li> </ul> <p>Best used for frontend changes and static content updates, not for major backend development.</p>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#13-explain-the-importance-of-publish-profile-of-azure","title":"13. Explain the importance of publish profile of Azure ?","text":"<p>\ud83d\udce4 Importance of Azure Publish Profile</p> <p>An Azure Publish Profile is an XML configuration file that contains all the necessary settings and credentials needed to deploy your web application to Azure App Service directly from development tools like Visual Studio, VS Code, or CI/CD pipelines.</p> <p>\ud83e\uddfe What is a Publish Profile?</p> <ul> <li>A <code>.PublishSettings</code> file downloaded from the Azure Portal.</li> <li>Contains:</li> <li>Web app name</li> <li>Deployment method (Web Deploy, FTP)</li> <li>Credentials (usernames/passwords)</li> <li>Target URL</li> <li>Configuration settings</li> </ul> <p>\ud83d\udcc1 File format: <code>appname.PublishSettings</code></p> <p>\ud83c\udfaf Why is a Publish Profile Important?</p> <p>\u2705 1. Simplifies Deployment</p> <ul> <li>Enables one-click publishing from Visual Studio or VS Code.</li> <li>No need to manually configure FTP or deployment endpoints.</li> </ul> <p>\ud83c\udfaf Especially useful for developers deploying from local environments.</p> <p>\u2705 2. Secure Deployment Credentials</p> <ul> <li>Includes deployment credentials for FTP/WebDeploy in encrypted form.</li> <li>You don\u2019t need to remember or manually enter credentials each time.</li> </ul> <p>\u2705 3. Supports Multiple Deployment Methods</p> <ul> <li>Works with:</li> <li>Web Deploy (MSDeploy): Common for .NET apps via Visual Studio.</li> <li>FTP/S: For manual uploads.</li> <li>CI/CD Pipelines: Can be used in GitHub Actions or Azure DevOps.</li> </ul> <p>\u2705 4. Consistent and Repeatable Deployments</p> <ul> <li>Developers and build servers use the same configuration, reducing setup errors.</li> <li>Ideal for teams where multiple people deploy the same app.</li> </ul> <p>\u2705 5. Integrates with Dev Tools</p> <ul> <li>Visual Studio automatically reads and configures deployment settings after importing the profile.</li> <li>Works with MSBuild, GitHub Actions, Azure DevOps, etc.</li> </ul> <p>\ud83d\udd10 Security Considerations</p> <ul> <li>\u26a0\ufe0f Do not commit the publish profile file to source control (e.g., GitHub).</li> <li>Treat it like a password file \u2014 it contains encoded credentials.</li> <li>Rotate or regenerate if exposed.</li> </ul> <p>\ud83e\udded How to Download Publish Profile</p> <ol> <li>Go to Azure Portal \u2192 App Services</li> <li>Select your app</li> <li>Click \"Get Publish Profile\" on the Overview blade</li> <li>A <code>.PublishSettings</code> file will be downloaded</li> </ol> <p>\ud83d\udee0\ufe0f How to Use in Visual Studio</p> <ol> <li>In Visual Studio, right-click your project \u2192 Publish</li> <li>Click \"Import Profile\"</li> <li>Browse and select the downloaded <code>.PublishSettings</code> file</li> <li>Click Publish</li> </ol> <p>\u2705 Summary</p> Feature Benefit Deployment Configuration Contains all required settings Easy Integration with IDEs Works seamlessly with Visual Studio, VS Code Supports Multiple Methods Web Deploy, FTP, MSBuild Simplifies CI/CD Use in automated pipelines Security Risk if Misused Should never be checked into version control <p>\ud83d\udd10 Always treat your publish profile like a sensitive credential. Use Key Vault or secrets in CI/CD when needed.</p>"},{"location":"Cloud/Azure/IAAS_PASS_SAAS/#14-how-to-deploy-application-in-azure-web-app","title":"14. How to deploy application in Azure Web app?","text":"<p>Answer</p> <ul> <li>Please find the below tutorial do deploy application in Azure Web App.   Deployment link</li> </ul>"},{"location":"Dotnet/csharp/advanced/","title":"Advanced Topics in C","text":"<p>This section explores delegates, events, LINQ, async/await, and other advanced features.</p>"},{"location":"Dotnet/csharp/basics/","title":"Basics of C","text":""},{"location":"Dotnet/csharp/oop/","title":"OOP Concepts in C","text":"<p>This section explains classes, objects, inheritance, polymorphism, encapsulation, and abstraction.</p>"},{"location":"Poject_Management/Jira/Jira_detailed_course/","title":"JiraDetailed","text":""},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-jira","title":"what is JIRA?","text":"<ul> <li>JIRA uses Agile, it is a project management tool</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-an-agile","title":"What is an agile?","text":"<ul> <li>A software developemtn methodology focuses in empowering teams to efficientlt organize their work through self-managment and collaboration</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-an-agile-board","title":"What is an Agile Board?","text":"<ul> <li>They are two types of Agile baords<pre><code>- Scrum and kanban\n</code></pre> </li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-scrum","title":"What is scrum?","text":"<ul> <li>An agile framework that uses sprints to get work done.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-sprint","title":"What is sprint?","text":"<ul> <li>A sprint is a pre-determined amount of time for your team ot get a pre-deermined amounut of work done.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-happens-when-we-get-some-another-hours-of-work-in-the-mid-of-sprint","title":"what happens when we get some another hours of work in the mid of sprint","text":"<ul> <li>The best practice for this is we can add the task in the backlog or if we want to do in current print the same amount of working hours task has to move to backlog and bring this task to current sprint.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-kanban-board","title":"What is Kanban board","text":"<ul> <li>An agile framework that uses a continuos workflow instead of sprints to get work done.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-a-stand-ups","title":"What is a stand-ups?","text":"<ul> <li>A stand-up meeting is a fase meeting(so fast that no one needs tp sit down) every morning where everyone on the team review wheat they did the day before, hwat they'll do today and any roadblocks they're having.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-a-story","title":"What is a Story","text":"<ul> <li> <p>In Agile it follows the below the principle for creating a story and that is the reason it is called as UseR Story</p> <pre><code>-As a &lt;type of user&gt;, I want &lt;some goal&gt; so that &lt;some reason&gt;\n</code></pre> </li> <li> <p>Creating a new issue in Jira, the Story issue type referes to an agile term known as User stories</p> </li> <li> <p>story is a default issue type in JIRA</p> </li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-epic","title":"What is EPIC","text":"<ul> <li> <p>A epic is how you tack multiple stories that correlate with each other. this helps everyone on the team see the progress of the bigger picture.</p> </li> <li> <p>Under Epic we have stories</p> </li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-an-issue","title":"What is an issue","text":"<ul> <li>Issues are at hte heart of what you need to track. They are contianers for all the fields in jira.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-a-project","title":"What is a Project","text":"<ul> <li>Projects are containets for issues. They are how you configure issue types, fields, etc..,</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#team-managed-projects","title":"Team Managed Projects","text":"<ul> <li>In team managed projects. the project scope entities (issues types, statuses, fields, etc) that live only with in a single team - managed project.</li> <li>it is managed by anyone in the team.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#company-managed-projects","title":"Company managed projects","text":"<ul> <li>In Company Projects, the entities are created globally and this entities are shared by all the projects.</li> <li>it is managed by jira administrator.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#how-to-turn-kanban-board-to-scrum-board","title":"How to turn kanban board to Scrum board?","text":"<ul> <li>basically we the difference b/w both is the scrum have backlogs and sprints, so we need to turn on the below features</li> <li>Features : Backlog, sprints, timeline</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-story-points","title":"What is Story Points","text":"<ul> <li>A unit of measurement that is defined by your team (Similar to sprints) to determine the overall amount of work, complexity, and uncerainty involved.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#how-the-issues-are-stories-are-dispalyed-in-calendar","title":"How the issues are stories are dispalyed in calendar","text":"<ul> <li>if the issue has the due date it is automatically displayed in calendar view.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-the-name-of-the-location-where-goals-are-managed-in-jira","title":"What is the name of the location where goals are managed in Jira?","text":"<ul> <li>Atlassian Home.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-a-release","title":"What is a release","text":"<ul> <li>Releases are built-in way of tracking the issues that are associated with different versions of your software.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-a-component","title":"What is a component?","text":"<ul> <li>Components are a built-in way of tracking the issues that are associated with differen functionality or areas of focus.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-jql","title":"What is JQL","text":"<ul> <li>JQL is jira Query language, with this we can give queries to the filters.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#this-is-the-name-of-the-feature-that-adds-a-horizontal-swimlane-across-aa-jira-board","title":"This is the name of the feature that adds a horizontal swimlane across aa jira board.","text":"<ul> <li>GroupBy.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-permission-will-allow-you-to-settup-a-new-board-admin","title":"What permission will allow you to settup a new board admin?","text":"<ul> <li>Project admin permissions.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#how-do-you-configure-the-issues-visible-on-board","title":"How do you configure the issues visible on board?","text":"<ul> <li>Filters</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#on-a-scrum-board-what-is-displayed-under-the-breadcrumb","title":"On a scrum board, what is displayed under the breadcrumb?","text":"<ul> <li>Sprint name</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#whats-another-name-that-jira-uses-for-releases-in-the-interface","title":"what's another name that jira uses for releases in the interface?","text":"<ul> <li>versions</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#when-you-save-a-search-in-jira-whats-that-called","title":"When you save a search in Jira, what's that called?","text":"<ul> <li>Filter</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#filters-control-which-issues-drive-what-feature-on-the-dashboard","title":"Filters control which issues drive what feature on the dashboard?","text":"<ul> <li>Gadjets.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#reports-in-jira-are-attached-to-what-feature","title":"Reports in Jira are attached to what feature?","text":"<ul> <li>Boards.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#when-creating-a-team-yu-can-only-add-users-already-in-your-company-managed-projects","title":"When creating a team, yu can only add users already in your company- managed projects?","text":"<ul> <li>False</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#whats-the-first-step-to-doing-a-buld-edit-in-jira","title":"What's the first step to doing a buld edit in jira?","text":"<ul> <li>Searching for the issues to edit.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#product-acceess","title":"Product Acceess","text":"<ul> <li>Active users are those with prduct access. For example, Jira, BitBucket, And Confluence are all different Products.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-workflows","title":"What is Workflows","text":"<ul> <li>Workflows are used to control the statuses of issues and how they trnsistion from one status to another.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#where-do-you-go-to-turn-off-the-calendar-view-in-a-company-mnanaged-project","title":"**Where do you go to turn off the calendar view in a company- mnanaged project?","text":"<ul> <li>Features</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-do-workflows-control-in-jira","title":"What do workflows control in jira?","text":"<ul> <li>Statuses and transitions</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#after-you-create-a-custom-issue-type-how-do-yoi-make-it-show-up-in-a-comapny-manged-project","title":"After you create a custom issue type, how do yoi make it show up in a comapny-manged project?","text":"<ul> <li>With a scheme.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#after-you-create-a-custom-filed-how-do-you-make-it-show-up-on-an-issue-type-in-a-company-managed-project","title":"After you create a custom filed, how do you make it show up on an issue type in a company-managed Project?","text":"<ul> <li>with a scheme.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-the-proper-sequence-for-an-autamtion-rule-in-jira","title":"What is the proper sequence for an autamtion rule in jira?","text":"<ul> <li>trigger, condition, action</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-does-jira-call-their-third-party-add-ons-that-you-can-purchase-from-marketplace","title":"What does jira call their third-party add-ons that you can purchase from marketplace?","text":"<ul> <li>apps</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#where-do-you-go-to-see-all-the-changes-made-by-users-across-jira","title":"Where do you go to see all the changes made by users across jira?","text":"<ul> <li>Audit log</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#burndown-chart","title":"Burndown Chart","text":"<ul> <li>What it is: Diagram of actual and estimated amount of work to be done in a sprint.</li> <li>What it tells Us: How team is delivering work during a sprint. The likelihood of achieving the sprint goal.</li> <li>Why it is uses To track sprint progress and identify issues.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#burnup-chart","title":"Burnup chart","text":"<ul> <li>What it is : Diagram of totalscope compared to amount of work completed in a sprint.</li> <li>What it tells us: How team mkaed progress towards the total scope with in a sprint , the amount of scope creep.</li> <li>Why use it: To track sprint progress and identitfy scope creep.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#sprint-report","title":"sprint Report","text":"<ul> <li>What it is : Summary of the sprint progress to date</li> <li>What it tells us: What the team delivered (and didn't deliver) for a specified sprint.</li> <li>Why use it: To view a summary of sprint.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#epic-report","title":"Epic Report","text":"<ul> <li>What it is : Diagram of complete, incomplete, and non estimated issues in a epic</li> <li>What it tells us: Trending progress toward completing an epic over time and also track remaining work</li> <li>Why use it: To plan work for an epic that may span mutiple sprints.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#velocity-report","title":"Velocity Report","text":"<ul> <li>What it is : Chart of planned work vs compelted work for mutiple sprints</li> <li>What it tells us: What team commits to and delivers each sprint</li> <li>Why use it: To identify and address trends in team commitment and delivery.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#control-report","title":"Control Report","text":"<ul> <li>What it is : Diagram of cycle time in each workflow state for project</li> <li>What it tells us: Where bottlenecks and slow downs occurs in workflow</li> <li>Why use it:To identify and address workflow design prblems and to enable moore predictale estimatons.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#average-age-report","title":"Average age Report","text":"<ul> <li>What it is : Displays average age of unresolved issues for project or filter</li> <li>What it tells us: Whether baclog is kept up to date</li> <li>Why use it:To identify aging issues so backlog can be updated.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#which-jira-software-reports-would-you-use-to-determine-if-your-team-will-acheive-their-current-sprint-goal","title":"which jira software reports would you use to determine if your team will acheive their current sprint goal","text":"<ul> <li>Burndown chart</li> <li>sprint report</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#which-jira-aglie-report-would-you-use-to-estimate-when-a-version-will-be-complete","title":"which jira aglie report would you use to estimate when a version will be complete","text":"<ul> <li>Release Burndown</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#which-tow-jira-agile-reports-provide-forecastin","title":"Which tow jira agile reports provide forecastin?","text":"<ul> <li> <p>Release Burndown</p> </li> <li> <p>Version report</p> </li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-is-a-gadget","title":"What is a Gadget?","text":"<ul> <li>Dispaly protect/issue information</li> <li>visualize information</li> <li>customizable</li> <li>centralized location</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#jira-administrators-are-the-only-ones-that-can-create-dashboards-if-you-need-a-gadget-added-to-your-dashboard-contact-your-jira-administrator","title":"Jira administrators are the only ones that can create dashboards. if you need a gadget added to your dashboard, contact your jira administrator","text":"<ul> <li>FALSE Any jira use can create a dashboard and can add gadjets to a dashboard</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#as-the-scrum-master-you-want-to-compre-how-well-your-team-does-at-meeting-their-goal-for-the-last-five-sprints-in-an-epic-which-gadget-or-report-would-you-use","title":"As the scrum master, you want to compre how well your team does at meeting their goal for the last five sprints in an epic. which gadget or report would you use?","text":"<ul> <li>Velocity chart</li> <li>Epic burndown chart</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#as-the-scrum-master-you-need-to-help-your-team-estimate-how-many-story-points-the-team-can-complete-in-the-next-sprint-which-report-or-gadget-would-you-use-for-this-task","title":"As the scrum Master, You need to help your team estimate how many story points the team can complete in the next sprint, which report or gadget would you use for this task?","text":"<ul> <li>velocity chart</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#jira-products","title":"JIRA Products","text":"<ul> <li>JIRA Software</li> <li>JIRA Service Management</li> <li>Jira Core</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#cloud-vs-data-center","title":"Cloud V/s Data Center","text":"<ul> <li>Is for the organizations that don't want to maintain servers, softwares upgrades, and tasks that an IT system administrator, serer administrators and a host of other experts must take care of.</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#data-center","title":"Data center","text":"<ul> <li>Is used by business that need to host their own data and applications</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#what-will-happen-if-you-exceed-the-number-of-licensed-users","title":"what will happen if you exceed the number of licensed users?","text":"<ul> <li>Users wont able to create new issues</li> </ul>"},{"location":"Poject_Management/Jira/Jira_detailed_course/#jira-roles","title":"JIRA Roles","text":"<p>Projet administrator</p> <p>What can they do</p> <ul> <li>Assign project members to project roles</li> <li>Edit project details</li> <li>Edit workflow and screens</li> <li>create and mamage versions and components</li> </ul> <p>Who can be assigned</p> <ul> <li>one or more users and/or groups</li> </ul> <p>by default, the project administrator rile is granted to the jira administrators groups when project is created.</p> <p>Project Lead</p> <p>What they can do</p> <ul> <li>manage the project</li> <li>used in schemas and more</li> </ul> <p>Who can be assigned?</p> <ul> <li>only one use</li> <li>any user can be assigned</li> </ul> <p>by default, the jira admin who created the projet is assigned as the project lead</p> <p>Default assignee</p> <p>What they can do?</p> <ul> <li>assigned to issues if no other user is assigned to the issue on creation.</li> </ul> <p>who can be assinged?</p> <ul> <li>project lead, componen lead, or unassigned</li> </ul> <p>default issue assignes will be set according to system, componenent, or project settings.</p> <p>Borad Administrator</p> <p>What they can do?</p> <ul> <li>Configure boards(change columns, configure swimlanes)</li> </ul> <p>Who can be assigned?</p> <ul> <li>one or more users or groups.</li> </ul> <p>the user who created the board is automaticallu granted the board administrator role.</p>"},{"location":"Poject_Management/Jira/basics/","title":"Jira Interview Questions","text":""},{"location":"Poject_Management/Jira/basics/#1-what-is-jira","title":"1. What is JIRA?","text":"<p>Answer:</p> <ul> <li>Jira is a project management software to help teams plan, organize, manage, and track their work</li> <li>it's used by teams of all kinds, from engineering to marketing and has advanced features to make it the complete PM tool.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#2-how-does-your-team-use-jira","title":"2. How does your team use JIRA?","text":"<p>Answer:</p> <ul> <li>Managing work in Jira helps your tema be moew transparent, efficient, and collaborative.</li> <li>Transparency: Everyone on your team can view and ocment on your work, You don't nee dot work in silos.</li> <li>Efficneinecy: your team can plan a head by breaking down big projects and knowing exactly when work will start and end.</li> <li>Colllaboration: You can woek closely with you teammates, organizing your discussions and decisions.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#3-what-is-workitems","title":"3. What is workitems?","text":"<p>Answer</p> <ul> <li>In Jira, Individual pieces of woek are work items. You'll updae woek items to show work progress or relevant information, like notes from a meeting or a question for a teammate.</li> <li>Work items can vary in size depending on how your tea uses them, Some work items take months to complete. Others finish in a few hours. Like most things in Jira, you can use work items however works best for the team</li> </ul>"},{"location":"Poject_Management/Jira/basics/#4-what-is-a-project","title":"4. What is a project?","text":"<p>Answer</p> <ul> <li>A Proejct is a collection of realted woek items. In Jira, every work items belomgs to a project.</li> <li>Teams can use projects differenctly depending on how they watn to categorize and organize work.</li> <li>Project keys are shoet version of the project name that identify the work items in that project.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#5-what-is-a-board","title":"5. What is a board?","text":"<p>Answer</p> <ul> <li>A board is a visulaization of hte work in a project. Boards have columns with work items in them. The Columns represent the statuses that your work, represented by woek items, moves through. Your entire board represnts you team's workflow, the path of statuses an work items moves through.</li> </ul> <p>For Example: Kate's team uses the marketing team board to track their work. Their work moves through three satuses, To DO, In Progress, and Done. Their work moves through three columns, one for each status. When kates starts working on a work item, she moves it form the TO Do Column to the in progress Column.</p> <ul> <li>Boards belong to the Projects. A single Jira project can have several boards depending on its Configuration.   For Example: The Legal department has three boards in their project:</li> <li>Contract Team, Acquistions Team, and patents Team. Each boards traxks work for a different team within their department.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#6-whar-are-the-ytpes-of-work","title":"6. Whar are the ytpes of work?","text":"<p>Answer</p> <ul> <li>With a wide variety of work, teams need differetn ways to categorize it. The work type tells you the category and size of the tasks a work item represents.   For Example The subtask woek type indicates this woek is just one piece of a larger deliverable.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#7-what-is-an-epic","title":"7. what is an epic?","text":"<p>Answer</p> <ul> <li>An epic represents large body of work that can break down ito smaller chunks.   For example You might create an epic for a redesign of a homepage, or writing and publishing a series of blog posts.</li> <li>know as \"paretn\" work items, epics contain smaller work items within them. You and your team can decide what's a large bodu of woek and what isn't.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#8-what-is-a-story","title":"8. What is a Story?","text":"<p>Answer</p> <ul> <li>A Story is a deliverable from the user's perspective. Theyu define work items in non-techincal language.   For Example A story titled \"Design retrun function\" might have this description:\"As a user, I need a back button on this screen\"</li> </ul>"},{"location":"Poject_Management/Jira/basics/#9-what-is-task","title":"9. what is Task?","text":"<p>Answer</p> <ul> <li>A task cointains a detailed description of a work item, usually from your perspective.   For Example: A task titled \"Review Syrvery data\" might have a list of metrics that you want to analyze and specific requirements for your review.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#10-what-is-a-bug","title":"10. what is a Bug?","text":"<p>Answer</p> <ul> <li>A bug describes a problem or error.   For Example: You might use a bug to represent the work needed to investigate and determine a solution for a broken button on your website.</li> </ul>"},{"location":"Poject_Management/Jira/basics/#11-what-is-subtask","title":"11. what is subtask?","text":"<p>Anwer</p> <ul> <li>A subtask is the smallest piece of work, just one step toward completing a larger work item.   For Example: You might have a story called \" Send markteting email\" with several subtasks. One of the subtasks could be \"Test subject lines for effectiveness.\"</li> <li>Subtaks must have a parent work item. They can't exist on their own.</li> </ul>"},{"location":"React/hooks/","title":"React Hooks Interview Questions","text":""},{"location":"React/hooks/#1-what-is-usestate","title":"1. What is useState?","text":"<p>Answer: <code>useState</code> is a React Hook that lets you add state to functional components.</p> <p>Sample Code:</p> <pre><code>const [count, setCount] = useState(0);\n&lt;button onClick={() =&gt; setCount(count + 1)}&gt;Click&lt;/button&gt;;\n</code></pre>"},{"location":"React/state/","title":"React State Management","text":"<ul> <li>Explain Context API.</li> <li>When should you use Redux?</li> </ul>"},{"location":"Scenarios/migration/","title":"Legacy Migration Scenarios","text":"<ul> <li>How do you migrate a monolith to microservices?</li> </ul>"},{"location":"Scenarios/realworld/","title":"Real-World Q&amp;A","text":"<ul> <li>How do you handle versioning in an API?</li> </ul>"},{"location":"browserstack/bascis/","title":"\ud83d\udcd8 BrowserStack Documentation","text":""},{"location":"browserstack/bascis/#what-is-browserstack","title":"What is BrowserStack?","text":"<p>BrowserStack is a cloud-based cross-browser testing platform that allows developers and QA teams to test their web and mobile applications across a wide range of browsers, operating systems, and devices without the need for physical infrastructure.</p> <p>Key Features</p> <ul> <li>\u2705 Real Device Cloud: Test on actual devices (not emulators or simulators).</li> <li>\ud83c\udf10 Cross-Browser Testing: Run tests on 3000+ real browsers and OS combinations.</li> <li>\ud83d\udcf1 Mobile App Testing: Supports both Android and iOS app testing.</li> <li>\u26a1 Instant Access: No installations needed; access via browser.</li> <li>\ud83e\uddea Automated Testing: Integrates with Selenium, Appium, Cypress, Playwright, and more.</li> <li>\ud83d\udd01 Parallel Testing: Run multiple tests at the same time to reduce test execution time.</li> <li>\ud83d\udd10 Secure and Private: Data is encrypted, and environments can be firewalled.</li> </ul> <p>\ud83d\udee0\ufe0f Use Cases</p> <ol> <li>\u2705 Manual Testing of websites on different browsers and devices.</li> <li>\ud83e\udd16 Automated Regression Testing using CI/CD integrations.</li> <li>\ud83d\udcf1 Native App Testing across real mobile devices.</li> <li>\ud83d\udcf8 Screenshot Testing for visual layout and responsive design checks.</li> <li>\ud83e\udde9 Testing in Dev Environments with secure local testing options.</li> </ol> <p>\ud83d\udd17 Integrations</p> <p>BrowserStack integrates with:</p> <ul> <li>CI/CD tools like Jenkins, GitHub Actions, Azure DevOps, CircleCI</li> <li>Test frameworks like Selenium, Appium, TestNG, Cypress, Playwright</li> <li>Collaboration tools like Slack, Jira, Trello</li> </ul> <p>\ud83c\udf10 Official Website</p> <p>https://www.browserstack.com</p> <p>\ud83d\udcc4 Example Use in CI/CD Pipeline (YAML - GitHub Actions)</p> <pre><code>name: Run Tests on BrowserStack\n\non: [push]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Install Dependencies\n        run: npm install\n\n      - name: Run Selenium Tests on BrowserStack\n        run: npm run test:e2e\n        env:\n          BROWSERSTACK_USERNAME: ${{ secrets.BROWSERSTACK_USERNAME }}\n          BROWSERSTACK_ACCESS_KEY: ${{ secrets.BROWSERSTACK_ACCESS_KEY }}\n</code></pre> <p>\ud83d\udcb0 Plans &amp; Pricing BrowserStack offers:</p> <ul> <li>\ud83c\udd93 Free Trial: Limited access to devices, browsers, and testing time. Ideal for evaluation.</li> <li>\ud83d\udcbc Individual Plans: Basic features for solo developers or freelancers.</li> <li>\ud83d\udc65 Team Plans: Shared access, parallel test execution, CI integrations.</li> <li>\ud83c\udfe2 Enterprise Plans: Advanced features, dedicated support, SSO, unlimited testing, and security compliance.</li> <li>\ud83d\udcca Pricing Based On:</li> <li>Number of parallel test sessions</li> <li>Access to real mobile/desktop devices</li> <li>Automation vs Manual testing capabilities</li> <li>Team collaboration features</li> </ul> <p>\ud83d\udccc See detailed pricing here</p> <p>\ud83e\udde0 Summary</p> <p>BrowserStack helps ensure your applications work seamlessly across different platforms and devices without the need to maintain a costly, in-house test lab.</p> <p>Benefits include:</p> <ul> <li>\ud83d\ude80 Boosted test coverage across browsers/devices</li> <li>\ud83e\uddea Seamless integration with CI/CD pipelines</li> <li>\u23f1\ufe0f Reduced development and testing cycles</li> <li>\ud83d\udd12 Secure, scalable infrastructure</li> <li>\ud83d\udc69\u200d\ud83d\udcbb Improved end-user experience with real-world testing</li> </ul> <p>BrowserStack offers:</p> <ul> <li>\ud83c\udd93 Free Trial: Limited access to devices, browsers, and testing time. Ideal for evaluation.</li> <li>\ud83d\udcbc Individual Plans: Basic features for solo developers or freelancers.</li> <li>\ud83d\udc65 Team Plans: Shared access, parallel test execution, CI integrations.</li> <li>\ud83c\udfe2 Enterprise Plans: Advanced features, dedicated support, SSO, unlimited testing, and security compliance.</li> <li> <p>\ud83d\udcca Pricing Based On:</p> </li> <li> <p>Number of parallel test sessions</p> </li> <li>Access to real mobile/desktop devices</li> <li>Automation vs Manual testing capabilities</li> <li>Team collaboration features</li> </ul> <p>\ud83d\udccc See detailed pricing here</p> <p>\ud83e\udde0 Summary</p> <p>BrowserStack helps ensure your applications work seamlessly across different platforms and devices without the need to maintain a costly, in-house test lab.</p> <p>Benefits include:</p> <ul> <li>\ud83d\ude80 Boosted test coverage across browsers/devices</li> <li>\ud83e\uddea Seamless integration with CI/CD pipelines</li> <li>\u23f1\ufe0f Reduced development and testing cycles</li> <li>\ud83d\udd12 Secure, scalable infrastructure</li> <li>\ud83d\udc69\u200d\ud83d\udcbb Improved end-user experience with real-world testing</li> </ul>"},{"location":"browserstack/bascis/#types-of-browserstack-products","title":"** Types of BrowserStack Products**","text":"<p>BrowserStack offers a suite of products for different testing needs:</p> <p>\ud83d\udcf1 App Automate Overview</p> <p>App Automate allows automated app testing using frameworks like Appium, Espresso, and XCUITest.</p> <ul> <li>\ud83d\udd01 Supports real Android and iOS devices.</li> <li>\ud83e\uddea Runs automated regression, functional, and integration tests.</li> <li>\u2699\ufe0f Integrates with CI/CD pipelines and test frameworks.</li> <li>\ud83d\udcca Provides detailed logs, video recordings, and screenshots.</li> </ul> <p>\u2705 Best For: Continuous automated mobile app testing.</p> <p>\ud83d\udcf1 App Live Overview</p> <p>App Live enables manual testing of mobile apps on real devices directly from your browser.</p> <ul> <li>\ud83d\udc46 Upload and interact with your app in real time.</li> <li>\ud83e\uddea Test gestures, camera, GPS, network simulation.</li> <li>\ud83d\udee0\ufe0f Debug using device logs and screenshots.</li> <li>\ud83d\udd10 No emulator \u2013 real device testing only.</li> </ul> <p>\u2705 Best For: Manual exploratory testing of Android/iOS apps.</p> <p>\ud83d\udcbb Automate Overview</p> <p>Automate is for automated web testing using Selenium, Cypress, and Playwright on real browsers and OS combinations.</p> <ul> <li>\ud83c\udf10 Supports 3000+ browser/OS combinations.</li> <li>\ud83d\udd01 Allows cross-browser automation at scale.</li> <li>\u2699\ufe0f Integrates with CI tools like Jenkins, GitHub Actions, CircleCI.</li> <li>\ud83d\udcf8 Visual debugging tools like screenshots and logs.</li> </ul> <p>\u2705 Best For: Scalable automated cross-browser web testing.</p> <p>\ud83d\udda5\ufe0f Live Overview</p> <p>Live enables manual cross-browser testing of websites across real desktop and mobile browsers.</p> <ul> <li>\ud83d\udd0d Test in real environments (not emulators).</li> <li>\ud83e\uddea Responsive testing, layout checks, and UI validation.</li> <li>\ud83d\udee0\ufe0f Developer tools access and debugging support.</li> <li>\ud83d\udd04 Instant switching between browsers and OS versions.</li> </ul> <p>\u2705 Best For: Manual UI/UX and responsive web testing.</p> <p>\ud83e\udde0 Summary</p> Product Purpose Type Platform App Automate Automated mobile app testing Automated Android, iOS App Live Manual mobile app testing Manual Android, iOS Automate Automated web testing Automated Web Browsers Live Manual cross-browser testing Manual Web Browsers <p>\ud83d\ude80 Each product is designed to fit into your development and QA workflow, enabling faster releases and better quality assurance across platforms.</p>"}]}